# 附录 A：计算与容量估算（公式与算例）

### A.1 开篇段落

本附录旨在为大型多模态模型预训练项目提供一套核心的计算与容量估算框架。对于 AI 科学家和基础设施工程师而言，在项目启动前精确地估算所需资源（计算、存储、网络）与成本，是确保项目顺利进行、获得预算批准、并设定合理预期的关键。本章将提供一系列关键公式、经验法则和具体算例，覆盖从训练 FLOPs 到存储成本的完整链路，帮助您为 1B 和 10B 规模的项目制定可靠的资源计划。

### A.2 训练 FLOPs 与时长估算

训练一个大型模型所需的总计算量（以 FLOPs 为单位）是项目规划的基石。它直接决定了训练时长和主要的算力成本。

#### A.2.1 FLOPs 估算公式

对于一个 Decoder-only 的 Transformer 模型，一次完整的前向和后向传播所需的计算量可以近似为

$$
\text{FLOPs}_{\text{fwd+bwd}} \approx 6 \times N \times D
$$

其中：
*   **N**: 模型的参数量 (e.g., 10B for a 10 billion parameter model)。
*   **D**: 训练数据的总 token 数 (e.g., 10T tokens)。
*   **6**: 这是一个经验系数。其中 `2` 来自前向传播（主要是两个大的矩阵乘法），`4` 来自后向传播（梯度计算大致是前向传播的两倍）。

因此，训练 `10T` token 的总计算量为：

$$
\text{Total FLOPs} = 6 \times N \times (10 \times 10^{12})
$$

#### A.2.2 训练时长估算

训练时长由总计算量和集群的有效算力决定。

$$
\text{Training Time (seconds)} = \frac{\text{Total FLOPs}}{\text{Num GPUs} \times \text{FLOPs per GPU} \times \text{MFU}}
$$

其中：
*   **Num GPUs**: GPU 数量（本项目中为 256）。
*   **FLOPs per GPU**: 单张 GPU 的理论峰值算力。对于 H100 80GB，使用 FP8/BF16 精度时，峰值算力约为 `2000 TFLOP/s` (i.e., $2 \times 10^{15}$ FLOPs/s)。
*   **MFU (Model FLOPs Utilization)**: 模型 FLOPs 利用率。这是一个关键的实际指标，表示 GPU 实际达到的计算效率，通常远低于 100%。它受限于通信开销、流水线气泡、IO 瓶颈等因素。
    *   **经验法则**: 对于大规模、高度优化的训练任务，MFU 通常在 **30% 到 50%** 之间。一个较为实际的规划值是 **40%**。

#### A.2.3 算例：1B 和 10B 模型（基于 256xH100, 10T tokens）

| 参数 | 1B 模型 | 10B 模型 |
| :--- | :--- | :--- |
| 模型参数 (N) | $1 \times 10^9$ | $10 \times 10^9$ |
| 训练 Token 数 (D) | $10 \times 10^{12}$ | $10 \times 10^{12}$ |
| **总 FLOPs** | $6 \times 10^9 \times 10^{13} = 6 \times 10^{22}$ | $6 \times 10^{10} \times 10^{13} = 6 \times 10^{23}$ |
| 集群理论峰值算力 | $256 \times 2000 \text{ TFLOP/s} = 5.12 \times 10^{17} \text{ FLOP/s}$ | $5.12 \times 10^{17} \text{ FLOP/s}$ |
| 集群有效算力 (MFU=40%) | $5.12 \times 10^{17} \times 0.4 = 2.05 \times 10^{17} \text{ FLOP/s}$ | $2.05 \times 10^{17} \text{ FLOP/s}$ |
| **预估训练时长 (秒)** | $6 \times 10^{22} / (2.05 \times 10^{17}) \approx 2.93 \times 10^5$ s | $6 \times 10^{23} / (2.05 \times 10^{17}) \approx 2.93 \times 10^6$ s |
| **预估训练时长 (天)** | $\approx 3.4$ 天 | $\approx 34$ 天 |
| **预估训练时长 (周)** | $\approx 0.5$ 周 | $\approx 4.8$ 周 |

**注意**: 此处时长为纯计算时间，未包含数据预处理、实验调试、节点故障等导致的额外时间。

### A.3 显存与激活开销

显存是训练大型模型的关键瓶颈。理解其构成对于设计并行策略至关重要。

$$
\text{Total Memory} = \text{Model States} + \text{Activations} + \text{KV Cache (Inference)} + \text{Misc}
$$

#### A.3.1 模型状态 (Model States)

这部分主要包括模型参数、梯度和优化器状态。

*   **模型参数 (Model Parameters)**: $N \times (\text{bytes per parameter})$
    *   BF16/FP16: $N \times 2$ bytes
*   **梯度 (Gradients)**: $N \times (\text{bytes per gradient})$
    *   通常与参数精度相，即 $N \times 2$ bytes
*   **优化器状态 (Optimizer States)**:
    *   **经验法则**: 使用 AdamW 优化器时，其状态（一阶和二阶动量）通常需要 `8 * N` 字节（FP32 存储）或更多。ZeRO 等优化策略会改变其分布，但总量不变。一个安全的估算是模型参数量的 **8-12 倍**。

#### A.3.2 激活 (Activations)

激活值是显存中最动态、也最难估算的部分。它与 batch size 和序列长度强相关。
$$
\text{Memory}_{\text{activations}} \approx B \times S \times H \times L \times (\text{constants})
$$
*   **B**: Batch size per GPU
*   **S**: Sequence length
*   **H**: Hidden dimension
*   **L**: Number of layers

**关键技术**: **Activation Checkpointing (or Recomputation)**
该技术通过在前向传播中丢弃中间激活，在后向传播时重新计算来大幅减少显存占用，代价是增加了约 30% 的计算量。在训练大模型时，这几乎是**必需**的。

#### A.3.3 KV Cache (推理时)

虽然预训练不直接使用 KV Cache，但在规划模型用途时必须考虑。
$$
\text{Memory}_{\text{KV Cache}} \approx 2 \times B \times S_{\text{kv}} \times H \times L \times (\text{bytes per element})
$$
*   $S_{\text{kv}}$: KV cache 的序列长度。
*   `2`: 分别为 Key 和 Value cache。

#### A.3.4 算例：1B 和 10B 模型显存估算（单 GPU，BF16）

| 组件 | 1B 模型 | 10B 模型 | 备注 |
| :--- | :--- | :--- | :--- |
| 模型参数 | $1 \times 10^9 \times 2 = 2$ GB | $10 \times 10^9 \times 2 = 20$ GB | BF16 精度 |
| 梯度 | $1 \times 10^9 \times 2 = 2$ GB | $10 \times 10^9 \times 2 = 20$ GB | BF16 精度 |
| 优化器状态 (AdamW) | $\approx 8 \times 1 \text{ G} \times 2 = 16$ GB | $\approx 8 \times 10 \text{ G} \times 2 = 160$ GB | 假设 FP32 状态，ZeRO 会分片 |
| **模型状态总和 (单份)** | **20 GB** | **200 GB** | 远超单卡 H100 80GB 显存 |
| 激活与杂项 | 依赖 BS/SeqLen，约 5-20 GB | 依赖 BS/SeqLen，约 10-40 GB | 开启 Activation Checkpointing |

**结论**: 即便是 1B 模型，完整的模型状态也可能超过单卡显存。因此，必须采用 **ZeRO-DP**、**张量并行 (TP)** 或 **流水线并行 (PP)** 来将模型状态分散到多张 GPU 上。

### A.4 IO 与存储：多模态分层存储

多模态数据的存储成本和 IO 吞吐是另一个关键考量，尤其是对于视频和音频。

#### A.4.1 数据量估算（以 100T token 池为例）

假设我们的 100T token 池（训练时从中采样 10T）有如下构成：

| 模态 | Token 占比 | 原始数据估算 | Tokenized 数据估算 | 存储建议 |
| :--- | :--- | :--- | :--- | :--- |
| 文本 | 70% (70T) | ~140 PB (假设 2 bytes/token) | ~140 PB (基本无压缩) | 热存储 (高速文件系统) |
| 图像 | 10% (10T) | ~20 PB (假设每图 256 token, 200KB/图) | ~20 PB | 热存储 |
| 音频 | 10% (10T) | ~15 PB (假设 50 tokens/s, 16kHz/16bit) | ~4 PB (RVQ 压缩率 ~4x) | 冷/温存储 (对象存储) |
| 视频 | 10% (10T) | ~100 PB (假设 10fps, 1Mbps 码率) | ~10 PB (RVQ 压缩率 ~10x) | 冷存储 (对象存储) |
| **总计** | **100% (100T)** | **~245 PB** | **~174 PB** | - |

**关键洞察**:
1.  **视频是存储巨兽**: 原始视频数据占据了绝大部分存储空间。
2.  **离散化显著压缩**: RVQ 对音视频的 tokenization 带来了可观的压缩，降低了训练时 IO 的压力。

#### A.4.2 分层存储策略

*   **冷存储 (Cold Storage)**: 用于存放原始的、很少访问的视频/音频文件。例如 AWS S3 Glacier Deep Archive。成本极低，但访问延迟高。
*   **温存储 (Warm Storage)**: 用于存放处理中间产物，或不常访问但需要较快读取的数据。例如 AWS S3 Standard。
*   **热存储 (Hot Storage)**: 用于存放最终的、tokenized 的、训练时高频读取的数据集（如 WebDataset 格式）。通常是高性能并行文件系统（如 Lustre, GPFS）或高速对象存储。

```ascii
      [Raw Video/Audio]      [Processed/Cleaned]      [Tokenized Datasets]
             |                        |                         |
      (e.g., S3 Glacier)      (e.g., S3 Standard)      (e.g., Lustre/FSx)
+------------------------+  +---------------------+  +----------------------+
|       Cold Storage     |  |     Warm Storage    |  |      Hot Storage     |
|   (>100 PB, ~$1/TB/mo)  |  |  (~20 PB, ~$20/TB/mo)|  | (~170 PB, >$100/TB/mo)|
+------------------------+  +---------------------+  +----------------------+
```

### A.5 成本模型：1B/10B（单轮 10T token）的预算区间

以下是一个粗略的成本估算模型，实际价格因供应商、合同和地区而异。

| 成本项 | 1B 模型 | 10B 模型 | 备注 |
| :--- | :--- | :--- | :--- |
| **算力成本 (Compute)** | 256 GPUs * 3.4 days * 24h * $3/h = **~$62.6 K** | 256 GPUs * 34 days * 24h * $3/h = **~$626 K** | 假设 H100 按需价 $3/h，预留实例更低 |
| **存储成本 (Storage)** | - | - | 存储成本与模型大小无关，与数据池大小有关 |
|  - 热存储 (174 PB) | 174,000 TB * $100/TB/mo * 3 mo = **~$52.2 M** | **~$52.2 M** | 假设项目周期为 3 个月 |
|  - 冷存储 (71 PB) | 71,000 TB * $1/TB/mo * 3 mo = **~$213 K** | **~$213 K** | 原始数据压缩后存放 |
| **带宽成本 (Bandwidth)** | 抓取 200 PB 数据，假设 $0.01/GB = **~$2 M** | **~$2 M** | 一次性成本，高度可变 |
| **人力成本 (Human)** | 10 人 * 6 个月 * $15k/mo = **~$0.9 M** | 10 人 * 9 个月 * $15k/mo = **~$1.35 M** | 涵盖数据、模型、Infra 团队 |
| **总预算区间 (粗估)** | **$55 M - $60 M** | **$56 M - $65 M** | **存储是主要成本**，其次是人力和带宽 |

**关键结论**: 在自采数据的多模态项目中，**数据存储和处理的成本可能远超算力成本**。这也是为什么“尽量复用别人洗好的数据包”是重要的降本策略。

### A.6 网络与通信瓶颈分析

在 256-GPU 规模下，节点间通信是决定 MFU 的关键。

*   **AllReduce**: 主要用于数据并行 (DP) 中的梯度同步。通信量与模型参数量 `N` 成正比。对于 10B 型（BF16），每次 AllReduce 需要传输 `10G * 2 bytes = 20 GB` 的数据。
*   **AllToAll**: 主要用于张量并行 (TP) 和专家并行 (MoE)。通信模式更为复杂，对网络拓扑的 bisection bandwidth 要求很高。

```ascii
     +-------+         AllReduce          +-------+
     | GPU 0 | <------------------------> | GPU 1 |
     | Grad0 |                            | Grad1 |
     +-------+                            +-------+
         |                                    |
         \--------> (Grad0+Grad1)/2 <---------/

     +-------+          AllToAll          +-------+
     | GPU 0 | --[Shard A1]-->[Shard B1]-- | GPU 1 |
     | [A0,B0] | <-[Shard B0]--[Shard A0]<- | [A1,B1] |
     +-------+                            +-------+
```

**经验法则**:
*   对于 **NVLink/NVSwitch** 连接的节点内部（e.g., DGX H100），通信带宽极高 (900 GB/s)，通常不会是瓶颈。
*   对于跨节点的 **InfiniBand (IB)** 网络 (e.g., 400 Gbps)，必须精心设计并行策略TP/PP/DP 组合）和通信/计算重叠，以避免网络成为瓶颈。使用 `nccl-tests` 等工具在训练前充分 benchmark 网络性能至关重要。

### A.7 经验法则速查表

| 项目 | 经验法则 / 公式 | 注释 |
| :--- | :--- | :--- |
| 训练 FLOPs | $6 \times N \times D$ | N=参数量, D=Token数 |
| MFU 范围 | 30% - 50% | 实际集群效率，40% 是个不错的起点 |
| AdamW 优化器显存 | $8 \times N \times \text{bytes_per_param}$ | FP32 状态下约 16x 模型参数大小 |
| Activation Checkpointing | 必选项 | 节省大量激活显存，增加 ~30% 计算 |
| 多模态存储成本 | 视频 >> 音频 > 图像 > 文本 | 视频是成本和 IO 挑战的主要来源 |
| 数据策略 | 复用 > 自采 | 优先使用高质量的开源数据集以降低存储和合规成本 |
| 网络 Benchmark | 必须先于训练 | 使用 `nccl-tests` 验证通信拓扑和带宽 |
| 预算大头 | 数据存储 > 人力 > 计算 | 在自采数据的场景下 |

### A.8 本章小结

本附录提供了一套用于规划大规模多模态预训练项目的量化估算工具。我们从计算量的核心公式 $6ND$ 出发，推导了训练时长，并分析了显存的关键构成（模型、梯度、优化器、激活）。我们强调了多模态数据，特别是视频，对存储和 IO 带来的巨大挑战，并提出了分层存储策略。最后，通过一个成本模型匡算了 1B 和 10B 项目的预算范围，揭示了数据成本在整个项目中的主导地位。这些估算模型和经验法则是您在启动类似项目时进行技术选型、资源申请和风险评估的坚实基础。

### A.9 常见陷阱与错误 (Gotchas)

1.  **过度乐观的 MFU**: 直接使用理论峰值算力进行规划是项目延期的最常见原因。务必考虑 30%-50% 的实际利用率。
2.  **忽略优化器显存**: 新手常常只计算模型参数的显存，而忽略了 AdamW 等优化器状态是其数倍大小，导致错误的并行策略设计。
3.  **低估数据 IO 瓶颈**: 认为只要 GPU 够快训练就够快。但在处理海量视频/音频数据时，数据加载和预处理流水线很容易成为瓶颈，导致 GPU 空闲。
4.  **隐藏的云成本**: 只计算了虚拟机或 GPU 的小时价，忽略了数据传输（尤其是出向流量）、存储 API 调用、快照等长期且隐蔽的成本。
5.  **混淆训练与推理显存**: 用训练时的显存占用去规划推理服务，忘记了 KV Cache 在长上下文推理时会消耗巨量显存。
6.  **静态数据配比**: 认为数据准备是一次性工作。实际上，数据的动态采样、清洗和扩充贯穿项目始终，需要为“数据迭代”预留存储和计算资源。
