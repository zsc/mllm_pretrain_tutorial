# 第 11 章：数据混采与课程策略（10T token 单轮）

## 11.1 开篇与学习目标

在前面的章节中，我们已经精心设计了数据配方，并完成了对文本、音频、视频、图像四个模态的数据采集、清洗、离散化，构建了一个规模宏大的 100T token 数据池。然而，拥有高质量的数据池只是第一步。如何将这些数据以最高效、最合理的方式“喂”给模型，直接决定了训练的效率、稳定性和最终模型的性能。本章将聚焦于数据“供给侧”的动态策略，即**数据混采（Data Mixing）**与**课程学习（Curriculum Learning）**。

我们将把数据供给过程类比为为一位天才学生（我们的模型）设计一套从易到难、从广到精的学习计划。这个计划不是一成不变的，而是随着学生的成长（模型训练的进展）动态调整的。通过本章的学习，您将掌握：

*   **动态采样**：如何使用温度和退火策略，在训练的不同阶段动态调整不同模态、语言和质量域的数据比例。
*   **课程学习**：如何设计上下文长度的扩展课程，以加速早期训练并提升模型处理长序列的能力。
*   **多模拼接与打包**：在自回归框架下，如何将不同模态的离散 token 拼接成统一序列，并应用恰当的注意力屏蔽规则。
*   **数据新鲜度控制**：如何通过再采样和去重，保证模型在单轮（epoch）训练中看到最多样化的数据。
*   **高级策略**：了解基于损失的在线数据重加权等前沿技术，以进一步挖掘数据价值。

本章内容是连接“静态数据”与“动态训练”的关键桥梁，是决定大规模预训练成败的核心工程技艺之一。

## 11.2 模态/语言/域的采样温度与退火计划

一个常见的误区是采用固定的数据混合比例贯穿整个训练过程。例如，始终保持 70% 文本、10% 图像、10% 音频、10% 视频的比例。这种静态策略忽略了模型在不同学习阶段对不同类型数据的需求。一个更优的策略是引入**采样温度（Sampling Temperature）**和**退火计划（Annealing Schedule）**。

### 采样温度

假设我们为数据池中的第 `i` 个数据源（例如，维基百科、YouTube 播客、LAION-5B 等）分配了一个基础权重 $w_i$（详见第 3 章）。在采样时，我们不直接使用 $w_i$，而是通过一个温度参数 $T$ 来调节其采样概率 $P_i$：

$$
P_i = \frac{w_i^{1/T}}{\sum_{j} w_j^{1/T}}
$$

温度 $T$ 的作用如下：
*   **$T > 1$**：**平滑分布**。概率分布会变得更加均匀，即使是基础权重 $w_i$ 较低的数据源也有更高的机会被选中。这有助于模型在早期接触到更广泛、更多样的数据，建立对世界的基本认知，防止“视野狭窄”。
*   **$T = 1$**：**原始分布**。采样概率严格正比于基础权重 $w_i$。
*   **$0 < T < 1$**：**锐化分布**。概率分布会向高权重的数据源集中，使得高质量、高价值的数据被更频繁地采样。这有助于模型在训练后期进行“精修”，专注于提升在核心任务上的表现。

### 退火计划

退火计划是指在整个训练过程中（例如，完成 10T token 的一轮训练）如何动态调整温度 $T$。一个典型的退火计划是**从高温向低温过渡**。

**经验法则 (Rule-of-Thumb):**

*   **初始阶段 (0 ~ 2T tokens)**：设置较高的温度，例如 **$T \in [1.5, 2.0]$**。这鼓励模型探索多样性，特别是对于多模态数据，让模型充分接触各种模态组合和长尾领域。
*   **中期阶段 (2T ~ 7T tokens)**：线性或余弦退火，将温度从高点逐步降低到 **$T \approx 1.0$**。此时，模型已经建立了基础能力，开始按照我们设计的核心数据配方进行学习。
*   **后期阶段 (7T ~ 10T tokens)**：将温度进一步降低到 **$T \in [0.8, 1.0]$**。此时，模型开始收敛，我们需要让它更专注于高质量的数据集（如高质量代码、书籍、学术论文、精选的多模态对齐数据），进行“精加工”。

可以为不同维度（模态、语言、质量域）设置独立的退火计划。例如，对于语言分布，我们可能始终希望保持对低资源语言的一定探索性；而对于质量域，我们则明确希望在后期高度聚焦于 A 类数据。

```ascii
      ▲ 温度 (T)
 2.0 -┤     ************
      │    *              *
 1.5 -┤   *                *
      │  *                  *
 1.0 -┤ *                    *****************
      │*
 0.8 -┤                        *
      │
      └──────────────────────────────────► 训练进度 (Tokens)
         0T      2T          7T          10T
         | 高温探索期 |   线性/余弦退火   |   低温精修期  |
```

## 11.3 长序列比例与上下文扩展课程

从训练开始就使用完整的上下文长度（例如 32K tokens）是极其低效的。原因有二：
1.  **计算成本**：Transformer 的自注意力机制计算复杂度是序列长度的平方 ($O(L^2)$)，长序列会极大地拖慢训练速度。
2.  **学习效率**：在训练初期，模型尚不具备处理长距离依赖的能力，强行喂给长序列数据，模型也只能学到局部信息，造成计算资源浪费。

因此，我们设计一个**上下文扩展课程 (Context Length Extension Curriculum)**。

**课程设计:**

*   **阶段一：短序列启动 (0 ~ 1T tokens)**
    *   **上下文长度**：2048 或 4096。
    *   **目标**：让模型快速学习 token 间的局部依赖关系、基本语法和多模态 token 的局部结构。
    *   **优势**：训练速度快，可以使用更大的全局批次大小（Global Batch Size），加速模型参数的初步收敛。

*   **阶段二：中序列过渡 (1T ~ 4T tokens)**
    *   **上下文长度**：8192。
    *   **目标**：逐步培养模型处理更长依赖的能力，例如段落级理解、短视频/音频片段的连贯性。
    *   **操作**：在切换上下文长度时，可能需要重置优化器状态或应用一个短暂的学习率预热期，以避免训练震荡。同时，需要关注 RoPE 等位置编码的基频（base frequency）是否需要调整以适应更长的上下文。

*   **阶段三：全序列训练 (4T ~ 10T tokens)**
    *   **上下文长度**：32768 (或目标最大长度)。
    *   **目标**：训练模型掌握长距离依赖、篇章级理解、长视频/音频的全局关联性等复杂能力。
    *   **数据**：在此阶段，应有意识地增加数据流中长文档、长视频、长音频的采样比例，确保长上下文窗口得到充分利用。

这个课程设计不仅节省了大量的计算资源，也符合认知科学中“循序渐진”的学习规律，让训练过程更稳定、高效。

## 11.4 多模拼接与 Mask 规则

对自回归模型，所有模态的数据最终都需要被转换成一个一维的离散 token 序列。如何组织这个序列，以及如何应用注意力掩码（Attention Mask），是多模态融合的关键。

### 多模拼接

我们使用特殊的 token 来分隔和标识不同模态的数据段。一个典型的多模态序列样本如下：

```
[BOS] <|text_start|> 这是一段描述下方图片的文字。 <|text_end|> <|img_start|> [IMG_0] [IMG_1] ... [IMG_255] <|img_end|> <|aud_start|> [AUD_0] [AUD_1] ... [AUD_1023] <|aud_end|> [EOS]
```

*   `[BOS]` / `[EOS]`：序列开始和结束符。
*   `<|modal_start|>` / `<|modal_end|>`：模态专用分隔符，明确告知模型数据段的类型。这些是我们在第 8 章中添加到词表里的特殊 token。
*   `[IMG_i]` / `[AUD_i]`：来自图像/音频 RVQ 编码器的离散 token。

**Packing 策略**：为了最大化 GPU 利用率，通常会将多个短样本拼接（pack）成一个长的序列，直到达到目标上下文长度不同样本之间用 `[EOS]` 隔开。在计算损失时，我们会忽略样本间填充部分的 logits。

### Mask 规则

标准的自回归模型使用**因果掩码 (Causal Mask)**，即每个 token 只能注意到它之前的所有 token。

```ascii
      t_1  t_2  t_3  t_4
t_1 |  1    0    0    0  |   (t_1 只能看到自己)
t_2 |  1    1    0    0  |   (t_2 能看到 t_1, t_2)
t_3 |  1    1    1    0  |   ...
t_4 |  1    1    1    1  |
```

在多模态场景下，有时需要更精细的 **段间屏蔽 (Inter-segment Masking)** 规则来引导模型学习特定的跨模态关联，而非“作弊”。

*   **场景**：一段文字描述紧随其后的图片。
*   **问题**：在预测图片 token `[IMG_j]` 时，模型不仅能看到前面的文本，还能看到前面的图片 token `[IMG_0]...[IMG_{j-1}]`。模型可能会过度依赖图片内部的局部相关性（例如，天空的蓝色像素块后面很可能还是蓝色的），而忽略了文本的指导作用。
*   **解决方**：可以设计一种特殊的 mask，使得图片 token 在被预测时，其注意力**只能**看到前序模态（文本）的 token，而不能看到当前模态（图片）内已经生成的 token。这强迫模型必须依赖文本来生成图片，从而学习更强的图文对齐能力。

这是一个高级选项，对于旨在增强“理解”能力的一体化模型尤其有价值。对于纯粹的生成任务，标准的因果掩码通常已足够。

## 11.5 去重后再采样与新鲜度控制

在第 4-7 章中，我们对整个数据池做了严格的去重。但这还不够。在构建一个 10T token 的训练批次时，由于不同数据源的大小和采样权重差异巨大，某些来自大型语料库（如 Common Crawl）的文档可能会被重复采样多次，而一些来自小型、高质量语料库（如书籍）的文档可能一次也未被选中。这降低了数据的**新鲜度 (Freshness)**。

**策略：单轮去重 (Epoch-level Deduplication)**

1.  **生成采列表**：根据当前的动态采样概率（由温度和退火计划决定），生成一个足够大的样本标识符列表，其总 token 数远超 10T（例如，15T）。
2.  **对列表去重**：使用 MinHash 或 SimHash 对这个样本列表进行快速的近邻去重。去除那些在本次采样中被重复选中的样本。
3.  **截取最终批次**：从去重后的列表中，按顺序取样，直到凑够 10T token 的训练数据。

这个过程确保了在单轮训练中，模型看到的绝大多数数据都是独一无二的，极大地提升了数据多样性，有效防止模型对高频数据产生“记忆性”过拟合。

## 11.6 在线质量重加权（loss-based reweighting）

这是一项更前沿和实验性的技术，旨在让模型“自主”地发现并关注有价值的数据。

**核心思想**：
如果一个数据样本在模型上产生了很高的损失（loss），这通常意味着该样本包含模型尚未学好的“新知识”，或者它与模型当的“世界观”冲突。因此，这些高损失的样本可能具有更高的学习价值。

**简易流程**：
1.  **探测（Probing）**：使用一个较早的、稳定的模型检查点（或者一个独立的、更小的“探测”模型），对下一批待训练的数据进行一次前向传播，计算每个样本的损失。
2.  **重加权（Reweighting）**：根据样本的损失值，动态提升高损失样本的采样权重。例如，可以直接将损失值作为权重的乘数。
3.  **训练**：使用重加权后的数据分布进行正式的梯度更新。

**风险与权衡**：
*   **噪声放大**：高损失也可能意味着**脏数据或噪声**。直接放大高损失样本的权重，有可能会让模型学习到错误或有害的信息。因此，该技术必须与高质量的数据过滤相结合。
*   **计算开销**：需要额外的前向传播，增加了数据预处理的复杂度与计算成本。

在实践中，可以采用一个更温和的策略：只对被判定为高质量（A/B 层）的数据应用基于损失的重加权，并对权重提升设置一个上限，以防极端异常值主导训练。

## 11.7 本章小结

本章详细阐述了将静态数据池转化为动态、智能的学习流的各项关键策略。这些策略共同构成了一套复杂而精密的“课程”，指导模型高效、稳定地完成大规模预训练。

*   **动态采样是核心**：通过**温度**和**退火计划**，我们可以动态调整数据“食谱”，在探索与利用之间取得平衡，让模型在不同阶段学习最适宜的内容。
*   **课程学习提效显著**：**上下文长度课程**通过由短到长的训练，显著加速了早期收敛，并稳定地将模型能力扩展到长序列。
*   **拼接与屏蔽是多模融合的基础**：标准化的**多模态 token 拼接**和灵活的**注意力掩码**规则，是实现跨模态自回归学习的基石。
*   **数据新鲜度至关重要**：通过**单轮去重**，我们确模型在有限的训练时间内接触到尽可能广泛和新颖的数据。
*   **高级技术提供优化空间**：**基于损失的重加权**等策略虽有风险，但为进一步提升训练效率和模型能力提供了可能性。

成功实施这些策略，需要数据、模型和 infra 团队的紧密协作，是衡量一个团队大模型工程能力的重要标尺。

## 11.8 常见陷阱与错误 (Gotchas)

1.  **陷阱：采用静态混合比例**
    *   **症状**：训练初期，模型在复杂模态（如视频）上收敛缓慢；训练后期，模型在核心能力（如高质量文本和代码）上的提升趋于饱和。
    *   **原因**：一成不变的数据配方无法满足模型在不同阶段的学习需求。
    *   **规避**：必须设计并实施动态的采样退火计划。

2.  **陷阱：上下文长度切换过于激进**
    *   **症状**：在将上下文长度从 2K 突增到 32K 时，训练损失突然飙升，甚至导致模型发散（NaN loss）。
    *   **原因**：模型的位置编码、归一化层统计量以及优化器状态都无法适应序列长度的剧变。
    *   **规避**：采用分阶段、渐进式的上下文扩展。在每次切换后，考虑使用短暂的学习率预热或重置优化器状态，让模型平稳过渡。

3.  **陷阱：注意力掩码实现错误**
    *   **症状**：训练损失异常低，但模型在生成任务中表现极差，只会逐字复制或生成无意义的重复序列。
    *   **原因**：因果掩码或多模态掩码逻辑出错，导致模型“偷看”到了未来的 token。这是一个非常隐蔽且致命的 bug。
    *   **规避**：对数据加载器和模型前向传播中的掩码逻辑进行严格的单元测试。可以构造一个简单的序列，手动验证注意力矩阵是否符合预期。

4.  **陷阱：忽视数据新鲜度，重复“咀嚼”旧数据**
    *   **症状**：模型在某些常见的网页内容上表现出色，但在需要广博知识或处理见模式时能力不足。验证集损失下降缓慢。
    *   **原因**：训练数据被少数几个大型语料库中的高频样本主导，模型反复学习相同的内容，导致过拟合。
    *   **规避**：实施单轮去重策略，并监控训练过程中实际使用的数据源分布，确保其多样性。

5.  **陷阱：盲目应用基于损失的重加权**
    *   **症状**：模型开始生成一些奇怪、有毒或完全错误的内容，训练稳定性下降。
    *   **原因**：将数据噪声（例如，格式错误的文本、损坏的图像）产生的高损失误认为是“知识”，并对其进行了放大。
    *   **规避**：只对经过严格清洗和质量分层的高质量数据子集应用此技术，并设置合理的权重上限。始终将此视为一个可选的“增强项”，而非基础流程。
