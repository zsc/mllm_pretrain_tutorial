# 第 6 章 — 视频数据：合规抓取、切片与时空离散化

## 6.1 开篇与学习目标

视频是信息密度最高、也最具挑战性的模态。它融合了时序变化的视觉信息、同步的音频流以及潜在的文本内容（字幕、场景文字）。处理视频数据的复杂性不仅在于其巨大的存储和计算开销，更在于如何将其有效地转化为模型可以理解的、统一的离散符号序列。本章将全面拆解构建大规模视频预训练数据集的完整流程，从数据源的合规获取，到时空维度的智能采样，再到最终转换为离散 token 的核心技术。

**学习目标:**

*   掌握视频数据的主流来源、合规抓取方法与元数据的重要性。
*   理解视频时空采样策略（镜头切分、关键帧、Tubelet）的原理与权衡。
*   学会从视频中抽取并对齐多模态信息（音轨、字幕、OCR）。
*   深入理解视频离散化技术，特别是基于 VQ-VAE 和 RVQ 的时空 token 化方案。
*   建立视频数据处理的成本模型，并学会设计高效的存储与搬运策略。

## 6.2 来源与许可（YouTube/学术/CC 库），字幕/元数据抓取

与文本和图像相比，高质量、大规模且拥有清晰许可的视频数据更为稀缺。因此，数据源的选择和合规性审查是整个项目的第一道防线。

*   **主要来源**:
    1.  **YouTube**: 最大的用户生成内容视频库，内容覆盖极其广泛。但许可复杂，需严格遵守其服务条款（TOS）和 `robots.txt` 规则。抓取时应优先选择带有知识共享（Creative Commons, CC）许可的频道和视频。
    2.  **学术数据集**: 例如 HowTo100M、Something-Something V2、WebVid 等，这些数据集通常经过初步清洗，有明确的研究用途许可，是构建高质量训练集的绝佳起点。
    3.  **专业视频库**: 如 Vimeo、Pexels 等，其中包含大量高质量、CC 许可的素材，适合用于补充特定领域（如自然风光、艺术创作）。

*   **抓取工具与策略**:
    *   `yt-dlp` 是抓取 YouTube 视频的事实标准工具。务必配置其以下载所有可用资源：最高分辨率的视频、所有语言的音轨、所有格式的字幕（自动生成和手动上传）、以及视频的元数据（标题、描述、标签、上传者信息）。
    *   对于受 API 保护的学术库，应使用官方提供的下载脚本，并注意遵循其速率限制。

*   **元数据的重要性**:
    视频的标题、描述和用户标签是极其宝贵的弱监督信号。这些文本信息可以用来：
    *   初步过滤内容（例如，搜索包含“教程”、“演讲”等关键词的视频）。
    *   与视频内容进行语义对齐，作为 CLIP 式对比学习的原始文本对。
    *   辅助生成视频的文本描述。

> **经验法则 (Rule-of-Thumb)**
> 永远优先抓取并存储所有可用的元数据和多语言字幕。手动上传的字幕质量远高于自动生成的字幕，应给予更高权重。即使是低质量的 ASR 字幕，也比没有文本信息要好。一个视频文件的价值，很大程度上取决于其附带的元数据质量。

## 6.3 镜头切分与采样（fps、关键帧、Tubelet）

原始视频流在时间和空间上都存在大量冗余。直接处理完整视频不仅计算成本高昂，也可能稀释有效信息。因此，必须进行智能的切分与采样。

*   **镜头切分 (Shot Detection)**: 一个视频通常由多个镜头组成。在镜头切换点进行切分，可以得到语义上连贯的视频片段。这远优于按固定时长（如每10秒）切分的“硬切”方法。
    *   **工具**: 可以使用 `PySceneDetect` 或基于深度学习的镜头检测模型。
    *   **目标**: 将长视频分解为一系列时长在 3-15 秒的语义单元（shots）。

*   **帧采样率 (Frames Per Second, FPS)**: 原始视频的 FPS（通常为 24/30/60）对于模型来说过高。需要进行降采样。
    *   **策略**: 对于内容变化缓慢的视频（如演讲、教学），1-2 FPS 可能就足够了。对于动作密集型视频（如体育、舞蹈），可能需要 4-8 FPS。
    *   **权衡**: 更高的 FPS 能更好地捕捉动态信息，但会线性增加 token 数量和计算成本。一个常见的折中是全局采用 3-4 FPS。

*   **Tubelet 采样**: 这是将视频片段输入到视觉编码器的标准方式。一个 Tubelet 是一个时空“立方块”，例如 `16帧 x 224x224像素`。
    *   **流程**: 从一个镜头片段中，以一定的步长（stride）采样一系列 Tubelet。如果片段长度不足一个 Tubelet，可以进行填充（padding）或舍弃。

下面的 ASCII 图展示了从视频到 Tubelet 的过程：
```ascii
原始视频 (30fps)
  |
  +-- [镜头切分] --> 镜头1 (长度: 8秒)
                          |
                          +-- [降采样 @ 4fps] --> 32 帧
                                                  |
                                                  +-- [Tubelet 采样] --> Tubelet A (帧 1-16)
                                                  |
                                                  +-- [Tubelet 采样] --> Tubelet B (帧 17-32)
```

## 6.4 音轨/字幕抽取、OCR 与语义对齐

视频的价值在于其多模态的内在对齐。必须将这些伴生信息完整地抽取出来，并保留其时序关系。

*   **音轨抽取**: 使用 `ffmpeg` 等工具可以无损地从视频容器中分离出音频流。`ffmpeg -i video.mp4 -vn -acodec copy audio.aac`。该音频流将进入第 5 章描述的音频处理流水线。
*   **字幕对齐**: 字幕文件（如 `.srt`）天然包含了文本与时间戳。这是最强的监督信号之一，必须精确地将每个字幕片段与对应的视频帧和音频片段对齐。
*   **场景文字识别 (OCR)**: 视频画面中常常包含重要的文本信息，如演示文稿的标题、路标、产品标签等。以对关键帧定期运行 OCR 模型（如 PaddleOCR、EasyOCR）来提取这些文字。提取出的文字及其出现的时间范围（帧号）也应被记录下来。

所有抽取出的信息（音频片段、字幕、OCR 文本）都必须与视频帧的时间戳严格对齐，形成一个统一的多模态时间轴。

## 6.5 离散化：视频 VQ + RVQ（帧块码本、token/秒）

这是将连续的视频信号转换为离散 token 序列的核心步骤，其目标是将一个 Tubelet 压缩成一小组离散的 ID。

*   **整体架构**:
    1.  **视频编码器 (Encoder)**: 一个 3D 卷积网络（如 3D ResNet）或时空 Transformer（如 ViViT），接收一个 Tubelet 作为输入，输出一个低维度的时空特征图（latent feature map）。例如，一个 `16x224x224` 的 Tubelet 可能被编码为 `8x14x14` 的特征图。
    2.  **量化器 (Quantizer)**: 对特征图中的每一个特征向量，使用 **残差向量量化 (Residual Vector Quantization, RVQ)** 将其映为一组离散的码本索引（codebook indices）。

*   **为何使用 RVQ**:
    相比于标准的 VQ-VAE，RVQ 使用多个级联的、较小的码本。第一级 VQ 对原始向量进行量化，然后后续的 VQ 对前一级的量化残差进行量化。
    *   **优势**: 可以在码本大小（词表）可控的前提下，实现更高精度的表示。例如，使用 4 个大小为 1024 的码本（`4 x 1024`），可以组合出 `1024^4` 种可能的表示，远超单个大码本。这对于细节丰富的视频至关重要。

*   **Token 速率计算**:
    假设：
    *   视频采样率为 4 FPS。
    *   Tubelet 包含 16 帧（即 4 秒内容）。
    *   编码器输出 `t * h * w` = `8 * 14 * 14` 的时空特征。
    *   RVQ 使用 4 个码本。
    *   每个特征向量被量化为 4 个 token。

    总 token 数 = `8 * 14 * 14 * 4 = 6272` tokens
    视频时长 = `16 帧 / 4 FPS = 4` 秒
    **Token 速率** = `6272 tokens / 4 秒 = 1568` tokens/秒

这个速率是设计模型上下文长度、计算资源规划和数据预算的基石。

```ascii
Tubelet (T, H, W, C)
     |
     v
3D ConvNet / ViT Encoder
     |
     v
Latent Feature Map (t, h, w, d)
     |
     v
+-----------------------------+
|    Residual Vector          |
|    Quantization (RVQ)       |
|                             |
|  Codebook 1 -> Token_1      |
|  Residual_1 -> Codebook 2 -> Token_2 |
|  Residual_2 -> ...          |
+-----------------------------+
     |
     v
Discrete Token Sequence (N_tokens)
```

## 6.6 质量过滤

原始视频数据质量参差不齐，必须进行严格的自动化过滤。

*   **模糊/低分辨率**: 使用拉普拉斯算子方差（Laplacian variance）等方法评估帧的清晰度，过滤掉模糊的片段。
*   **水印/台标**: 训练一个专门的 logo/水印检测器，或者使用模板匹配方法，过滤掉带有大量商业水印的视频。
*   **重复内容**: 对关键帧提取感知哈希（如 pHash, dHash）或 CLIP 特征向量，检测并除高度相似或完全重复的视频片段。
*   **无关内容**: 使用一个预训练的视频/图像分类模型（如 CLIP），过滤掉不希望出现的内容，例如静态的“即将开始”画面、视频末尾的订阅推广、纯广告片段等。

## 6.7 存储与搬运成本模型（TB/小时、分层存储、预取）

视频数据是存储和带宽的“吞噬巨兽”，必须精打细算。

*   **成本模型**:
    一个粗略的成本模型可以表示为：
    `总成本 = 存储成本 + 计算成本 + 网络成本`
    `存储成本 = (原始数据 TB * 冷存单价) + (中间/离散化数据 TB * 热存单价)`
    `网络成本 = 数据搬运 TB * 带宽单价`

*   **分层存储策略**:
    1.  **冷存储 (Cold Storage)**: 如 AWS S3 Glacier Deep Archive。用于存放抓取下来的原始视频文件。成本极低，但访问延迟高。
    2.  **热存储 (Hot Storage)**: 如高性能对象存储或并行文件系统（Lustre, GPFS）。用于存放已经切片、离散化为 token 的数据。这些数据需要被训练集群高速、低延迟地访问。

*   **数据搬运与预取**:
    数据处理流水线（ETL）应部署在离冷存储近的计算资源上，将处理后的 tokenized 数据推送到离 H100 集群近的热存储上。训练时，数据加载器（data loader）必须实现高效的**预取（prefetching）**机制，确保在一个 mini-batch 的计算完成前，下一个 batch 的数据已经从存储拉取到 GPU 内存中，避免 GPU 因等待 I/O 而空闲。

> **经验法则 (Rule-of-Thumb)**
> 假设一个 256xH100 集群的训练吞吐量为 20k tokens/秒/GPU，总吞吐量约为 5.12M tokens/秒。如果视频 token 速率为 1.5k tokens/秒，则集群每秒消耗约 3400 秒时长的视频数据。数据预处理和加载流水线的吞吐能力必须至少是这个数字的 1.2 倍，才能保证不成为瓶颈。

## 6.8 合成视频与数据扩增（可选）

在特定领域数据不足时，可以考虑使用合成数据。

*   **合成视频**: 使用先进的文本到视频生成模型（如 Sora 的小型开源替代品），根据特定文本提示生成视频，用于补足长尾分布中的场景。例如，生成“宇航员在火星上弹吉他”的视频。这需要极高的计算成本，通常只用于小规模的精确补足。
*   **数据扩增**: 在将 Tubelet 送入编码器之前，可以应用标准的视觉数据扩增技术，如随机水平翻转、颜色抖动、随机裁切等。这可以有效增加数据多样性，提高模型泛化能力。

## 6.9 Token 预算与课程调度

视频数据在整个 30T token 的总预算中应占有合理比例。其高信息密度和高 token 速率意味着它会快速消耗 token 预算。

*   **Token 预算**: 假设视频数据占总 token 预算的 10-20%（即 3T - 6T tokens）。根据之前计算的 1.5k tokens/秒速率，这对应着约 55万 - 110万 小时的视频内容。这是一个巨大的数据工程挑战。
*   **课程调度 (Curriculum Learning)**: 建议采用分阶段的策略：
    1.  **阶段一**: 优先使用高质量、短小（5-30秒）、与文本描述强相关的视频（如教学片段、产品评测）。这有助于模型在早期稳定地学习跨模态关联。
    2.  **阶段二**: 逐渐引入更长、更复杂、文本监督信号更弱的视频（如电影、纪录片），并相应增加模型的上下文长度。

## 6.10 本章小结

本章详细阐述了将原始视频转化为模型可用的离散 token 序列的全过程。我们从合规的数据源选择和元数据抓取开始，强调了其对项目成功的基础性作用。接着，我们讨论了如何通过镜头切分和 Tubelet 采样来处理视频的时空冗余。核心技术部分深入解析了基于 VQ-VAE 和 RVQ 的视频离散化方案，并给出了 token 速率的估算方法。最后，我们覆盖了质量过滤、存储成本管理和课程学习等关键工程实践。成功处理好视频数据，是构建真正强大的多模态模型的关键一步，它本质上是一个大规模的分布式数据物流与计算问题。

## 6.11 常见陷阱与错误（Gotchas）

1.  **忽略许可证与服务条款**: 直接抓取大量受版权保护的视频内容可能导致严重的法律风险，甚至项目中止。务必建立严格的合规审查流程。
2.  **时间戳错位**: 在抽取音轨、字幕、OCR 时，如果时间戳未能精确对齐，模型将学到错误的跨模态关联（例如，声音与口型不匹配）。必须使用统一的时钟基准，并进行严格的同步校验。
3.  **存储 I/O 瓶颈**: 低估了训练集群对数据加载的吞吐量需求。在热存储选型和数据加载器设计上投入不足，会导致昂贵的 GPU 资源大量时间在等待数据。
4.  **“一刀切”的采样率**: 对所有类型的视频使用相同的 FPS 进行降采样。这可能导致在动作密集视频中丢失关键动态信息，或在静态视频中浪费存储和计算。应考虑基于内容动态性自适应采样策略。
5.  **RVQ 码本崩溃 (Codebook Collapse)**: 在训练视频 VQ-VAE 时，如果超参数或初始化不当，可能导致码本中的大量 code 从未被使用，从而降低表示能力。需要仔细监控码本利用率，并使用码本重置等技巧。
6.  **元数据丢失**: 在多步 ETL 流程中，原始视频的标题、描述等宝贵元数据意外丢失，使得后续无法利用这些弱监督信号。数据处理的每一步都应确保元数据的完整传递。
7.  **视频质量评估不足**: 未能有效过滤掉低质量（模糊、严重压缩、带巨大水印）的视频，导致模型在“脏”数据上浪费了大量的计算资源，并可能学习到噪声模式。
