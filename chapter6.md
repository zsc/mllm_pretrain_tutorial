# chapter6.md — 视频数据：合规抓取、切片、时空离散化

### 开篇段落

视频是信息密度最高、也最具挑战性的模态。它融合了时序变化的视觉信息、同步的音频流以及潜在的文本内容（如字幕、场景文字）。本章的目标是将复杂、高维度的原始视频数据，转化为一种统一、离散的 token 序列，使其能够与文本、音频等其他模态数据一同被自回归 Transformer 模型处理。我们将深入探讨从合规的数据源选择与抓取，到精细化的预处理流水线（包括镜头切分、元数据抽取），再到核心的时空离散化技术（Video VQ-VAE + RVQ）的全过程。学完本章，您将掌握构建一个生产级视频数据处理管线的完整方法论与工程实践，为最终的多模态预训练奠定坚实基础。

### 6.1 视频数据源与合规抓取

与有数据工作一样，合规性是第一原则。视频内容的版权通常比文本和图像更复杂。我们的策略是仅使用具有明确开放许可（如 Creative Commons）或机构授权的内容。

**1. 主要数据源类型:**

*   **YouTube (通过 API)**: YouTube 是最大的视频库，但必须严格遵守其服务条款 (ToS)。关键在于利用其 API 筛选具有 Creative Commons 许可（`videoLicense='creativeCommon'`）的视频。我们应建立一个“频道白名单”，优先抓取教育、讲座、纪录片、产品评测等信息密度高的频道。
*   **Vimeo**: 同样提供基于 CC 许可的筛选功能，其内容质量通常较高，尤其是在艺术和技术领域。
*   **Internet Archive**: 包含大量公有领域的视频资料，如历史纪录片、政府档案等，是高质量、合规数据的宝库。
*   **学术与机构平台**: 如大学公开课（MIT OpenCourseWare）、学术会议录像等，通常有明确的使用许可。

> **Rule-of-Thumb: 内容选择优先级**
> 讲解/教程 > 纪录片 > 演讲/访谈 > 生活记录 > 纯娱乐/MV/电影预告片。
> 优先选择“有话可说、有物可看”的视频，即视觉内容与旁白/字幕强相关，这对于学习跨模态对齐至关重要。

**2. 合规抓取工具链:**

推荐使用 `yt-dlp` 的 Python 库或命令行工具，它功能强大且尊重 `robots.txt`。关键配置包括：
*   **许可过滤**: 编写脚本通过 YouTube Data API v3 搜索并过滤出 CC-BY 许可的视频 ID 列表。
*   **格式与质量**: 下载特定分辨率（如 720p）的 MP4 格式，避免下载最高码率以节省带宽和存储。
*   **元数据**: 同时下载所有可用的元数据，包括自动/手动字幕（`.vtt`）、描述、标题等。
*   **速率限制**: 遵守 API 的调用频率限制，设置合理的下载延迟，避免对源站造成过大压力。

### 6.2 预处理流水线：从原始视频到待编码帧

原始视频无法直接使用，必须经过一系列细的清洗、切片和对齐操作。这是一个计算密集型环节，建议使用 Ray 或 Dask 等分布式计算框架进行并行处理。

**ASCII 流程图:**
```
[Raw Video MP4] --(FFmpeg)--> [Audio Stream] & [Video Frames]
       |                                |
       |                                +----(Chapter 5 Pipeline)----> [Audio Tokens]
       |
       +----(PySceneDetect)----> [Scene Boundaries]
       |
       +----(FFmpeg @ 6fps)----> [Frame Samples]
       |
       +----(easyocr/PaddleOCR)----> [Frame-level OCR Text]
       |
       +----(Subtitle Parser)----> [Timestamped Subtitles]
       |
       V
[Scene Clip] -> [Quality Scorer] -> [GOOD Clips] -> [Frames for Discretizer]
```

**1. 镜头切分 (Scene Detection):**
直接处理长视频效率低下且上下文可能不相关。我们使用 `PySceneDetect` 或基于内容的哈希算法进行镜头切分。
*   **策略**: 基于内容的检测器（`ContentDetector`）能有效识别场景切换，比固定时长的切块如每 10 秒一切）更能保留语义完整性。
*   **产出**: 一系列带有时间戳的视频片段（clips），每个片段代表一个连贯的视觉场景。

**2. 帧率策略 (Frame Rate Strategy):**
这是一个关键的成本与质量的权衡。过高的帧率会产生大量冗余信息，急剧增加 token 数量；过低则会丢失关键的动态信息。
*   **我们的选择**: **6 fps (每秒 6 帧)**。
*   **理由**:
    *   **信息量**:足以捕捉大多数人类活动和常见的物体运动。
    *   **冗余度**: 相较于原始的 24/30 fps，大大减少了相邻帧的相似度。
    *   **Token 预算**: 这是一个经过计算的折衷。根据 `index.md` 的规划，每帧离散化为约 256 个 token，6 fps 意味着每秒视频产生 `6 * 256 = 1536` 个 token。这使得我们在 2.5T token 的总预算下，可以容纳约 45 万小时的视频，覆盖足够的多样性。

**3. 元数据抽取与对齐:**
*   **音频**: 将视频中的音轨分离出来，送入第 5 章描述的音频处理流水线，生成音频 token 和 ASR 文本。
*   **字幕**: 解析下载的 `.vtt` 字幕文件，得到带时间戳的文本。
*   **OCR**: 对采样的帧（可降低频率，如每秒 1 帧）运行 OCR 模型（如 `PaddleOCR`），提取画面中的文字。这对于教程、课件类视频至关重要。

**4. 质量打分 (Quality Scoring):**
并非所有视频片段都有价值。我们设计一个启发式打分函数来过滤低质量内容：
$Score = w_1 \cdot \log(\text{Resolution}) + w_2 \cdot \text{Clarity} - w_3 \cdot \text{MotionBlur} + w_4 \cdot \text{OCR_Density} + w_5 \cdot \text{Speech_Ratio} - w_6 \cdot \text{Silence_Ratio}$

*   **Clarity**: 可用拉普拉斯算子计算图像的清晰度。
*   **OCR_Density**: 画面中有效 OCR 文本的数量。
*   **Speech_Ratio**: 音轨中语音活动检测（VAD）出的人声占比。
*   **权重 $w_i$**: 根据经验设定，例如，对于教育类视频，可以调高 $w_4$ 和 $w_5$ 的权重
*   **过滤**: 只保留得分高于预设阈值的视频片段。同时，过滤掉片头、片尾、广告等低信息量内容。

### 6.3 核心技术：时空离散化

这是将视频从像素空间转换到离散 token 空间的核心步骤。我们采用 **VQ-VAE (Vector Quantized Variational Autoencoder)** 结合 **RVQ (Residual Vector Quantization)** 的方案。

**1. 视频编码器 (Video Encoder):**
编码器负责将一小块时空区域（tubelet）或单个帧压缩成一个低维的 latent representation。
*   **方案 A (2.5D, 推荐)**: 使用一个强大的预训练 **图像 VQ-VAE** (如 VQGAN 或 dVAE) 作为帧编码器。它将每帧 (例如 `256x256` 分辨率) 编码成一个 latent map (例如 `16x16`)。这种方法实现简单，可以复用强大的图像模型能力。
*   **方案 B (3D)**: 训练一个 3D CNN 或时空 Transformer 作为编码器，它直接处理视频块 (如 `2x16x16` 的 tubelet)，能更好地捕捉局部运动信息，但训练成本更高。

**2. 残差向量量化 (RVQ):**
标准的 VQ 使用单个码本 (codebook)，其表达能力受限于码本大小。RVQ 通过串联多个码本（quantizer）来解决这个问题。
*   **原理**: 输入一个 latent vector $z$。
    1. 第一个量化器找到码本 $C_1$ 中最接近的码字 $c_1$，计算残差 $r_1 = z - c_1$。
    2. 第二个量化器对残差 $r_1$ 进行量化，找到 $C_2$ 中最接近的码字 $c_2$，计算新残差 $r_2 = r_1 - c_2$。
    3. 重复 N 次。最终的量化表示是码字索引的序列 $(idx_1, idx_2, ..., idx_N)$，重构的向量为 $\hat{z} = c_1 + c_2 + ... + c_N$。
*   **优势**: 如果每个码本大小为 `V`，使用 `N` 个 RVQ 层，等效的码本大小是 $V^N$，极大地增强了表示能力，同时参数量仅线性增长。
*   **我们的配置**: 对于每帧 `16x16` 的 latent map，共 256 个 latent vector。对每个 vector 使用 **4-8 级 RVQ** (N=4 to 8)，每级码本大小为 1024 (V=1024)。

**3. Token 序列的生成:**
经过离散化后，一个视频片段被转换成如下格式的 token 序列：

```
<|vid_start|>  <|time_0.00|>  <tok_f0_p0_q0> <tok_f0_p0_q1> ... <tok_f0_p255_qN>
               <|time_0.16|>  <tok_f1_p0_q0> <tok_f1_p0_q1> ... <tok_f1_p255_qN>
               ...
<|vid_end|>
```
*   `<|vid_start|>`/`<|vid_end|>`: 特殊 token，标识视频序列的开始和结束（详见第 8 章）。
*   `<|time_t|>`: （可选）时间戳 token，显式编码时间信息。
*   `tok_f_p_q`: 代表第 `f` 帧，第 `p` 个位置（patch），第 `q` 级 RVQ 的码字索引。这些索引就是最终喂给 Transformer 的 token。
*   **展平 (Flattening)**: 在实践中，我们会将一帧的所有 token (`256 * N` 个) 按空间顺序（如光栅扫描顺序）展平，作为一个整体序列。

### 6.4 存储与成本考量

视频数据处理的成本主要在计算和原始数据存储，而非 token 化后的数据。
*   **原始数据**: 假设 720p 视频码率约为 2 Mbps，45 万小时的视频原始大小约为 `450,000 hours * 2 Mbps / 8 bits/byte * 3600 s/hr ≈ 405 TB`。这是一个巨大的存储挑战。
*   **Token 化数据**:
    *   每小时视频产生的 token 数: `1536 tokens/s * 3600 s/hr = 5,529,600 tokens/hr`
    *   每个 token 使用 `uint16` 存储（2 字节）。
    *   45 万小时视频的 token 存储大小: `450,000 hr * 5.53M tokens/hr * 2 bytes/token ≈ 5.0 TB`。

> **Rule-of-Thumb: 离散化带来巨大存储优势**
> 视频离散化后的存储体积仅为原始文件的 **1-2%** 左右。这极大降低了训练时的数据读取 I/O 压力和存储成本。建议将 token 化的数据存储为 `WebDataset` 或 `TFRecord` 格式，便于高效流式读取。数据的搬运成本也主要体现在原始视频的下载和上传，因此建议将数据处理集群部署在靠近原始数据存储或训练集群的地理位置。

### 本章小结

本章详细阐述了将原始视频转化为模型可用的离散 token 序列的全流程。
1.  **合规先行**: 始终具有明确开放许可（如 CC-BY）的视频源开始，并通过白名单策略保证内容质量。
2.  **精细化预处理**: 自动化流水线是关键，包括基于内容的镜头切分、6 fps 的均衡帧率策略、以及 OCR/ASR 等多模态元数据抽取。
3.  **质量是生命线**: 通过一个可配置的打分函数，系统性地过滤掉低信息密度、低质量的视频片段。
4.  **时空离散化**: 采用 **图像 VQ-VAE + 多级 RVQ** 的方案，将视频帧高效地压缩为离散 token，在保证重建质量的同时，实现了超过 98% 的存储压缩。
5.  **成本意识**: 认识到主要成本在于原始视频的存储和预处理计算，而 token 化后的数据轻便且易于管理。

通过上述步骤，我们成功地将视频这一非结构化、高维度的模态，融入到了统一的自回归建模框架中。

### 常见陷阱与错误 (Gotchas)

1.  **陷阱：忽视合规性审查**
    *   **问题**: 批量下载时，误将“标准 YouTube 可”的视频混入，导致严重的版权风险。
    *   **调试与规避**: 必须在下载前通过 API 严格验证 `videoLicense` 字段。对下载后的数据进行二次抽样和人工审查，建立数据源的可追溯日志。

2.  **陷阱：预处理中的“垃圾”帧泛滥**
    *   **问题**: 视频中大量的黑屏、静态转场、片头片尾被一同采样和编码，浪费了大量 token 预算，并引入噪声。
    *   **调试与规避**: 在质量打分环节，加入简单的图像统计指标，如色彩直方图的方差、帧间像素差的均值。低于特定阈值的帧（如纯黑/白/绿屏）或帧序列应被直接丢弃。

3.  **陷阱：离散化模型泛化能力不足**
    *   **问题**: 用于离散化的 VQ-VAE 在特定类型的视频（如动画、红外图像）上表现很差，导致重建效果崩潰，token 序列失去意义。
    *   **调试与规避**: VQ-VAE 的训练集必须足够多样，覆盖不同风格、领域的图像和视频。在离散化流程中，监控码本的使用率（codebook perplexity），如果某个码本的使用率长期过低，说明模型没有学会有效的表示。

4.  **陷阱：时空信息错位**
    *   **问题**: 字幕、ASR 文本的时间戳与视频帧的时间戳未能精确对齐，导致模型学习到错误的跨模态关联。
    *   **调试与规避**: 统一使用 FFmpeg 等工具提取的精确时间戳。对于 ASR，考虑使用强制对齐（Forced Alignment）技术来获得单词级别的时间戳。在数据打包时，进行交叉验证，确保时间戳在容许的误差范围内（如 +/- 100ms）。

5.  **陷阱：分布式处理中的“长尾效应”**
    *   **问题**: 少数超长或编码有问题的视频文件，导致整个批处理任务长时间卡顿。
    *   **调试与规避**: 在任务分发前，设置一个合理的视频时长上限（如 2 小时）。为每个子任务设置超时（timeout）机制。对处理失败的文件进行隔离和记录，进行专门的离线分析，而不是阻塞主流程。
