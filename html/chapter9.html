<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>模型结构与超参数（1B/10B）</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">index.md（导读与目录）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 1: 总览与技术路线</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">项目管理与里程碑</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第三章：数据配方设计（100T token 池）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 4 章 文本数据：抓取、清洗、去重与质量分层</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 5 章 — 音频数据：合规抓取、ASR 对齐、RVQ 离散化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 6 章 — 视频数据：合规抓取、切片与时空离散化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章 — 图像数据：采集、质量打分与离散化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Tokenizer 与词表：多模扩词、特殊符与对齐</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">模型结构与超参数（1B/10B）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十章：训练基础设施（Megatron @ 256×H100）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 11 章：数据混采与课程策略（10T token 单轮）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十二章：监控与可观测性</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十三章 评测体系：从自回归困惑度到视觉语言指标</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十四章：安全与合规</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十五章：交付与落地（Checkpoint & 蒸馏）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="appendixA.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">附录 A：计算与容量估算（公式与算例）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="appendixB.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">附录 B：工具链与脚手架（配置与模板）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="1b10b">模型结构与超参数（1B/10B）</h1>
<h2 id="91">9.1 开篇与学习目标</h2>
<p>欢迎来到第九章。在本章中，我们将深入探讨模型的心脏——其架构设计与超参数配置。选择正确的结构和参数，是决定模型能力上限、训练稳定性以及最终推理效率的关键。我们将从宏观的架构选型出发，深入到微观的模块组件，最终为我们的 <strong>1B</strong> 和 <strong>10B</strong> 两档生产级模型提供一份详尽的配置蓝图。</p>
<p><strong>学习目标:</strong></p>
<ul>
<li>理解为何选择密集（Dense）自回归解码器（Decoder-only）作为多模态大模型的骨干。</li>
<li>熟悉 RoPE、RMSNorm、SwiGLU 等关键模块的原理与作用。</li>
<li>掌握如何根据目标参数规模（1B/10B）设计模型的具体配置（层数、维度、头数）。</li>
<li>了解训练稳定性的关键超参数设置（优化器、学习率、梯度裁剪）。</li>
<li>学会估算模型在推理阶段的 K/V 缓存开销。</li>
</ul>
<h2 id="92-qwen-dense-decoder-only">9.2 类 Qwen 的 dense decoder-only 主体（早期融合输入）</h2>
<p>对于生成式多模态任务，自回归（Autoregressive）的解码器（Decoder-only）架构已成为事实标准。它天然适合“给定前文，预测下一个 token”的生成范式，无论是文本、图像、还是音频的离散 token，都可以被统一处理。</p>
<p>我们选择的路线是：</p>
<ol>
<li><strong>密集（Dense）模型</strong>: 相较于专家混合（MoE）模型，密集模型在训练和推理的工程实现上更简单、稳定，尤其是在 256x H100 规模下，通信开销更可控，非常适合作为中型企业首次尝试大规模预训练的选择。</li>
<li><strong>类 Qwen 架构</strong>: 我们参考了 Qwen 系列模型的设计，它是一个经过充分验证、性能优异的 Transformer 架构。其核心是标准的 Decoder-only Transformer block 堆叠。</li>
<li><strong>早期融合（Early Fusion）</strong>: 所有模态的数据（文本、图像、音频、视频）在输入模型前，都经过各自的离散化模块（Tokenizer 或 RVQ Encoder）转换成统一的 token 序列。这些序列被拼接在一起，送入 Transformer 的最底层。这种方式允许模型在最早期就开始学习跨模态的深层交互与对齐。</li>
</ol>
<p>其数据流可以简化为如下 ASCII 图所示：</p>
<div class="codehilite"><pre><span></span><code><span class="c">      </span><span class="nb">+----------------+</span><span class="c">   </span><span class="nb">+----------------+</span><span class="c">   </span><span class="nb">+----------------+</span>
<span class="c">      |  Text Tokens   |   |  Image Tokens  |   |  Audio Tokens  |</span>
<span class="c">      </span><span class="nb">+----------------+</span><span class="c">   </span><span class="nb">+----------------+</span><span class="c">   </span><span class="nb">+----------------+</span>
<span class="c">               |                    |                    |</span>
<span class="c">               </span><span class="nb">+--------------------+--------------------+</span>
<span class="c">                                    |</span>
<span class="c">                    </span><span class="k">[</span><span class="c"> </span><span class="nv">&lt;</span><span class="c">bos</span><span class="nv">&gt;</span><span class="nt">,</span><span class="c"> T1</span><span class="nt">,</span><span class="c"> T2</span><span class="nt">,</span><span class="c"> </span><span class="nt">...,</span><span class="c"> </span><span class="nv">&lt;</span><span class="c">img</span><span class="nv">&gt;</span><span class="nt">,</span><span class="c"> I1</span><span class="nt">,</span><span class="c"> </span><span class="nt">...,</span><span class="c"> </span><span class="nv">&lt;</span><span class="c">aud</span><span class="nv">&gt;</span><span class="nt">,</span><span class="c"> A1</span><span class="nt">,</span><span class="c"> </span><span class="nt">...</span><span class="c"> </span><span class="k">]</span>
<span class="c">                                    |</span>
<span class="c">                         </span><span class="nb">+------------------------+</span>
<span class="c">                         | Embedding </span><span class="nb">+</span><span class="c"> Positional |</span>
<span class="c">                         </span><span class="nb">+------------------------+</span>
<span class="c">                                    |</span>
<span class="c">                       </span><span class="nb">+----------------------------+</span>
<span class="c">                       | Transformer Block 1 (Self</span><span class="nb">-</span><span class="c">Attention</span><span class="nt">,</span><span class="c"> FFN) |</span>
<span class="c">                       </span><span class="nb">+----------------------------+</span>
<span class="c">                                    |</span>
<span class="c">                                   </span><span class="nt">...</span>
<span class="c">                                    |</span>
<span class="c">                       </span><span class="nb">+----------------------------+</span>
<span class="c">                       | Transformer Block N (Self</span><span class="nb">-</span><span class="c">Attention</span><span class="nt">,</span><span class="c"> FFN) |</span>
<span class="c">                       </span><span class="nb">+----------------------------+</span>
<span class="c">                                    |</span>
<span class="c">                              </span><span class="nb">+-------------+</span>
<span class="c">                              |  Logits Out |</span>
<span class="c">                              </span><span class="nb">+-------------+</span>
</code></pre></div>

<h2 id="93-ropermsnormswiglukv-cache">9.3 关键模块：RoPE、RMSNorm、SwiGLU、KV Cache</h2>
<p>一个现代化的 Transformer 由几个关键模块构成，它们共同保证了模型的性能与效率。</p>
<ul>
<li>
<p><strong>旋转位置编码 (RoPE - Rotary Position Embedding)</strong>:
    RoPE 是一种相对位置编码方案，它通过在注意力计算中对 Query 和 Key 向量进行旋转操作来引入位置信息。相比于绝对位置编码，RoPE 在长序列上具有更好的外推性，且实现简单，已成为主流选择。
    对于多模态序列，RoPE 统一应用于整个拼接后的 token 序列，使其能够自然地处理不同模态 token 之间的相对位置关系。</p>
</li>
<li>
<p><strong>均方根层归一化 (RMSNorm - Root Mean Square Layer Normalization)</strong>:
    RMSNorm 是 LayerNorm 的一种简化变体。它仅对输入向量进行缩放（scaling），而不进行中心化（centering），计算更高效，同时在实践中被证明同样稳定且有效。
    其计算公式为：
    $$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^{n} x_i^2 + \epsilon}} \cdot \gamma
$$
    其中，$x$ 是输入向量，$n$ 是向量维度，$\epsilon$ 是为防止除零而设置的极小值（如 <code>1e-6</code>），$\gamma$ 是可学习的缩放参数。</p>
</li>
<li>
<p><strong>SwiGLU 激活函数</strong>:
    在前馈网络（FFN）中，我们采用 SwiGLU (Swish-Gated Linear Unit) 激活函数。它相比传统的 ReLU 或 GELU 能够供更好的性能，尽管这会增加 FFN 层的参数量。
    其计算公式为：
    $$
\text{SwiGLU}(x, W, V, b, c) = \text{Swish}(xW + b) \otimes (xV + c)
$$
    其中 $\text{Swish}(x) = x \cdot \sigma(x)$，$\sigma$ 是 Sigmoid 函数，$\otimes$ 表示逐元素相乘。在实践中，通常会有一个 up-projection 和一个 gate-projection，然后再通过一个 down-projection 将维度降回 <code>d_model</code>。</p>
</li>
<li>
<p><strong>键值缓存 (KV Cache)</strong>:
    在自回归生成（推理）时，为了避免重复计算历史 token 的 Key 和 Value 向量，我们会将它们缓存起来。这极大地加速了生成过程，但代价是消耗大量显存。KV Cache 的大小是部署模型时需要考虑的核心因素之一。</p>
</li>
</ul>
<h2 id="94-1b-10b">9.4 尺度配置表：1B 与 10B</h2>
<p>以下是我们为 1B 和 10B 级别模型设计的具体配置。表中也加入了其他常见尺寸作为参考，以展示参数规模的扩展规律。</p>
<p>| 参数规模 | 层数 (N) | 隐藏维 (d_model) | 头数 (n_heads) | 头维 (d_head) | FFN 中间维 | 上下文长度 (L) |</p>
<table>
<thead>
<tr>
<th style="text-align: left;">参数规模</th>
<th style="text-align: left;">层数 (N)</th>
<th style="text-align: left;">隐藏维 (d_model)</th>
<th style="text-align: left;">头数 (n_heads)</th>
<th style="text-align: left;">头维 (d_head)</th>
<th style="text-align: left;">FFN 中间维</th>
<th style="text-align: left;">上下文长度 (L)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>~1.3B</strong></td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">5504 (SwiGLU)</td>
<td style="text-align: left;">8192</td>
</tr>
<tr>
<td style="text-align: left;">~7B</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">4096</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">11008 (SwiGLU)</td>
<td style="text-align: left;">8192</td>
</tr>
<tr>
<td style="text-align: left;"><strong>~10B</strong></td>
<td style="text-align: left;">40</td>
<td style="text-align: left;">4096</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">11008 (SwiGLU)</td>
<td style="text-align: left;">16384</td>
</tr>
<tr>
<td style="text-align: left;">~14B</td>
<td style="text-align: left;">40</td>
<td style="text-align: left;">5120</td>
<td style="text-align: left;">40</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">13696 (SwiGLU)</td>
<td style="text-align: left;">16384</td>
</tr>
<tr>
<td style="text-align: left;">~30B</td>
<td style="text-align: left;">48</td>
<td style="text-align: left;">6656</td>
<td style="text-align: left;">52</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">17920 (SwiGLU)</td>
<td style="text-align: left;">16384</td>
</tr>
</tbody>
</table>
<p><strong>经验法则 (Rule-of-Thumb):</strong></p>
<ul>
<li><strong>头维 (d_head)</strong>: 通常固定为 64, 96 或 128。128 是现代模型中一个非常常见的选择。</li>
<li><strong>隐藏维 (d_model)</strong>: 总是等于 <code>n_heads * d_head</code>。</li>
<li><strong>FFN 中间维</strong>: 对于 SwiGLU，FFN 中间层的维度通常是 <code>d_model</code> 的 2.66 倍左右（例如 Llama 使用 $\frac{2}{3} \cdot 4 \cdot d_{model}$），并向上取整到某个倍数（如 128）以提高计算效率。</li>
<li><strong>层数与宽度</strong>: 增加模型的深度（层数）通常比增加宽度（隐藏维）更能有效地提升性能，直到达到某个瓶颈。</li>
</ul>
<h2 id="95">9.5 位置编码多模对齐与段间屏蔽</h2>
<p>在多模态场景下，我们需要特别处理拼接序列的注意力机制：</p>
<ul>
<li><strong>位置编码对齐</strong>: RoPE 应用于整个序列的绝对位置索引（1, 2, 3, ...）。这意味着模型可以学习到“图像 token 在文本 token 之后”这样的相对关系。对于视频和音频这类带有时序性的模态，其内部的 token 顺序自然地编码了时间流逝，RoPE 会对此进行建模。</li>
<li><strong>段间屏蔽 (Inter-segment Masking)</strong>: 当我们将多个不相关的样本打包（packing）到一个序列以提高训练效率时，必须确保它们在注意力计算中互相隔离。例如，一个序列中包含 <code>[样本A的文本和图像]</code> 和 <code>[样本B的音频]</code>，我们需要修改注意力掩码（attention mask），使得样本 A 的 token 不能关注到样本 B 的 token，反之亦然。这可以防止信息泄漏，保证模型学习到正确的因果关系。</li>
</ul>
<h2 id="96">9.6 优化器与正则</h2>
<ul>
<li>
<p><strong>优化器 (Optimizer)</strong>: 我们选用 <strong>AdamW</strong>。它在 Adam 的基础上解耦了权重衰减（weight decay），使其成为一种真正的 L2 正则化项，而不是与梯度更新混合在一起。</p>
<ul>
<li>$\beta_1 = 0.9$</li>
<li>$\beta_2 = 0.95$ (0.95 在大规模训练中比默认的 0.999 更稳定)</li>
<li>$\epsilon = 10^{-8}$</li>
</ul>
</li>
<li>
<p><strong>正则化 (Regularization)</strong>:</p>
<ul>
<li><strong>权重衰减 (Weight Decay)</strong>: 设置一个较小的值，如 <code>0.1</code>。</li>
<li><strong>Dropout</strong>: 在大规模预训练中，Dropout 的作用存在争议。为简化起见，我们建议将其设置为 <code>0.0</code> 或一个非常小的值（如 <code>0.05</code>），尤其是在数据量足够大的情况下。</li>
<li><strong>标签平滑 (Label Smoothing)</strong>: 可选。可以设置一个较小值如 <code>0.1</code>，有助于提高模型的校准度并防止过拟合，但可能会轻微牺牲困惑度（perplexity）。</li>
</ul>
</li>
</ul>
<h2 id="97">9.7 训练稳定性技巧</h2>
<p>大规模训练中最怕的就是不稳定。以下是保障数周乃至数月的训练顺利进行的生命线：</p>
<ul>
<li>
<p><strong>学习率调度 (Learning Rate Schedule)</strong>:</p>
<ul>
<li><strong>预热 (Warmup)</strong>: 在训练初期，使用一个线性学习率预热阶段，例如 2000 到 5000 个 steps。这有助于模型在开始时稳定收敛。</li>
<li><strong>余弦衰减 (Cosine Decay)</strong>: 预热结束后，学习率按照余弦曲线从峰值衰减到接近零。</li>
<li><strong>峰值学习率 (Peak Learning Rate)</strong>:<ul>
<li><strong>1B 模型</strong>: <code>3.0e-4</code></li>
<li><strong>10B 模型</strong>: <code>1.5e-4</code></li>
</ul>
</li>
<li><strong>Rule-of-Thumb</strong>: 模型越大，最佳的峰值学习率通常越小。</li>
</ul>
</li>
<li>
<p><strong>梯度裁剪 (Gradient Clipping)</strong>:
    这是防止梯度爆炸、维持训练稳定的 <strong>最重要</strong> 技巧。我们将全局梯度的 L2 范数裁剪到一个固定值，通常为 <code>1.0</code>。
    $$
g \leftarrow \min\left(1, \frac{\text{clip_val}}{|G|_2}\right) G
$$
    其中 $G$ 是所有参数的梯度组成的向量，$g$ 是裁剪后的梯度。</p>
</li>
</ul>
<h2 id="98-kv">9.8 推理与 KV 内存估算</h2>
<p>部署模型前，必须精确估算其显存占用，其中 KV Cache 是大头。</p>
<ul>
<li>
<p><strong>KV Cache 内存估算公式</strong>:
    $$
\text{Memory}_{\text{KV}} \approx 2 \times N \times L \times d_{\text{model}} \times \text{precision_bytes} \times \text{batch_size}
$$</p>
<ul>
<li><code>2</code>: 因为要同时缓存 Key 和 Value。</li>
<li><code>N</code>: 模型层数。</li>
<li><code>L</code>: 上下文长度。</li>
<li><code>d_model</code>: 隐藏维度。</li>
<li><code>precision_bytes</code>: 每个浮点数占用的字节数（BF16/FP16 为 2，FP8 为 1）。</li>
<li><code>batch_size</code>:批处理大小。</li>
</ul>
</li>
<li>
<p><strong>算例：10B 模型单样本推理</strong></p>
<ul>
<li>N = 40, L = 16384, d_model = 4096, precision = BF16 (2 bytes), batch_size = 1</li>
<li><code>Memory_KV = 2 * 40 * 16384 * 4096 * 2 * 1 ≈ 10.74 GB</code></li>
<li>这意味着，仅 KV Cache 一项，在满上下文长度时就需要超过 10GB 显存。这是选择部署硬件时的重要依据。</li>
</ul>
</li>
</ul>
<h2 id="99">9.9 本章小结</h2>
<p>本章确定了我们多模态大模型的架构蓝图。我们选择了基于 Transformer 的密集解码器模型，采用 RoPE、RMSNorm 和 SwiGLU 等现代化组件。我们为 1B 和 10B 两种规模提供了详细的配置表，并讨论了 AdamW 优化器、学习率调度和梯度裁剪等关键训练超参数。最后，我们给出了估算 KV Cache 显存占用的实用公式，为后续的部署工作奠定了基础。</p>
<h2 id="910-gotchas">9.10 常见陷阱与错误 (Gotchas)</h2>
<ol>
<li><strong>超参数盲目迁移</strong>: 切忌将小模型（如 1B）上表现良好的学习率或其他超参数直接用于大模型（如 10B）。更大规模的模型通常需要更小的学习率和更长的 warmup。</li>
<li><strong>数值不稳定导致 <code>NaN</code> Loss</strong>: 训练中损失突然变为 <code>NaN</code> 是最常见的问题。99% 的情况源于梯度爆炸。<strong>检查点</strong>：梯度裁剪是否开启？RMSNorm 的 epsilon 是否过小？是否 BF16 模式下混合使用了 FP32 的操作导致溢出？</li>
<li><strong>忽略 KV Cache 的内存爆炸</strong>: 在设计推理服务时，低估了长上下文对 KV Cache 的内存需求。这会导致在高并发或处理长文档时频繁出现 OOM (Out of Memory) 错误。务必使用我们提供的公式进行容量规划。</li>
<li><strong>RoPE 实现错误</strong>: RoPE 的具体实现（例如旋转的 base frequency）细节繁多，微小的错误可能不会导致训练崩溃，但会严重影响模型性能，且难以察觉。建议优先使用成熟框架（如 <code>flash-attn</code>）中经过验证的实现。</li>
<li><strong>词表大小与模型不匹配</strong>: 模型定义的 <code>vocab_size</code> 必须与 Tokenizer 的词表大小 <strong>完全一致</strong>。一个常见的错误是在扩展多模态词表后，只更新了 Tokenizer 文件，而忘记修改模型配置，导致最后一层 <code>lm_head</code> 的维度错误，引发难以调试的 shape mismatch 错误或静默的性能下降。</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter8.html" class="nav-link prev">← Tokenizer 与词表：多模扩词、特殊符与对齐</a><a href="chapter10.html" class="nav-link next">第十章：训练基础设施（Megatron @ 256×H100） →</a></nav>
        </main>
    </div>
</body>
</html>