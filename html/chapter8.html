<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第八章：Tokenizer 与词表：多模扩词、特殊符与对齐标注</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <ul class="nav-list"><li class=""><a href="index.html">index.md（导读与目录）</a></li><li class=""><a href="chapter1.html">Chapter 1: 总览与技术路线</a></li><li class=""><a href="chapter2.html">chapter2.md — 项目管理与里程碑</a></li><li class=""><a href="chapter3.html">chapter2.md — 项目管理与里程碑</a></li><li class=""><a href="chapter4.html">第四章：文本数据——抓取、清洗、去重与质量分层</a></li><li class=""><a href="chapter5.html">第五章：音频数据——合规抓取、质量过滤与 RVQ 离散化</a></li><li class=""><a href="chapter6.html">chapter6.md — 视频数据：合规抓取、切片、时空离散化</a></li><li class=""><a href="chapter7.html">第七章：图像数据——采集、质量打分与离散化</a></li><li class="active"><a href="chapter8.html">第八章：Tokenizer 与词表：多模扩词、特殊符与对齐标注</a></li><li class=""><a href="CLAUDE.html">Untitled</a></li></ul>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="tokenizer">第八章：Tokenizer 与词表：多模扩词、特殊符与对齐标注</h1>
<h2 id="_1">开篇段落</h2>
<p>本章是连接<strong>原始多模态数据</strong>与<strong>Transformer 模型</strong>的桥梁。我们的目标是将图像、音频、视频这些连续的、高维的信号，与离散的文本符号，统一转换成一个模型能够理解的“语言”——一个由离散 token 组成的序列。我们将详细阐述如何基于一个成熟的文本 Tokenizer (Qwen)，通过引入特殊控制符和利用残差矢量量化 (Residual Vector Quantization, RVQ) 技术，构建一个统一的多模态词表。学完本章，你将能够设计并实现一个完整的从原始数据到模型输入 ID 序列的转换流水线，为后续的模型结构设计和训练打下坚实的基础。</p>
<h2 id="81-token">8.1 统一表征的哲学：万物皆 Token</h2>
<p>自回归 Transformer 模型的核心是在一个固定的词汇表上进行一个 token 的预测。为了将这个强大的范式扩展到多模态，最直接的思路就是将所有模态都“翻译”成这个离散词汇表中的符号。这种“万物皆 Token”的哲学有几个核心优势：</p>
<ol>
<li><strong>架构统一</strong>：模型的主体部分可以保持为一个标准的 Decoder-only Transformer，无需为不同模态设计复杂的融合模块或特殊的预测头。整个模型的学习目标被统一为简单的下一个 token 预测，大大降低了工程和研究的复杂度。</li>
<li><strong>早期融合</strong>：在 token 层面进行融合，意味着多模态信息在模型的第一层就可以开始交互。这使得模型能够学习跨模态的底层关联，而不是等到高层特征才开始融合，理论上潜力更大。</li>
<li><strong>生成灵活性</strong>：由于所有模态都表示为 token，模型在生成时可以自然地在不同模态之间切换，例如生成一段文字后，接着生成一段音频的 token，再生成一张图片的 token。</li>
</ol>
<p>我们的技术路是：</p>
<ul>
<li><strong>文本</strong>：直接使用成熟的 BPE (Byte Pair Encoding) Tokenizer。</li>
<li><strong>其他模态（图像/音频/视频）</strong>：通过一个两阶段过程实现离散化。<ol>
<li><strong>编码器 (Encoder)</strong>：一个特定于模态的神经网络（如 ViT 用于图像，1D CNN 用于音频）将原始数据压缩成一系列低维的连续特征向量。</li>
<li><strong>量化器 (Quantizer)</strong>：将连续的特征向量映射到离散的 codebook (码本) 中的索引，这个索引就是我们需要的 token ID。我们将重点采用 RVQ 技术。</li>
</ol>
</li>
</ul>
<p>下面是这个流程的示意图：</p>
<div class="codehilite"><pre><span></span><code><span class="k">[原始模态数据]  -----&gt; [特定模态编码器] -----&gt; [连续特征序列] -----&gt; [RVQ 量化器] -----&gt; [离散 Token ID 序列]</span>
<span class="na">(e.g., Image)        (e.g., ViT)              (e.g., 16x16 grid)       (e.g., 8-stage VQ)      (e.g., 256x8 tokens)</span>
</code></pre></div>

<h2 id="82-qwen-tokenizer">8.2 文本基石：复用并扩展 Qwen Tokenizer</h2>
<p>我们选择直接复用 <strong>Qwen Tokenizer</strong> 作为文本处理的基石，而非从头训练。</p>
<ul>
<li><strong>理由</strong>：Qwen Tokenizer 是一个基于 BPE 的大规模多语言词表（约 15 万），对中英文都做了深度优化，并且经过了万亿级别 token 的验证。复用它可以让我们直接站在巨人的肩膀上，避免在文本侧踩坑。</li>
</ul>
<p>我们的核心工作是<strong>扩展 (Expand)</strong> 这个词表，使其能够“理解”多模态的指令和数据。这需要添加一系列<strong>特殊 token (Special Tokens)</strong>。</p>
<p>| Token 类型           | 示例 Token                                     | 用途说明                                                                                               |</p>
<table>
<thead>
<tr>
<th>Token 类型</th>
<th>示例 Token</th>
<th>用途说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>模态控制符</strong></td>
<td><code>&lt;img&gt;</code>, <code>&lt;/img&gt;</code>, <code>&lt;aud&gt;</code>, <code>&lt;/aud&gt;</code>, <code>&lt;vid&gt;</code>, <code>&lt;/vid&gt;</code></td>
<td>明确标识多模态数据的开始和结束，让模型知道接下来要处理或生成的是哪种模态的 token。</td>
</tr>
<tr>
<td><strong>时间/空间分隔符</strong></td>
<td><code>&lt;seg&gt;</code></td>
<td>(可选) 用于视频或长音频中，标记镜头切换、段落更迭或重要的时间节点，为模型提供结构化提示。</td>
</tr>
<tr>
<td><strong>多模态数据占位符</strong></td>
<td><code>&lt;image_vq1_0&gt;</code> ... <code>&lt;image_vq1_1023&gt;</code></td>
<td>用于图像模态，第一级 RVQ 的 1024 个码本 token。我们将为每个模态的每一级 RVQ 都分配一个专属的 token 范围。</td>
</tr>
<tr>
<td></td>
<td><code>&lt;audio_vq1_0&gt;</code> ... <code>&lt;audio_vq8_2047&gt;</code></td>
<td>用于音频模态，假设有 8 级 RVQ，每级 2048 个码本，就需要 8 * 2048 个新 token。</td>
</tr>
</tbody>
</table>
<p><strong>Rule-of-Thumb: 词表扩展策略</strong></p>
<blockquote>
<p>新增的特殊 token 应被添加到 tokenizer 的词汇表中，并确保它们不会被 BPE 算法切分。在模型侧，需要调用 <code>model.resize_token_embeddings(len(tokenizer))</code> 来扩展词嵌入矩阵。新增的 embedding 向量通常以现有向量的均值进行初始化，以便在训练初期保持稳定。</p>
</blockquote>
<h2 id="83-rvq">8.3 连续模态的离散化：编码器与 RVQ</h2>
<p>这是将连续信号转换为离散 token 的技术核心。对于图像、音频、视频，我们都需要训练一个专用的自编码器模型，其“瓶颈”部分就是一个矢量量化层。</p>
<h3 id="831-encoder">8.3.1 特定模态编码器 (Encoder)</h3>
<ul>
<li><strong>图像</strong>：主流选择是 <strong>Vision Transformer (ViT)</strong> 或 <strong>VQGAN</strong> 的编码器。它将一张 <code>224x224</code> 的图片切分成 <code>16x16</code> 的 patch，每个 patch 经过线性映射和 Transformer 编码后，输出一个 <code>14x14=196</code> 个特征向量的序列。</li>
<li><strong>音频</strong>：通常使用基于 <strong>1D CNN</strong> 的编码器，如 <strong>Encodec</strong> 模型。它通过多层带步长 (stride) 的卷积，将原始音频波形（例如 24kHz 采样率）下采样，输出一个特征序列，其时间分辨率远低于原始波形（例如，每秒输出 75 个特征向量）。</li>
<li><strong>视频</strong>：这是一个活跃的研究领域。一种有效的方法是“伪 3D”，即用 2D 的图像编码器（如 ViT逐帧处理，然后通过一个时间维度的 Transformer 融合。更直接的是使用 <strong>3D CNN</strong> 或 <strong>Tubelet ViT</strong>，将视频切分成时空立方体 (tubelets)，直接编码成特征序列。</li>
</ul>
<h3 id="832-residual-vector-quantization-rvq">8.3.2 残差矢量量化 (Residual Vector Quantization, RVQ)</h3>
<p>标准的矢量量化 (VQ) 试图用一个码本向量来近似一个输入特征向量，当码本不够大时，近似误差（量化误差）会很大，导致信息损失。</p>
<p><strong>RVQ</strong> 通过一种迭代求精的方式解决了这个问题。它使用多级 (stage) 码本，每一级都试图去量化上一级的“残差”（即量化误差）。</p>
<p>假设我们有 $D$ 维的输入向量 $x$，以及 $N$ 个码本 $\{C_1, C_2, ..., C_N\}$，每个码本有 $V$ 个码字 (codeword)。</p>
<ol>
<li>
<p><strong>第一级量化</strong>:
    $$
\hat{x}_1 = C_1(i_1) \quad \text{where} \quad i_1 = \arg\min_{j} |x - C_1(j)|_2
$$
    残差为 $r_1 = x - \hat{x}_1$。</p>
</li>
<li>
<p><strong>第二级量化</strong>:
    $$
\hat{r}_1 = C_2(i_2) \quad \text{where} \quad i_2 = \arg\min_{j} |r_1 - C_2(j)|_2
$$
    新的残差为 $r_2 = r_1 - \hat{r}_1 = x - \hat{x}_1 - \hat{r}_1$。</p>
</li>
<li>
<p><strong>... 第 n 级量化</strong>:
    $$
\hat{r}_{n-1} = C_n(i_n) \quad \text{where} \quad i_n = \arg\min_{j} |r_{n-1} - C_n(j)|_2
$$</p>
</li>
</ol>
<p>最终，原始向量 $x$ 的近似重构为 $\hat{x} = \sum_{n=1}^{N} C_n(i_n)$。而 $x$ 被成功地表示成了一组离散索引 $(i_1, i_2, ..., i_N)$。这组索引就是我们要送给大模型的 <strong>token 序列</strong>。</p>
<div class="codehilite"><pre><span></span><code><span class="w">      </span><span class="n">x</span>
<span class="w">      </span><span class="o">|</span>
<span class="p">[</span><span class="n">Quantize</span><span class="w"> </span><span class="n">w</span><span class="o">/</span><span class="w"> </span><span class="n">C1</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">i_1</span>
<span class="w">      </span><span class="o">|</span>
<span class="w">   </span><span class="n">r_1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">C1</span><span class="p">(</span><span class="n">i_1</span><span class="p">)</span>
<span class="w">      </span><span class="o">|</span>
<span class="p">[</span><span class="n">Quantize</span><span class="w"> </span><span class="n">w</span><span class="o">/</span><span class="w"> </span><span class="n">C2</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">i_2</span>
<span class="w">      </span><span class="o">|</span>
<span class="w">   </span><span class="n">r_2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">r_1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">C2</span><span class="p">(</span><span class="n">i_2</span><span class="p">)</span>
<span class="w">      </span><span class="o">|</span>
<span class="w">      </span><span class="p">...</span>
<span class="w">      </span><span class="o">|</span>
<span class="p">[</span><span class="n">Quantize</span><span class="w"> </span><span class="n">w</span><span class="o">/</span><span class="w"> </span><span class="n">CN</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">i_N</span>
</code></pre></div>

<p><strong>Rule-of-Thumb: RVQ 参数选择</strong></p>
<blockquote>
<ul>
<li><strong>音频</strong>：通常需要较高的保真度。采用 <strong>8-12 级 RVQ</strong>，每级码本大小 <strong>1024-2048</strong> 是一个很好的起点。这可以在保证质量的同时，将音频压缩到约 400-800 token/秒。</li>
<li><strong>图像/视频帧</strong>：对视觉信息的压缩容忍度稍高。采用 <strong>1-4 级 RVQ</strong> 即可。对于单张图像，通常直接用一级 VQ（例如 VQGAN 的码本大小为 16384）或者多级 RVQ（如 4x1024）。我们推荐多级，因为它更灵活。</li>
<li><strong>训练</strong>：这些模态的 VQ-VAE/RVQ-VAE 编码器<strong>必须</strong>在各自的大规模数据集上预训练好，直到达到良好的重构质量。<strong>不要</strong>试图将它们的训练与大语言模型的预训练耦合在一起，这会大大增加训练的不稳定性。</li>
</ul>
</blockquote>
<h2 id="84">8.4 最终输入序列的构建</h2>
<p>当所有组件准备就绪后，我们可以将一段多模态内容转换成一个统一的 token ID 序列。假设我们有一段 "介绍我的新宠物猫 <img>，听听它的叫声 <aud>" 的内容。</p>
<ol>
<li>
<p><strong>文本 Tokenize</strong>:</p>
<ul>
<li><code>"介绍我的新宠物猫"</code> -&gt; <code>[234, 567, 89, 1024, 666]</code> (示例 ID)</li>
<li><code>"，听听它的叫声"</code> -&gt; <code>[11, 2048, 999, 314, 555]</code> (示例 ID)</li>
</ul>
</li>
<li>
<p><strong>图像离散化</strong>:</p>
<ul>
<li><code>cat.jpg</code> -&gt; ViT Encoder -&gt; <code>196</code> 个特征向量</li>
<li>每个特征向量经过 4 级 RVQ (码本大小 1024) -&gt; 得到 <code>196 x 4</code> 个 token ID。</li>
<li><code>img_tokens</code> = <code>[img_vq1_id_1, img_vq2_id_1, img_vq3_id_1, img_vq4_id_1, img_vq1_id_2, ...]</code></li>
</ul>
</li>
<li>
<p><strong>音频离散化</strong>:</p>
<ul>
<li><code>meow.wav</code> (1 秒) -&gt; 1D CNN Encoder -&gt; <code>75</code> 个特征向量</li>
<li>每个特征向量经过 8 级 RVQ (码本大小 2048) -&gt; 得到 <code>75 x 8</code> 个 token ID。</li>
<li><code>aud_tokens</code> = <code>[aud_vq1_id_1, aud_vq2_id_1, ..., aud_vq8_id_1, aud_vq1_id_2, ...]</code></li>
</ul>
</li>
<li>
<p><strong>序列拼接</strong>:
    将上述所有部分用特殊 token 拼接起来。</p>
<p><code>[234, 567, ..., 666]</code> (文本)
<code>+</code>
<code>[&lt;img&gt;_id]</code> (图像开始)
<code>+</code>
<code>[img_tokens]</code> (图像数据，<code>196*4</code> 个 token)
<code>+</code>
<code>[&lt;/img&gt;_id]</code> (图像结束)
<code>+</code>
<code>[11, 2048, ..., 555]</code> (文本)
<code>+</code>
<code>[&lt;aud&gt;_id]</code> (音频开始)
<code>+</code>
<code>[aud_tokens]</code> (音频数据，<code>75*8</code> 个 token)
<code>+</code>
<code>[&lt;/aud&gt;_id]</code> (音频结束)</p>
</li>
</ol>
<p>这就是最终喂给模型的输入序列。模型需要学习的，就是根据前面的 token 序列，预测下一个 token 是什么，无论这 token 是文本、图像、还是音频的一部分。</p>
<p><strong>空间与时间对齐</strong>:</p>
<ul>
<li><strong>图像</strong>: 196 个 patch token 的顺序通常是光栅扫描序（从左到右，从上到下）。模型通过自注意力机制和位置编码来理解它们的 2D 空间关系。</li>
<li><strong>音频/视频</strong>: token 序列天然地与时间对齐。视频中每一帧的 token 序列按时间顺序排列，帧与帧之间也是如此。我们可以插入 <code>&lt;seg&gt;</code> token 来显式地标记镜头切换，这可能有助于模型理解视频的结构。</li>
</ul>
<h2 id="_2">本章小结</h2>
<ul>
<li><strong>核心思想</strong>：采用“万物皆 Token”的哲学，将所有模态统一为离散 token 序列，以适配标准的自回归 Transformer 架构。</li>
<li><strong>文本侧</strong>：复用并扩展一个强大的预训练 Tokenizer (如 Qwen)，通过添加 <code>&lt;img&gt;</code>, <code>&lt;aud&gt;</code>, <code>&lt;/vid&gt;</code> 等特殊控制符来支持多模态。</li>
<li><strong>多模态侧</strong>：使用“编码器 + RVQ”的两阶段方法进行离散化。编码器负责特征提取，RVQ 负责将连续特征高地映射为多级离散 token。</li>
<li><strong>RVQ</strong>：通过逐级量化残差，RVQ 用较小的码本实现了高质量的向量重构，是平衡压缩率和保真度的关键技术。</li>
<li><strong>最终序列</strong>：通过将各模态的 token 序列与特殊控制符交错拼接，构建出喂给模型的统一输入格式，实现了模态的早期融合。</li>
</ul>
<h2 id="gotchas">常见陷阱与错误 (Gotchas)</h2>
<ol>
<li>
<p><strong>陷阱：词表大小失控</strong></p>
<ul>
<li><strong>问题</strong>：为每个模态的每一级 RVQ 都分配独立的 token，如果 RVQ 级数过多或码本过大（例如音频 12 级 * 2048 码本），会导致词表急剧膨胀。一个 30B 模型的词表若从 15 万膨胀到 25 万，仅 embedding 矩阵就会增加数十 GB 的显存占用。</li>
<li><strong>解决方案</strong>：a) 适度减小码本大小或级数；b) 探索<strong>码本共享</strong>，例如，不同模态的 RVQ 共享部分或全部码本。这需要实验验证是否会影响重构质量。c) 采用更先进的量化技术，如 Product Quantization (PQ) 的变体。</li>
</ul>
</li>
<li>
<p><strong>陷阱：离散化器训练不足</strong></p>
<ul>
<li><strong>问题</strong>：用于离散化的 VQ-VAE 或 Encodec 模型没有在对应模态的高质量、大规模数据上充分训练，导致重构效果差（图像模糊、音频有杂音）。这等于给大模型提供了“有损”的、错误的输入。</li>
<li><strong>调试技巧</strong>：建立一个严格的离散化器评估流程。除了重构损失 (MSE)，还要有人工评估和客观指标（如图像的 PSNR/SSIM，音频的 PESQ/STOI）。<strong>只有当离散化器达到“可接受”的保真度后，才应冻结并用于大模型预训练。</strong></li>
</ul>
</li>
<li>
<p><strong>陷阱：特殊 Token 初始化不当</strong></p>
<ul>
<li><strong>问题</strong>：在 <code>resize_token_embeddings</code> 后，新增的 embedding 向量默认为随机初始化。这可能在训练初期引入巨大的梯度，导致不稳定。</li>
<li><strong>解决方案</strong>：将新 token 的 embedding 初始化为现有词表中所有 token embedding 的均值。对于成组的 token（如 <code>&lt;image_vq1_0&gt;</code> 到 <code>&lt;image_vq1_1023&gt;</code>），可以在均值基础上增加少量、独立的正态分布噪声，以帮助模型区分它们。</li>
</ul>
</li>
<li>
<p><strong>陷阱：上下文长度溢出</strong></p>
<ul>
<li><strong>问题</strong>：音频和视频可以轻易产生极长的 token 序列。一段 30 秒的视频，若按 6fps 采样、每帧 256 token，就会产生 <code>30 * 6 * 256 = 46080</code> 个 token，远超当前模型的上下文窗口（8k/16k）。</li>
<li><strong>解决方案</strong>：必须在数据预处理阶段进行<strong>切片或降采样</strong>。例如，将长视频切成有意义的短片断（如 8-16 秒），或者降低帧率、采用更激进的编码器（更大的 patch 或 stride）来减少每秒生成的 token 数。这是一个必须权衡的工程决策：<strong>保真度 vs. 计算可行性</strong>。</li>
</ul>
</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter7.html" class="nav-link prev">← 第七章：图像数据——采集、质量打分与离散化</a><a href="CLAUDE.html" class="nav-link next">Untitled →</a></nav>
        </main>
    </div>
</body>
</html>