<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第 11 章：数据混采与课程策略（10T token 单轮）</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">index.md（导读与目录）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 1: 总览与技术路线</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">项目管理与里程碑</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第三章：数据配方设计（100T token 池）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 4 章 文本数据：抓取、清洗、去重与质量分层</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 5 章 — 音频数据：合规抓取、ASR 对齐、RVQ 离散化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 6 章 — 视频数据：合规抓取、切片与时空离散化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章 — 图像数据：采集、质量打分与离散化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Tokenizer 与词表：多模扩词、特殊符与对齐</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">模型结构与超参数（1B/10B）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十章：训练基础设施（Megatron @ 256×H100）</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 11 章：数据混采与课程策略（10T token 单轮）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十二章：监控与可观测性</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十三章 评测体系：从自回归困惑度到视觉语言指标</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十四章：安全与合规</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十五章：交付与落地（Checkpoint & 蒸馏）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="appendixA.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">附录 A：计算与容量估算（公式与算例）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="appendixB.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">附录 B：工具链与脚手架（配置与模板）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="appendixC.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">附录 C — 常见故障与排障（Runbook）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="11-10t-token">第 11 章：数据混采与课程策略（10T token 单轮）</h1>
<h2 id="111">11.1 开篇与学习目标</h2>
<p>在前面的章节中，我们已经精心设计了数据配方，并完成了对文本、音频、视频、图像四个模态的数据采集、清洗、离散化，构建了一个规模宏大的 100T token 数据池。然而，拥有高质量的数据池只是第一步。如何将这些数据以最高效、最合理的方式“喂”给模型，直接决定了训练的效率、稳定性和最终模型的性能。本章将聚焦于数据“供给侧”的动态策略，即<strong>数据混采（Data Mixing）</strong>与<strong>课程学习（Curriculum Learning）</strong>。</p>
<p>我们将把数据供给过程类比为为一位天才学生（我们的模型）设计一套从易到难、从广到精的学习计划。这个计划不是一成不变的，而是随着学生的成长（模型训练的进展）动态调整的。通过本章的学习，您将掌握：</p>
<ul>
<li><strong>动态采样</strong>：如何使用温度和退火策略，在训练的不同阶段动态调整不同模态、语言和质量域的数据比例。</li>
<li><strong>课程学习</strong>：如何设计上下文长度的扩展课程，以加速早期训练并提升模型处理长序列的能力。</li>
<li><strong>多模拼接与打包</strong>：在自回归框架下，如何将不同模态的离散 token 拼接成统一序列，并应用恰当的注意力屏蔽规则。</li>
<li><strong>数据新鲜度控制</strong>：如何通过再采样和去重，保证模型在单轮（epoch）训练中看到最多样化的数据。</li>
<li><strong>高级策略</strong>：了解基于损失的在线数据重加权等前沿技术，以进一步挖掘数据价值。</li>
</ul>
<p>本章内容是连接“静态数据”与“动态训练”的关键桥梁，是决定大规模预训练成败的核心工程技艺之一。</p>
<h2 id="112">11.2 模态/语言/域的采样温度与退火计划</h2>
<p>一个常见的误区是采用固定的数据混合比例贯穿整个训练过程。例如，始终保持 70% 文本、10% 图像、10% 音频、10% 视频的比例。这种静态策略忽略了模型在不同学习阶段对不同类型数据的需求。一个更优的策略是引入<strong>采样温度（Sampling Temperature）</strong>和<strong>退火计划（Annealing Schedule）</strong>。</p>
<h3 id="_1">采样温度</h3>
<p>假设我们为数据池中的第 <code>i</code> 个数据源（例如，维基百科、YouTube 播客、LAION-5B 等）分配了一个基础权重 $w_i$（详见第 3 章）。在采样时，我们不直接使用 $w_i$，而是通过一个温度参数 $T$ 来调节其采样概率 $P_i$：</p>
<p>$$
P_i = \frac{w_i^{1/T}}{\sum_{j} w_j^{1/T}}
$$</p>
<p>温度 $T$ 的作用如下：</p>
<ul>
<li><strong>$T &gt; 1$</strong>：<strong>平滑分布</strong>。概率分布会变得更加均匀，即使是基础权重 $w_i$ 较低的数据源也有更高的机会被选中。这有助于模型在早期接触到更广泛、更多样的数据，建立对世界的基本认知，防止“视野狭窄”。</li>
<li><strong>$T = 1$</strong>：<strong>原始分布</strong>。采样概率严格正比于基础权重 $w_i$。</li>
<li><strong>$0 &lt; T &lt; 1$</strong>：<strong>锐化分布</strong>。概率分布会向高权重的数据源集中，使得高质量、高价值的数据被更频繁地采样。这有助于模型在训练后期进行“精修”，专注于提升在核心任务上的表现。</li>
</ul>
<h3 id="_2">退火计划</h3>
<p>退火计划是指在整个训练过程中（例如，完成 10T token 的一轮训练）如何动态调整温度 $T$。一个典型的退火计划是<strong>从高温向低温过渡</strong>。</p>
<p><strong>经验法则 (Rule-of-Thumb):</strong></p>
<ul>
<li><strong>初始阶段 (0 ~ 2T tokens)</strong>：设置较高的温度，例如 <strong>$T \in [1.5, 2.0]$</strong>。这鼓励模型探索多样性，特别是对于多模态数据，让模型充分接触各种模态组合和长尾领域。</li>
<li><strong>中期阶段 (2T ~ 7T tokens)</strong>：线性或余弦退火，将温度从高点逐步降低到 <strong>$T \approx 1.0$</strong>。此时，模型已经建立了基础能力，开始按照我们设计的核心数据配方进行学习。</li>
<li><strong>后期阶段 (7T ~ 10T tokens)</strong>：将温度进一步降低到 <strong>$T \in [0.8, 1.0]$</strong>。此时，模型开始收敛，我们需要让它更专注于高质量的数据集（如高质量代码、书籍、学术论文、精选的多模态对齐数据），进行“精加工”。</li>
</ul>
<p>可以为不同维度（模态、语言、质量域）设置独立的退火计划。例如，对于语言分布，我们可能始终希望保持对低资源语言的一定探索性；而对于质量域，我们则明确希望在后期高度聚焦于 A 类数据。</p>
<div class="codehilite"><pre><span></span><code>      ▲ 温度 (T)
 2.0 -┤     ************
      │    <span class="gs">*              *</span>
 1.5 -┤   <span class="gs">*                *</span>
      │  <span class="gs">*                  *</span>
 1.0 -┤ <span class="gs">*                    *</span>****************
      │*
 0.8 -┤                        *
      │
      └──────────────────────────────────► 训练进度 (Tokens)
         0T      2T          7T          10T
         | 高温探索期 |   线性/余弦退火   |   低温精修期  |
</code></pre></div>

<h2 id="113">11.3 长序列比例与上下文扩展课程</h2>
<p>从训练开始就使用完整的上下文长度（例如 32K tokens）是极其低效的。原因有二：</p>
<ol>
<li><strong>计算成本</strong>：Transformer 的自注意力机制计算复杂度是序列长度的平方 ($O(L^2)$)，长序列会极大地拖慢训练速度。</li>
<li><strong>学习效率</strong>：在训练初期，模型尚不具备处理长距离依赖的能力，强行喂给长序列数据，模型也只能学到局部信息，造成计算资源浪费。</li>
</ol>
<p>因此，我们设计一个<strong>上下文扩展课程 (Context Length Extension Curriculum)</strong>。</p>
<p><strong>课程设计:</strong></p>
<ul>
<li>
<p><strong>阶段一：短序列启动 (0 ~ 1T tokens)</strong></p>
<ul>
<li><strong>上下文长度</strong>：2048 或 4096。</li>
<li><strong>目标</strong>：让模型快速学习 token 间的局部依赖关系、基本语法和多模态 token 的局部结构。</li>
<li><strong>优势</strong>：训练速度快，可以使用更大的全局批次大小（Global Batch Size），加速模型参数的初步收敛。</li>
</ul>
</li>
<li>
<p><strong>阶段二：中序列过渡 (1T ~ 4T tokens)</strong></p>
<ul>
<li><strong>上下文长度</strong>：8192。</li>
<li><strong>目标</strong>：逐步培养模型处理更长依赖的能力，例如段落级理解、短视频/音频片段的连贯性。</li>
<li><strong>操作</strong>：在切换上下文长度时，可能需要重置优化器状态或应用一个短暂的学习率预热期，以避免训练震荡。同时，需要关注 RoPE 等位置编码的基频（base frequency）是否需要调整以适应更长的上下文。</li>
</ul>
</li>
<li>
<p><strong>阶段三：全序列训练 (4T ~ 10T tokens)</strong></p>
<ul>
<li><strong>上下文长度</strong>：32768 (或目标最大长度)。</li>
<li><strong>目标</strong>：训练模型掌握长距离依赖、篇章级理解、长视频/音频的全局关联性等复杂能力。</li>
<li><strong>数据</strong>：在此阶段，应有意识地增加数据流中长文档、长视频、长音频的采样比例，确保长上下文窗口得到充分利用。</li>
</ul>
</li>
</ul>
<p>这个课程设计不仅节省了大量的计算资源，也符合认知科学中“循序渐진”的学习规律，让训练过程更稳定、高效。</p>
<h2 id="114-mask">11.4 多模拼接与 Mask 规则</h2>
<p>对自回归模型，所有模态的数据最终都需要被转换成一个一维的离散 token 序列。如何组织这个序列，以及如何应用注意力掩码（Attention Mask），是多模态融合的关键。</p>
<h3 id="_3">多模拼接</h3>
<p>我们使用特殊的 token 来分隔和标识不同模态的数据段。一个典型的多模态序列样本如下：</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">BOS</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">text_start</span><span class="o">|&gt;</span><span class="w"> </span><span class="n">这是一段描述下方图片的文字</span><span class="err">。</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">text_end</span><span class="o">|&gt;</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">img_start</span><span class="o">|&gt;</span><span class="w"> </span><span class="o">[</span><span class="n">IMG_0</span><span class="o">]</span><span class="w"> </span><span class="o">[</span><span class="n">IMG_1</span><span class="o">]</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="o">[</span><span class="n">IMG_255</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">img_end</span><span class="o">|&gt;</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">aud_start</span><span class="o">|&gt;</span><span class="w"> </span><span class="o">[</span><span class="n">AUD_0</span><span class="o">]</span><span class="w"> </span><span class="o">[</span><span class="n">AUD_1</span><span class="o">]</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="o">[</span><span class="n">AUD_1023</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">aud_end</span><span class="o">|&gt;</span><span class="w"> </span><span class="o">[</span><span class="n">EOS</span><span class="o">]</span>
</code></pre></div>

<ul>
<li><code>[BOS]</code> / <code>[EOS]</code>：序列开始和结束符。</li>
<li><code>&lt;|modal_start|&gt;</code> / <code>&lt;|modal_end|&gt;</code>：模态专用分隔符，明确告知模型数据段的类型。这些是我们在第 8 章中添加到词表里的特殊 token。</li>
<li><code>[IMG_i]</code> / <code>[AUD_i]</code>：来自图像/音频 RVQ 编码器的离散 token。</li>
</ul>
<p><strong>Packing 策略</strong>：为了最大化 GPU 利用率，通常会将多个短样本拼接（pack）成一个长的序列，直到达到目标上下文长度不同样本之间用 <code>[EOS]</code> 隔开。在计算损失时，我们会忽略样本间填充部分的 logits。</p>
<h3 id="mask">Mask 规则</h3>
<p>标准的自回归模型使用<strong>因果掩码 (Causal Mask)</strong>，即每个 token 只能注意到它之前的所有 token。</p>
<div class="codehilite"><pre><span></span><code>      t_1  t_2  t_3  t_4
t_1 |  1    0    0    0  |   (t_1 只能看到自己)
t_2 |  1    1    0    0  |   (t_2 能看到 t_1, t_2)
t_3 |  1    1    1    0  |   ...
t_4 |  1    1    1    1  |
</code></pre></div>

<p>在多模态场景下，有时需要更精细的 <strong>段间屏蔽 (Inter-segment Masking)</strong> 规则来引导模型学习特定的跨模态关联，而非“作弊”。</p>
<ul>
<li><strong>场景</strong>：一段文字描述紧随其后的图片。</li>
<li><strong>问题</strong>：在预测图片 token <code>[IMG_j]</code> 时，模型不仅能看到前面的文本，还能看到前面的图片 token <code>[IMG_0]...[IMG_{j-1}]</code>。模型可能会过度依赖图片内部的局部相关性（例如，天空的蓝色像素块后面很可能还是蓝色的），而忽略了文本的指导作用。</li>
<li><strong>解决方</strong>：可以设计一种特殊的 mask，使得图片 token 在被预测时，其注意力<strong>只能</strong>看到前序模态（文本）的 token，而不能看到当前模态（图片）内已经生成的 token。这强迫模型必须依赖文本来生成图片，从而学习更强的图文对齐能力。</li>
</ul>
<p>这是一个高级选项，对于旨在增强“理解”能力的一体化模型尤其有价值。对于纯粹的生成任务，标准的因果掩码通常已足够。</p>
<h2 id="115">11.5 去重后再采样与新鲜度控制</h2>
<p>在第 4-7 章中，我们对整个数据池做了严格的去重。但这还不够。在构建一个 10T token 的训练批次时，由于不同数据源的大小和采样权重差异巨大，某些来自大型语料库（如 Common Crawl）的文档可能会被重复采样多次，而一些来自小型、高质量语料库（如书籍）的文档可能一次也未被选中。这降低了数据的<strong>新鲜度 (Freshness)</strong>。</p>
<p><strong>策略：单轮去重 (Epoch-level Deduplication)</strong></p>
<ol>
<li><strong>生成采列表</strong>：根据当前的动态采样概率（由温度和退火计划决定），生成一个足够大的样本标识符列表，其总 token 数远超 10T（例如，15T）。</li>
<li><strong>对列表去重</strong>：使用 MinHash 或 SimHash 对这个样本列表进行快速的近邻去重。去除那些在本次采样中被重复选中的样本。</li>
<li><strong>截取最终批次</strong>：从去重后的列表中，按顺序取样，直到凑够 10T token 的训练数据。</li>
</ol>
<p>这个过程确保了在单轮训练中，模型看到的绝大多数数据都是独一无二的，极大地提升了数据多样性，有效防止模型对高频数据产生“记忆性”过拟合。</p>
<h2 id="116-loss-based-reweighting">11.6 在线质量重加权（loss-based reweighting）</h2>
<p>这是一项更前沿和实验性的技术，旨在让模型“自主”地发现并关注有价值的数据。</p>
<p><strong>核心思想</strong>：
如果一个数据样本在模型上产生了很高的损失（loss），这通常意味着该样本包含模型尚未学好的“新知识”，或者它与模型当的“世界观”冲突。因此，这些高损失的样本可能具有更高的学习价值。</p>
<p><strong>简易流程</strong>：</p>
<ol>
<li><strong>探测（Probing）</strong>：使用一个较早的、稳定的模型检查点（或者一个独立的、更小的“探测”模型），对下一批待训练的数据进行一次前向传播，计算每个样本的损失。</li>
<li><strong>重加权（Reweighting）</strong>：根据样本的损失值，动态提升高损失样本的采样权重。例如，可以直接将损失值作为权重的乘数。</li>
<li><strong>训练</strong>：使用重加权后的数据分布进行正式的梯度更新。</li>
</ol>
<p><strong>风险与权衡</strong>：</p>
<ul>
<li><strong>噪声放大</strong>：高损失也可能意味着<strong>脏数据或噪声</strong>。直接放大高损失样本的权重，有可能会让模型学习到错误或有害的信息。因此，该技术必须与高质量的数据过滤相结合。</li>
<li><strong>计算开销</strong>：需要额外的前向传播，增加了数据预处理的复杂度与计算成本。</li>
</ul>
<p>在实践中，可以采用一个更温和的策略：只对被判定为高质量（A/B 层）的数据应用基于损失的重加权，并对权重提升设置一个上限，以防极端异常值主导训练。</p>
<h2 id="117">11.7 本章小结</h2>
<p>本章详细阐述了将静态数据池转化为动态、智能的学习流的各项关键策略。这些策略共同构成了一套复杂而精密的“课程”，指导模型高效、稳定地完成大规模预训练。</p>
<ul>
<li><strong>动态采样是核心</strong>：通过<strong>温度</strong>和<strong>退火计划</strong>，我们可以动态调整数据“食谱”，在探索与利用之间取得平衡，让模型在不同阶段学习最适宜的内容。</li>
<li><strong>课程学习提效显著</strong>：<strong>上下文长度课程</strong>通过由短到长的训练，显著加速了早期收敛，并稳定地将模型能力扩展到长序列。</li>
<li><strong>拼接与屏蔽是多模融合的基础</strong>：标准化的<strong>多模态 token 拼接</strong>和灵活的<strong>注意力掩码</strong>规则，是实现跨模态自回归学习的基石。</li>
<li><strong>数据新鲜度至关重要</strong>：通过<strong>单轮去重</strong>，我们确模型在有限的训练时间内接触到尽可能广泛和新颖的数据。</li>
<li><strong>高级技术提供优化空间</strong>：<strong>基于损失的重加权</strong>等策略虽有风险，但为进一步提升训练效率和模型能力提供了可能性。</li>
</ul>
<p>成功实施这些策略，需要数据、模型和 infra 团队的紧密协作，是衡量一个团队大模型工程能力的重要标尺。</p>
<h2 id="118-gotchas">11.8 常见陷阱与错误 (Gotchas)</h2>
<ol>
<li>
<p><strong>陷阱：采用静态混合比例</strong></p>
<ul>
<li><strong>症状</strong>：训练初期，模型在复杂模态（如视频）上收敛缓慢；训练后期，模型在核心能力（如高质量文本和代码）上的提升趋于饱和。</li>
<li><strong>原因</strong>：一成不变的数据配方无法满足模型在不同阶段的学习需求。</li>
<li><strong>规避</strong>：必须设计并实施动态的采样退火计划。</li>
</ul>
</li>
<li>
<p><strong>陷阱：上下文长度切换过于激进</strong></p>
<ul>
<li><strong>症状</strong>：在将上下文长度从 2K 突增到 32K 时，训练损失突然飙升，甚至导致模型发散（NaN loss）。</li>
<li><strong>原因</strong>：模型的位置编码、归一化层统计量以及优化器状态都无法适应序列长度的剧变。</li>
<li><strong>规避</strong>：采用分阶段、渐进式的上下文扩展。在每次切换后，考虑使用短暂的学习率预热或重置优化器状态，让模型平稳过渡。</li>
</ul>
</li>
<li>
<p><strong>陷阱：注意力掩码实现错误</strong></p>
<ul>
<li><strong>症状</strong>：训练损失异常低，但模型在生成任务中表现极差，只会逐字复制或生成无意义的重复序列。</li>
<li><strong>原因</strong>：因果掩码或多模态掩码逻辑出错，导致模型“偷看”到了未来的 token。这是一个非常隐蔽且致命的 bug。</li>
<li><strong>规避</strong>：对数据加载器和模型前向传播中的掩码逻辑进行严格的单元测试。可以构造一个简单的序列，手动验证注意力矩阵是否符合预期。</li>
</ul>
</li>
<li>
<p><strong>陷阱：忽视数据新鲜度，重复“咀嚼”旧数据</strong></p>
<ul>
<li><strong>症状</strong>：模型在某些常见的网页内容上表现出色，但在需要广博知识或处理见模式时能力不足。验证集损失下降缓慢。</li>
<li><strong>原因</strong>：训练数据被少数几个大型语料库中的高频样本主导，模型反复学习相同的内容，导致过拟合。</li>
<li><strong>规避</strong>：实施单轮去重策略，并监控训练过程中实际使用的数据源分布，确保其多样性。</li>
</ul>
</li>
<li>
<p><strong>陷阱：盲目应用基于损失的重加权</strong></p>
<ul>
<li><strong>症状</strong>：模型开始生成一些奇怪、有毒或完全错误的内容，训练稳定性下降。</li>
<li><strong>原因</strong>：将数据噪声（例如，格式错误的文本、损坏的图像）产生的高损失误认为是“知识”，并对其进行了放大。</li>
<li><strong>规避</strong>：只对经过严格清洗和质量分层的高质量数据子集应用此技术，并设置合理的权重上限。始终将此视为一个可选的“增强项”，而非基础流程。</li>
</ul>
</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter10.html" class="nav-link prev">← 第十章：训练基础设施（Megatron @ 256×H100）</a><a href="chapter12.html" class="nav-link next">第十二章：监控与可观测性 →</a></nav>
        </main>
    </div>
</body>
</html>