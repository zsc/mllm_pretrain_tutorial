# 第 4 章 文本数据：抓取、清洗、去重与质量分层

## 4.1 开篇与学习目标

文本数据是多模态模型的基石。尽管我们的目标是构建一个能理解视频、音频和图像的模型，但高质量的文本语料不仅是模型语言能力的核心来源，也是连接和描述其他模态的“通用语”。一个在劣质文本上训练的模型，其推理、逻辑和生成能力将存在根本性缺陷。本章将深入探讨如何从零开始，构建一个万亿（Trillion）级别 token 的高质量文本数据集。我们将覆盖从数据源选择、合规抓取，到多阶段清洗、去重，再到最终的质量分层和预算分配的全过程。

**学习目标:**

*   掌握大规模文本数据的来源选择与合规性审查。
*   理解从网页抓取原始文本并进行有效解析的技术。
*   学习如何应用多阶段过滤策略（包括启发式、小模型和分类器）来提升数据质量。
*   掌握精确去重和近似去重的核心技术（如 MinHash LSH），并理解其在跨语言场景的应用。
*   能够设计并执行一个具体的数据集级 token 预算方案。

## 4.2 数据来源与合规

选择数据源是数据工程的第一步，它直接决定了数据质量的上限和法律风险的下限。我们的目标是构建一个多样化、高质量且合规的数据池。

**核心原则：**

1.  **质量优先**: 优先选择经过编辑或同行评审的内容，如百科、书籍、学术论文。
2.  **多样性**: 广泛覆盖不同领域，包括新闻、技术博客、论坛讨论、代码等，以增强模型的通用能力。
3.  **合规性**: 严格遵守每个数据源的 `robots.txt` 协议、服务条款（TOS）和内容许可（License）。这是规避法律风险的生命线。

以下是一些主流数据源及其特点：

| 数据源类型 | 示例                                   | 优点                               | 缺点与合规注意事项                                     |
| :----------- | :------------------------------------- | :--------------------------------- | :----------------------------------------------------- |
| **百科**     | Wikipedia (多语言), Baidu Baike        | 结构化、高质量、事实性强           | 领域覆盖有限，内容更新可能滞后                         |
| **书籍**     | Common Crawl Books, Project Gutenberg  | 长篇连贯、语法规范、深度内容       | 版权复杂，需仔细筛选公共领域或开放许可的书籍           |
| **网页**     | Common Crawl (CC), C4, RefinedWeb      | 规模巨大、覆盖面广、时效性强       | 质量参差不齐，含大量广告、导航栏等噪声，需强力清洗     |
| **代码**     | The Stack, GitHub                      | 训练代码生成与理解能力             | 需处理多种编程语言，许可证（GPL, MIT等）需严格跟踪     |
| **新闻**     | 新闻网站 RSS Feeds, 共新闻数据集     | 语法正式、时事性强                 | 可能存在观点偏见，需注意来源平衡                       |
| **论坛/社交**| Reddit, Stack Exchange, 知乎         | 对话性强、口语化、覆盖长尾问题     | 噪声多、毒性内容风险高，用户生成内容（UGC）版权需注意  |
| **学术**     | arXiv, PubMed Central                  | 专业性强、逻辑严密                 | 格式（LaTeX, PDF）解析复杂，领域非常垂直               |

在启动任何抓取任务前，法务团队必须介入，对每个目标来源的许可和TOS进行评估，建立一份“允许抓取清单”。

## 4.3 抓取与解析

将原始网页转化为干净的文本，是一个充满挑战的提取、转换和加载（ETL）过程。

*   **抓取策略**:
    *   **尊重 `robots.txt`**: 这是网络爬虫的“君子协定”。程序必须自动解析并遵守其 `Disallow` 规则。
    *   **速率限制**: 为避免对目标服务器造成大压力（以及被封禁），需设置合理的抓取延迟和并发数。采用分布式爬虫时，IP轮换是标准实践。
    *   **头部信息**: 模拟真实浏览器 `User-Agent`，处理 `Cookies` 和 `session`，以访问需要登录或动态交互的内容。

*   **解析与内容提取**:
    *   **HTML 清洗**: 原始 HTML 充满了标签、脚本和样式。首要任务是移除这些非内容元素。
    *   **模板与锅炉板去除 (Boilerplate Removal)**: 网页中的页眉、页脚、导航栏、广告等属于“锅炉板”内容，对模型训练是纯粹的噪声。可以使用 `trafilatura`, `BeautifulSoup` 等库，结合启发式规则（如文本密度、链接密度）进行剔除。
    *   **动态页面处理**: 对于大量使用 JavaScript 渲染的“单页应用”（SPA），简单的 HTTP 请求无法获取完整内容。此时需要动用如 `Selenium` 或 `Playwright` 这样的无头浏览器。但由于资源消耗巨大，此方法应仅用于关键且无法通过其他方式获取的网站。

一个典型的抓取解析流程如下（ASCII 图）：

```
[URL Queue] -> [Distributed Crawlers] -> [Raw HTML Storage (e.g., S3)]
      ^                  |                      |
      |                  v                      v
[URL Discovery] <- [HTML Parser & Cleaner] -> [Clean Text Storage]
```

## 4.4 语言识别与分布控制

为了实现中英 90%、其他语种 10% 的目标配比，我们需要在文档级别进行可靠的语言识别。

*   **工具选择**:
    *   `fastText` 语言识别模型：速度极快，精度高，支持数百种语言，是工业界大规模处理的首选。
    *   `pycld3`: Google 的紧凑语言检测器3的 Python 封装，同样高效可靠。

*   **实践流程**:
    1.  对每个从上一阶段获得的干净文档，取前 N 个字符（例如 1024）进行语言识别。
    2.  为每个文档打上语言标签（如 `lang_en`, `lang_zh_cn`）和置信度分数。
    3.  根据置信度阈值（如 > 0.8过滤掉无法确定语言的文档。
    4.  在构建最终训练批次时，根据目标语言分布（90/10）对不同语言的文档进行采样。

## 4.5 质量过滤

这是确保最终数据集“营养丰富”而非“充满垃圾”的核心环节。我们采用一个多阶段的过滤漏斗，逐步筛除低质量内容。

**数据质量过滤漏斗 (Data Quality Filtering Funnel):**

1.  **启发式过滤 (Heuristics-based Filtering)**:
    *   **长度过滤**: 移除过短（如 < 200 字符）或过长（如 > 100,000 字符）的文档。
    *   **符号比例**: 计算特殊符号、数字或非字母字符的比例，移除“乱码”或代码片段。
    *   **词汇多样性**: 使用“词元/类型比”（Token-Type Ratio, TTR）过滤重复性极高的文本（如“你好你好你好…”）。
    *   **停用词检测**: 确保文档包含足够比例的停用词，这通常是自然语言的一个标志。

2.  **分类器过滤 (Classifier-based Filtering)**:
    *   **`fastText` 主题/质量分类器**: 这是一个非常有效的“经验法则”。我们可以准备一个小的、高质量的“黄金”语料（如部分 Wikipedia）和一个已知的低质量语料（如从 CC 中抽取的含成人、垃圾广告内容的样本），然后训练一个 `fastText` 二分类模型。用这个模型为所有文档打一个质量分，并设定阈值。
    *   **毒性/仇恨言论/PII 检测**: 使用预训练的文本分类模型（如 `Jigsaw` 挑战赛的模型）或内部模型，识别并移除或标记化处理有害内容。对于个人身份信息（PII），可使用正则表达式和命名实体识别（NER）模型进行检测和脱敏。

3.  **小模型困惑度过滤 (Small Model Perplexity Filtering)**:
    *   **原理**: 使用一个中等规模（如 1B-3B）的预训练语言模型（例如 GPT-2 或一个早期的内部模型）来计算每个文档的困惑度（Perplexity, PPL）。
    *   **直觉**: PPL 衡量了模型对一段文本的“讶程度”。高质量、流畅的自然语言（如维基百科）通常具有较低的 PPL；而机器生成的、无意义的或语法错误的文本则具有较高的 PPL。
    *   **操作**: 对每个语言，按 PPL 从低到高对所有文档排序，移除 PPL 最高的 5%-10% 的文档。

## 4.6 去重

重复的数据会使模型在训练时“偷懒”，降低其泛化能力，并可能导致在生成时陷入重复循环。

*   **精确去重 (Exact Deduplication)**:
    *   对每个文档进行规范化处理（转小写、去标点、去空白）。
    *   计算文档的哈希值（如 SHA256）。
    *   存储所有哈希值，并丢弃任何哈希值已存在的新文档。这一步可以用哈希集合（HashSet）高效完成。

*   **近似去重 (Near-Deduplication)**:
    *   **挑战**: 许多文档只是略有改动（如新闻稿的不同版本、网页模板的微小变化），精确去重无法捕捉。
    *   **解决方案: MinHash LSH (Locality-Sensitive Hashing)**
        1.  **分词 (Shingling)**: 将文档分解为重叠的 n-grams（例如 5-grams 或 13-grams 的字符序列）。
        2.  **哈希 (Hashing)**: 将每个 shingle 哈希成一个整数。
        3.  **最小哈希 (Min-Hashing)**: 使用 K 个不同的哈希函数，找出每个文档 shingle 集合中经这 K 个函数计算后的 K 个最小值，形成文档的 MinHash 签名（一个 K 维向量）。
        4.  **局部敏感哈希 (LSH)**: 将 K 维签名分段（banding），只要任何一段完全相同，就将两个文档视为“候选对”。
        5.  **验证**: 对所有候选对，计算其真实的 Jaccard 相似度，超过阈值（如 0.8）的即判为重复，只保留其一。
    *   **跨语种去重**: 对于中英文等，可以先将一种语言翻译成另一种（使用低成本的机器翻译API），再进行近似去重；或者使用多语言句子嵌入（如 LaBSE），在向量空间中寻找近邻来识别语义重复。

## 4.7 Token 化、长度布与长序列采样

在完成所有清洗和去重工作后，我们需要将文本数据转化为模型能理解的格式。

*   **Token 化**: 使用项目统一的 Tokenizer（本指南中为 Qwen Tokenizer 的扩展版），将所有文档处理成 token ID 序列。
*   **长度分布分析**: 统计所有文档 token 化后的长度分布。这个直方图至关重要，它能告诉我们：
    *   数据集中长序列（如超过 8K 或 32K token）的比例是否足够。
    *   如果长序列稀缺，我们需要有针对性地补充长文本数据（如书籍、代码库），或者采用文档拼接（concatenation）策略来人为制造长序列。
*   **长序列采样**: 在数据打包（packing）阶段，确保有策略地采样长文档，以训练模型的长上下文理解能力。这可能是一个课程学习策略的一部分（详见第 11 章）。

## 4.8 数据集级 token 预算与表格

将我们宏大的 30T token 文本数据池（假设文本占总多模态 token 的 30%）分解到具体的数据集上。这是一个结合了数据质量、多样性和可用性的艺术。

**文本数据配方示例 (目标：~30T tokens)**

| 数据集名称       | 来源/构建方式                                    | 原始大小 (TB) | 清洗/去重后 Token (Billion) | 占比 (%) | 质量分层 | 许可证/备注                                 |
| :--------------- | :----------------------------------------------- | :------------ | :-------------------------- | :------- | :------- | :------------------------------------------ |
| **Wikipedia (中/英)** | Wikimedia Dumps                                | ~0.1          | 800                         | 2.7%     | A+       | CC-BY-SA 3.0                                |
| **Books (中/英)**   | CC-Books, 自有语料                               | ~2.0          | 4,000                       | 13.3%    | A+       | 公共领域, 需逐一审查版权                    |
| **RefinedWeb (英)** | 从 Common Crawl 精炼                           | ~5.0          | 5,000                       | 16.7%    | A        | OSCC-BY 4.0                                 |
| **C4 (中/英)**      | 从 Common Crawl 清洗                            | ~20.0         | 7,000                       | 23.3%    | B        | ODC-By, 需进一步清洗                      |
| **The Stack (代码)** | 从 GitHub 抓取                                   | ~3.0          | 3,000                       | 10.0%    | A        | 多种开源许可，需跟踪                        |
| **ArXiv (英)**      | arXiv API                                      | ~1.5          | 1,500                       | 5.0%     | A        | arXiv License                               |
| **新闻 (中/英)**    | RSS, 公共数据集                                  | ~10.0         | 3,000                       | 10.0%    | B+       | 需遵守各新闻源 TOS                          |
| **社交/论坛 (中/英)** | Reddit, StackEx, 知乎等                          | ~15.0         | 4,500                       | 15.0%    | B-       | UGC 内容, 需强力过滤毒性和 PII            |
| **其他语种**     | 多源混合 (mC4 等)                                | ~10.0         | 1,200                       | 4.0%     | C        | 作为长尾，支撑多语言能力                    |
| **总计**         | -                                                | **~66.6**     | **30,000 (30T)**            | **100%** | -        | -                                           |

## 4.9 本章小结

本章详细阐述了构建大规模、高质量文本数据预料库的全过程。我们强调了这是一个系统工程，始于合规的数据源选择，经过细致的抓取与解析，再通过一个包括启发式、分类器和模型困惑度在内的多阶段过滤漏斗。我们还讨论了精确和近似去重的必要性与技术实现，最后通过一个具体的 token 预算表将宏观目标落地。高质量的文本数据是模型能力的根本保障，在这环节投入再多的精力也不为过。

## 4.10 常见陷阱与错误 (Gotchas)

1.  **合规性疏忽**: 在项目后期发现早期抓取的数据存在严重的版权或TOS问题，导致需要废弃大量数据，甚至引发法律纠纷。**调试技巧**: 在项目启动时就让法务团队介入，建立数据源白名单制度。
2.  **“脏”数据渗透**: 过滤规则过于宽松，导致大量低质量、模板化或有害内容进入训练集，模型“学坏”了，表现为生成内容逻辑混乱、充满偏见或重复。**调试技巧**: 建立数据抽样审计机制，定期人工检查各阶段的样本，并可视化关键统计指标（如 PPL 分布、语言分布），发现异常立即告警。
3.  **过度清洗**: 过滤规则过于严苛，误伤了大量有价值的非标准文本（如诗歌、代码、方言），导致模型在这些领域的能力缺失。**调试技巧**: 在设置过滤规则时，不仅要看被“丢弃”的数据，也要抽样检查，确没有“错杀”。对特定领域（如代码）应采用专门的清洗策略。
4.  **去重不彻底**: 仅使用精确去重，忽略了大量近似重复的内容。这会导致评测指标虚高（因为模型记住了评测集中的相似样本），实际泛化能力差。**调试技巧**: 必须实施近似去重。定期计算训练数据与评测集、验证集之间的 n-gram 重叠率或 MinHash 相似度，确保没有数据泄漏。
5.  **PII 脱敏失败**: PII 检测与移除方案存在漏洞，导致模型记忆并可能泄露用户隐私。**调试技巧**: 使用更强的 PII 检测工具，并进行红队测试，主动探查模型是否会泄露在训练数据中见过的敏感信息。对高风险数据源进行更严格的匿名化处理。
6.  **Token 化不一致**: 在数据处理和模型训练阶段使用了不同版本或配置的 Tokenizer，导致灾难性的结果。**调试技巧**: 将 Tokenizer 的版本和配置文件作为项目核心资产进行严格的版本控，确保所有流程使用唯一的、确定的 Tokenizer。
