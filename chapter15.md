# 第十五章：交付与落地（Checkpoint & 蒸馏）

## 15.1 开篇与学习目标

欢迎来到本指南的最后一章。经过数据准备、模型设计、基础设施搭建和漫长的训练周期，我们终于得到了一个强大的多模态预训练模型。然而，训练的结束仅仅是应用的开始。本章将聚焦于“最后一公里”：如何将训练产出的 Checkpoint 转化为可部署、可使用、可优化的生产级资产。我们将探讨从交付物打包、推理流程，到高级优化技术（如 Gemma 式 logits 蒸馏和量化），最终到负责任的模型发布。

**学习目标：**

*   理解一个完整的多模态大模型交付物包含哪些核心组件。
*   掌握多模态模型的端到端推理与批处理流程。
*   评估 Gemma 式 logits 蒸馏方案的可行性、收益与成本。
*   了解主流的模型量化与加速技术，及其在多模态模型上的应。
*   学会编写规范的模型卡（Model Card）与数据卡（Data Card），确保模型的可追溯性与透明度。

## 15.2 交付物打包：Checkpoint、Tokenizer、离散化器版本

一个可用的模型不仅仅是一个 `.pth` 或 `.safetensors` 文件。一个健壮的交付包必须包含所有重构其行为所必需的组件，并严格进行版本锁定。任何一个组件的版本错配都可能导致灾难性的、难以调试的错误。

一个典型的多模态模型交付包应包含以下内容：

1.  **模型权重 (Model Checkpoint)**：
    *   **格式**：推荐使用 `safetensors`，它比 `pickle` 更安全，加载速度更快。同时提供原始的 Megatron-LM 格式，以备继续训练或进行框架内调试。
    *   **精度**：通常以 BF16 或 FP16 格式发布。这是训练时的原生精度，能保证最佳性能。
    *   **分片**：对于 10B 这样的大模型，权重文件通常是分片的（例如，按张量并行度切分）。需要提一个索引文件（`pytorch_model.bin.index.json`）来描述如何重组这些分片。

2.  **分词器 (Tokenizer)**：
    *   **文件**：包含 `tokenizer.json`, `vocab.json`, `merges.txt` 等定义 BPE 词表和规则的文件。
    *   **配置**：`tokenizer_config.json` 文件，其中定义了所有特殊 token（如 `<|im_start|>`, `<|im_end|>`）以及我们为多模态添加的 `<image>`, `<audio>`, `<video>`, `<|seg|>` 等。
    *   **版本锁定**：必须明确指出这是基于哪个版本的 Qwen tokenizer 扩展而来，并附上扩展脚本或词表。

3.  **离散化器 (Discretizers)**：
    *   **组件**：这是多模态模型特有的关键部分。对于图像、音频、视频，都需要提供其对应的 VQ-VAE (或 VQGAN, dVAE) 的权重。这些模型负责将连续的感官信号转换为离散的 token ID。
    *   **版本**：离散化器的版本必须与主模型的词表严格对应。主模型词表中的 `[IMG_TOKEN_0]` 到 `[IMG_TOKEN_8191]` 映射到图像 VQ-VAE 的码本。如果 VQ-VAE 更新了，主模型也必须重新训练或进行适配。

4.  **模型配置文件 (Model Configuration)**：
    *   `config.json` 文件，详细记录了模型的架构超参数，如 `hidden_size`, `num_hidden_layers`, `num_attention_heads`, `vocab_size`, `max_position_embeddings` 等。加载模型时，框架会依据此文件构建正确的网络结构。

**交付包结构 ASCII 图示例：**
```
/my_multimodal_model_10b/
├── checkpoints/
│   ├── model-00001-of-00008.safetensors
│   ├── ...
│   └── model-00008-of-00008.safetensors
├── discretizers/
│   ├── image_vqvae_model.pth
│   ├── audio_encodec_model.pth
│   └── video_vqvae_model.pth
├── tokenizer/
│   ├── tokenizer.json
│   ├── vocab.json
│   ├── merges.txt
│   └── tokenizer_config.json
├── config.json
├── generation_config.json
└── README.md (包含模型卡)
```

**经验法则 (Rule-of-Thumb):**
> 将所有组件（模型权重、Tokenizer、离散化器、配置）视为一个原子单元。使用唯一的版本号或 Git commit hash 来标记整个交付包，而不是各组件独立版本化。

## 15.3 推理与离线批处理

获得交付包后，下一步是让模型工作起来。多模态模型的推理流程比纯文本模型更复杂，因为它涉及到不同模态的预处理和后处理。

**端到端推理流程：**

1.  **输入准备**：接收一个混合了文本、图像、音频、视频的复杂输入。
2.  **多模态编码 (Tokenization)**：
    *   **文本**：使用 `tokenizer` 将文本字符串转换为 token ID。
    *   **图像/视频/音频**：
        a.  对原始数据进行预处理（如 resize, resample, normalization）。
        b.  将预处理后的数据送入对应的**离散化器**（如 `image_vqvae_model.pth`）。
        c.  离散化器的编码器 (encoder) 输出离散的码本索引，这些索引就是该模态的 token ID。
3.  **序列拼接 (Sequence Assembly)**：
    *   将所有模态的 token ID 序列按照用户指定的顺序，并用特殊 token（如 `<image>`, `</image>`）包裹起来，拼接成一个统一的 token ID 序列。
    *   例如：`[TEXT_TOKENS] <image> [IMAGE_TOKENS] </image> [MORE_TEXT_TOKENS]`
4.  **模型前向传播**：将拼接好的序列送入多模态大模型，进行自回归生成。
5.  **多模态解码 (De-tokenization)**：
    *   如果模型生成的是文本 token，直接使用 `tokenizer.decode()` 转换回文本。
    *   如果模型生成的是模态 token（例如，续写一张图片），则将这些 token ID 序列送入对应离散化器的**解码器 (decoder)**，重构出连续的信号（如像素矩阵或波形）。

**离线批处理：**

对于大规模的离线任务（如自动加字幕、内容摘要、数据标注），效率至关重要。
*   **数据并行**：将大量待处理数据分发到多个 GPU/CPU 上，并行执行上述推理流程。
*   **动态批处理 (Dynamic Batching)**：将长度相近的请求组合成一个批次，以最大化 GPU 利用率。这对于处理长度差异巨大的多模态序列尤其重要。
*   **IO 优化**：使用 `WebDataset` 或类似格式，将多模态数据（如视频、音频）和元数据打包存储，减少小文件 IO 开销。在处理前进行预取和缓存。

## 15.4 Gemma 式 logits 蒸馏：可行性、收益与代价

直接部署 10B 规模的模型成本高昂、延迟较大。一个有效的解决方案是训练一个更小的模型（如 1B）来“模仿”大模型的行为，即模型蒸馏。Google 的 Gemma 模型推广了一种基于 logits 的高效蒸馏方法。

**核心思想：**

传统的知识蒸馏通常使用 softmax 后的概率分布作为软标签。Gemma 式蒸馏则直接使用 teacher 模型输出的 **logits**（送入 softmax 前的原始分数）作为目标，让 student 模型去拟合。其损失函数通常是 student 和 teacher logits 之间的均方差 (MSE) 或其他 L_p 范数。

$$
\mathcal{L}_{\text{distill}} = \frac{1}{N \cdot V} \sum_{i=1}^{N} \sum_{j=1}^{V} (l_{i,j}^{\text{student}} - l_{i,j}^{\text{teacher}})^2
$$

其中，$N$ 是 batch size，$V$ 是词表大小，$l_{i,j}$ 是第 $i$ 个样本在第 $j$ 个词上的 logit。

**可行性分析：**

*   **技术可行性**：非常高。该技术已在多个顶尖模型中得到验证。实现上，只需将我们训练好的 10B 模型作为 teacher，冻结其参数，然后用它对大量无标签数据进行推理，将其输出的 logits 保存下来，作为 1B student 模型的训练目标。
*   **数据需求**：需要一个大规模、高质量、多样化的“蒸馏数据集”。这个数据集可以是预训练数据的一部分，也可以是专门为下游任务设计的指令数据。

**收益：**

1.  **性能传递**：小模型可以学到大模型推理过程中更细微的“知识”，而不仅仅是最终的 top-1 选择。实验表明，经过 logits 蒸馏 student 模型在多项基准测试上能显著超越同等规模、从头训练的模型。
2.  **成本效益**：产出一个推理成本低（内存、计算、延迟都显著降低）但性能接近大模型的“紧凑版”模型，非常适合边缘计算或实时在线服务。

**代价：**

1.  **计算成本**：最大的成本在于 teacher 模型的推理。要生成足够多的高质量 logits，需要对数万亿 token 的数据进行一次完整的前向传播。这可能需要数百到数千个 H100 GPU·天。
2.  **性能上限**：student 模型的性能理论上无法超越 teacher 模型。蒸馏过程本身也可能引入噪声或偏差。
3.  **实现复杂度**：需要搭建一套可靠的、可扩展的 teacher-student 训练流水线，包括 logits 的生成、存储、读取和对齐。

## 15.5 量化与加速

量化是另一种降低模型部署成本的关键技术，它通过降低权重的数值精度来实现。

1.  **主要技术**：
    *   **Post-Training Quantization (PTQ)**：训练完成后，在不重新训练的情况下对模型进行量化。通常需要一个小的“校准数据集”来确定量化参数（如缩放因子、零点）。
        *   **INT8**：将 FP16/BF16 权重和激活值转换为 8 位整数。这是目前最成熟、效果与性能损失最平衡的方案。
        *   **INT4/FP4**：更激进的量化，可将模型大小压缩近 4 倍。但性能损失风险更高，通常需要更复杂的量化算法（如 GPTQ, AWQ）来保持模型精度。
    *   **Quantization-Aware Training (QAT)**：在训练或微调过程中模拟量化的效应。这通常能获得比 PTQ 更好的性能，但需要额外的训练成本。

2.  **KV 缓存压缩**：
    *   在自回归生成中，Key-Value 缓存是显存消耗大户，尤其是在处理长上下文时。
    *   **对 KV 缓存进行量化**：可以直接将缓存的精度从 FP16 降至 INT8，显著减少显存占用。
    *   **注意力机制变体**：模型设计阶段采 Multi-Query Attention (MQA) 或 Grouped-Query Attention (GQA) 也是从根本上减小 KV 缓存的有效方法（详见第九章）。

**经验法则 (Rule-of-Thumb):**
> 启动时优先尝试 INT8 PTQ，它通常能提供约 2 倍的吞吐量和 50% 的显存节省，且性能下降在 1% 以内。如果性能下降不可接受，再考虑 AWQ/GPTQ 等更复杂的 INT4/FP4 方案或 QAT。

## 15.6 模型卡与数据卡发布

负责任的 AI 开发要求我们对模型的创建过程和能力边界保持透明。模型卡和数据卡是实现这一目标的核心工具。

**模型卡 (Model Card)**：

一份模型卡是一份简明的技术报告，旨在帮助用户理解模型的用途、能力和局限。它应包含：

*   **模型基本信息**：模型名称、版本、发布日期、参数规模、架构。
*   **预期用途**：该模型设计用于哪些任务（如多模态对话、内容生成）？哪些是禁止用途（如制造虚假信息）？
*   **性能指标**：在关键评集上的表现（如第十三章所述的 PPL、VQA 分数等），并与类似模型进行比较。
*   **训练数据**：对训练数据的总体描述（模态、语言、领域分布），并链接到完整的数据卡。
*   **局限性与偏见**：模型已知的弱点是什么？例如，在某些语种或文化背景下表现不佳，可能生成有偏见或不准确的内容。
*   **环境影响**：估算的训练碳足迹。

**数据卡 (Data Card)**：

数据卡专注于解释用于训练和评测的数据集。它应包含：

*   **数据集摘要**：数据集名称、版本、来源、数据量（token 数、小时数、图片数）。
*   **数据采集与处理**：数据是如何收集的（爬取、购买）？遵循了哪些许可协议和 `robots.txt`？经过了哪些清洗、去重和过滤步骤？
*   **数据分布**：语言、主题、人口统计学等维度的详细分布。
*   **隐私与 PII**：采取了哪些措施来识别和移除个人身份信息 (PII)？
*   **许可证**：数据集各组成部分的许可证信息。

## 15.7 本章小结

本章我们走完了从训练完成到模型落地的全过程。一个成功的模型交付不仅是提供权重文件，而是一个包含模型、分词器、离散化器和配置文件的、版本严格锁定的综合软件包。我们探讨了多模态推理的完整流程，并深入分析了两种主流的部署优化策略：**Gemma 式 logits 蒸馏**用于创建高性能的小模型，**量化与加速**技术用于降低现有模型的资源消耗。最后，我们强调了通过**模型卡**和**数据卡**进行透明、负责任发布的重要性。这些步骤确保了我们的巨大投入能够转化为真正有价值、可信赖的 AI 应用。

## 15.8 常见陷阱与错误 (Gotchas)

1.  **版本不匹配灾难**：最常见的错误。加载了新版模型权重，却使用了旧版的 Tokenizer 或离散化器。这会导致模型输入完全错误，输出无意义的内容，且极难调试。**解方案**：强制要求交付包的原子性，使用统一的版本号。
2.  **蒸馏数据污染**：在用于生成 logits 的蒸馏数据集中，不慎混入了下游评测任务的测试集或验证集。这会导致 student 模型在评测时表现虚高，产生误导性结果。**解决方案**：严格划分数据集，对评测集进行哈希校验，确保其未在任何训练或蒸馏数据中出现。
3.  **量化悬崖 (Quantization Cliff)**：对模型应用 PTQ 后，性能急剧下降。这通常发生在模型中存在少量对精度极度敏感的“离群点”激活值。**解决方案**：使用更先进的 PTQ 算法（如 AWQ），或将敏感的层（如第一个和最后一个线性层）排除在量化之外，保持其 FP16 精度。
4.  **无声的解码失败**：模型生成的模态 token 序列在送入离散化器解码器后，产生的是无意义的输出（如纯色图像、静电噪音），但程序不报错。**解决方案**：建立端到端的生成质量监控，对生成的媒体内容进行基本的内容检查（如图像的方差、音频的信噪比）。
5.  **模型卡流于形式**：发布的模型卡信息模糊、避重就轻，没有坦诚地揭示模型的局限性和潜在偏见。这会损害用户信任，并可能带来法律和道德风险。**解决方案**：将模型卡视为产品文档，由专门的团队（包括技术、法务、伦理）共同评审。
