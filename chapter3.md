# 第三章：数据配方设计——构建 100T Token 的多模态数据池

## 开篇段落

数据是预训练模型的基石和灵魂。本章将详细阐述如何设计一个生产级的、规模高达 100 万亿（100T）Token 的多模态数据池，这是我们整个项目的数据“蓝图”。我们将从顶层原则出发，深入探讨数据预算的量化分配、质量分层的重要性、动态采样的策略，以及如何将抽象的数据目标转化为具体的存储和计算需求。完成本章后，您将能够为任何规模的多模态预训练项目设计一份科学、可执行且可观测的数据配方，确保模型在“喂养”阶段就能获得最优的“营养”组合。

## 1. 数据配方的核心原则

一个优秀的数据配方不仅是数据集的简单堆砌，更是对模型预期能力的一种精心规划。它必须遵循下几个核心原则：

*   **质量优先于数量 (Quality over Quantity)**：低质量、重复或有害的数据会严重损害模型的性能和安全性，甚至导致训练过程崩溃。我们的策略是建立一个“数据金字塔”，对数据进行严格分层，优先使用高质量数据，并对低质量数据进行重度清洗或降权。
*   **多样性与覆盖度 (Diversity and Coverage)**：模型的泛化能力直接取决于数据来源的多样性。配方必须覆盖多种模态（文本、音频、视频、图像）、多种语言、多种领域（科技、文学、新闻、对话）、多种格式（长文、代码、表格）和多种风格（正式、口语化）。
*   **能力对齐与数据预算 (Capability Alignment and Data Budgeting)**：特定能力（如代码生成、多语翻译、视频理解）需要对应的数据进行“定向培养”。数据预算的分配过程，就是将训练资源（Token）战略性地投入到我们最看重的模型能力上。
*   **合规与安全先行 (Compliance and Safety First)**：所有数据来源必须严格遵守版权法、许可协议和隐私规定。在配方设计之初，就必须将 PII（个人身份信息）过滤、毒性内容检测和偏见缓解作为基础要求。

## 2. 100T Token 数据池的顶层设计

基于 `index.md` 中的总体规划，我们将 100T Token 的总预算进行如下分解。这里的 Token 是一个统一概念：文本 Token 来自 Qwen BPE tokenizer，而其他模态的 Token 来自其各自的离散化编码器（后接 RVQ）。

### 2.1 模态分布

下表明确了各模态的 Token 预算及其在数据池中的占比。

| 模态 (Modality) | Token 预算 (T) | 占比    | 核心目标与说明                                                                                               |
| :-------------- | :------------- | :------ | :----------------------------------------------------------------------------------------------------------- |
| **文本 (Text)**     | 94.0           | 94.0%   | **知识与推理的基座**。承载了最密集的结构化与非结构化知识，是其他模态语义对齐的“锚点”。                         |
| **音频 (Audio)**    | 2.5            | 2.5%    | **听觉理解与语音生成**。覆盖语音、音乐、环境声，重点在于语音内容（播客、演讲），以培养模型的听觉叙事和对话能力。       |
| **视频 (Video)**    | 2.5            | 2.5%    | **时空动态理解**。包含视觉、音频和时序信息，是培养模型理解物理世界、事件流程和多模态叙事的关键。               |
| **图像 (Image)**    | 1.0            | 1.0%    | **静态视觉理解**。主要以图文对的形式存在，用于建立扎实的视觉概念与文本描述之间的关联。                         |
| **合计**          | **100.0**      | **100%**  |                                                                                                              |

> **经验法则 (Rule-of-Thumb)**
>
> 为什么文本数据占比如此之高（94%）？
> 1.  **信息密度**：文本是人类知识最高效、最浓缩的载体。复杂的科学原理、抽象的哲学思辨、精密的法律条文，都主要以文本形式存在。
> 2.  **对齐基石**：在当前的自回归框架下，模型通过将其他模态“翻译”成类似文本的离散 Token 序列来学习。一个强大、稳固的文本理解基座，对于有效吸收其他模态信息至关重要。
> 3.  **成本效益**：文本数据的获取、清洗和存储成本，相对于同等信息量的视频和音频数据，要低几个数量级。

### 2.2 语言分布

语言分布主要针对包含语义语言的 99T Token（文本 + 音频 + 视频）。图像的 1T Token 不计入语言分布，因其视觉概念具有跨语言的普适性。

| 语言类别         | 目标占比 | 对应 Token 预算 (T) | 主要构成                                   | 战略意义                                     |
| :--------------- | :------- | :------------------ | :----------------------------------------- | :------------------------------------------- |
| **中文 (Chinese)** | ~36%     | ~35.6               | 简体中文为主，覆盖繁体。网页、书籍、百科、社区。 | 核心市场与文化基础，确保母语能力的深度和广度。 |
| **英文 (English)** | ~54%     | ~53.5               | 全球互联网主导语言。科技、学术、金融、娱乐。     | 获取全球最前沿、最广泛的知识，确保科技领先性。 |
| **其他语种**     | ~10%     | ~9.9                | 日/韩/法/德/西/俄/阿 等主要语言。         | 拓展模型的国际化能力和跨文化理解，服务更广泛用户。 |
| **合计**         | **100%** | **99.0**            |                                            |                                              |

## 3. 数据预算：从 Token 到存储的换算

对于 Infra 工程师而言，将抽象的 Token 预算转化为具体的存储（Petabytes）和处理需求是项目规划的第一步。下表提供了估算依据。

| 模态     | Token 预算 (T) | Token 化速率 (近似)                                  | 蕴含原始数据规模 (估算) | 原始数据存储 (估算, PB) |
| :------- | :------------- | :--------------------------------------------------- | :---------------------- | :---------------------- |
| **文本** | 94.0           | ~3-4 字节/Token (UTF-8)                              | ~320 PB 纯文本          | ~0.32                   |
| **音频** | 2.5            | ~400 Token/秒 (24kHz, 8xRVQ)                         | ~174 万小时             | ~5.5 (FLAC 压缩)        |
| **视频** | 2.5            | ~1536 Token/秒 (6 fps, 16x16 patch, 256 token/frame)  | ~45 万小时              | ~18.0 (H.264 压缩)      |
| **图像** | 1.0            | ~256 Token/图                                        | ~39 亿张图片            | ~3.9 (JPEG 压缩 @1MB/图) |
| **总计** | **100.0**      | -                                                    | -                       | **~27.7 PB**            |

> **经验法则 (Rule-of-Thumb)**
>
> 存储规划时，务必考虑 **3-5 倍的存储开销**。除了原始数据，你还需要存储：
> 1.  **中间版本**：清洗、去重、格式转换后的数据。
> 2.  **Token 化数据**：这是训练时直接读取的数据，体积可能比原始数据更大（特别是对于音视频）。
> 3.  **元数据 (Metadata)**：来源、许可、质量分数、哈希值、PII 标签等，对于数据治理和可追溯性至关重要。
> 因此，一个 28 PB 的原始数据池，最终可能需要 **80-140 PB** 的总存储空间。

## 4. 数据分层与质量金字塔

我们不应平等地对待所有数据。引入“数据金字塔”模型，将数据池划分为不同质量等级，以便在后续的采样和训练中实施更精细的策略。

```
      /▲\      <-- 顶层 (Apex / Gold): ~5% Token
     / ▲▲ \       - 教科书、学术论文、专业书籍、高质量百科
    / ▲▲▲▲ \      - 经过专家审核、事实性强、逻辑清晰
   / ▲▲▲▲▲▲ \     - 训练前期用于奠定模型核心知识与推理能力
  /----------\
 / ▲▲▲▲▲▲▲▲▲▲ \   <-- 中层 (Middle / Silver): ~35% Token
/ ▲▲▲▲▲▲▲▲▲▲▲▲ \    - 高质量网页 (RefinedWeb-like)、技术博客、StackExchange
/ ▲▲▲▲▲▲▲▲▲▲▲▲▲▲ \   - 经过严格的启发式规则和模型过滤，信噪比较高
/------------------\
/ ▲...      ...▲▲ \ <-- 底层 (Base / Bronze): ~60% Token
/ ▲▲...    ...▲▲▲ \   - 广泛的 Common Crawl 数据、论坛、社交媒体长文
/ ▲▲▲...  ...▲▲▲▲ \     - 提供了巨大的多样性和长尾知识，但噪声多
/ ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲ \   - 需要最强的清洗和去毒、去重、去 PII 策略
---------------------
```

**应用**：在 `chapter11` 讲述的动态采样中，我们可以为不同层级的数据赋予同的采样权重。例如，在训练初期，`Gold` 层数据的采样率可以被调高（比如权重设为 3.0），以快速构建模型的基础能力；而在训练后期，可以适当提高 `Bronze` 层数据的采样率，以增强模型的鲁棒性和知识广度。

## 5. 动态采样与退火策略

100T 的数据池不会在一次训练中全部用完。对于单次 10T Token 的训练轮次，我们需要设计一个**动态采样**策略，即数据配方在训练的不同阶段会有所侧重。这通常被称为**课程学习 (Curriculum Learning)** 或**退火 (Annealing)**。

一个典型的采样概率公式可以表示为：
$$
P(D_i, t) = \frac{w_i \cdot f_i(t)}{\sum_{j=1}^{N} w_j \cdot f_j(t)}
$$
其中：
- $P(D_i, t)$ 是在训练步数 $t$ 时，从数据源 $D_i$ 采样的概率。
- $w_i$ 是数据源 $D_i$ 的基础采样权重（例如，基于其质量层级）。
- $f_i(t)$ 是一个关于训练步数的退火函数，用于动态调整权重。它可以是分段函数指数衰减函数等。
- $N$ 是数据源的总数。

**示例三阶段退火策略 (应用于 10T 训练轮):**

1.  **阶段一 (0-3T Token, “奠基期”)**:
    *   **目标**: 建立稳固的语言模型基础和核心知识。
    *   **策略**: **提高文本数据，特别是 `Gold` 层文本的采样权重**。例如，文本采样率提升至 85%，音视频采样率相应降低。
2.  **阶段二 (3-7T Token, “融合期”)**:
    *   **目标**: 强化多模态对齐和跨模态理解能力。
    *   **策略**: **恢复到基准配方或略微提高多模态数据权重**。例如，文本 75%，音频 10%，视频 10%，图像 5%。
3.  **阶段三 (7-10T Token, “精化期”)**:
    *   **目标**: 提升对长上下文、复杂指令和特定技能的掌握。
    *   **策略**: **增加长序列数据（长文本、长音视频）和指令微调类数据的权重**，同时可能轻微降低通用网页数据的权重。

这种动态策略使得训练资源在不同阶段能最有效地利用。

## 6. 数据仪表盘：可观测性是关键

设计再好的配方，如果无法监控和度量，也只是纸上谈兵。必须建立一个全面的数据仪表盘来追踪数据池和采样过程中的关键指标。

**需要监控的关键指标 (KPIs)**:

*   **数据池维度**:
    *   各模态/语言/领域的 Token 总量。
    *   数据质量分布：`Gold/Silver/Bronze` 各层级的 Token 数量。
    *   去重率：精确去重和近似去重的比例。
    *   安全指标：PII/毒性/仇恨言论等内容的检出率和浓度。
    *   数据来源与许可分布：确保合规性可追溯。
*   **训练采样维度 (实时)**:
    *   实时采样混合比例：当前 batch 中各数据源的实际 Token 占比是否符合预设的动态策略。
    *   序列长度分布：确保 packing 效率和长下文训练目标的达成。
    *   Loss 尖峰与数据源关联分析：当训练出现不稳时，能快速定位到可能的问题数据源。

这个表盘是数据科学家和 Infra 工程师之间沟通的桥梁，也是整个预训练项目稳定运行的“驾驶舱”。

## 本章小结

*   **数据配方是战略蓝图**: 它决定了模型的“基因”，其核心原则是质量优先、多样性、能力对齐和合规安全。
*   **预算量化是工程基础**: 我们将 100T Token 的宏大目标分解到每个模态和语言，并进一步换算为 PB 级的实际存储需求，为资源规划提供了坚实依据。
*   **质量分层是精细化管理的核心**: 通过构建“数据金字塔”，我们可以实施更高级的采样策略，最大化高质量数据的价值。
*   **动态采样是效率倍增器**: 采用退火策略，在训练的不同阶段动态调整数据配方，可以更高效地构建模型能力。
*   **可观测性是成败关键**: 建立数据仪表盘，对数据池和采样过程进行持续监控，是保障项目顺利进行和快速排错的必要条件。

## 常见陷阱与错误 (Gotchas)

1.  **“越多越好”的误区 (The "More is Always Better" Fallacy)**: 盲目追求 Token 数量而忽视质量是灾难性的。一个混入了 10% 严重噪声数据的 10T 数据集，其效果可能远不如一个经过精细清洗的 8T 数据集。低质数据会拉低模型整体的逻辑和事实性。
2.  **忽视 Token 化成本与存储 (Ignoring Tokenization Costs)**: 尤其是视频和音频，从原始文件到离散 Token 的转换过程是计算密集型的，需要单独规划算力。Token 化后的数据通常无法被高效压缩，其存储体积可能远超你的预期，必须在项目早期就进行精确核算。
3.  **静态配方一成不变 (Static Recipe Rigidity)**: 市场和研究进展日新月异。在长达数月的训练周期中，你可能会发现模型在某些方面的能力不足。一个好的数据配方设计应该允许在中途进行微调（例如，通过调整采样权重），以应对评测中发现的问题。
4.  **语言和文化的“中心化”偏 (Ethnocentric Bias)**: 即使我们设计了 10% 的“其他语种”，如果这些数据仅来自少数几个发达国家，模型仍然会存在巨大的文化偏见。必须有意识地去搜集和平衡代表不同文化区域的数据，即使它们的绝对数量很少。
5.  **“不可能三角”：规模、质量、合规 (The "Impossible Triangle": Scale, Quality, Compliance)**: 在有限的资源和时间内，同时达到极大规模、极高质量和 100% 合规是极其困难的。项目管理者必须清醒地认识到这三者之间的权衡。例如，为了确保完全合规，可能需要放弃一些有价值但许可模糊的数据源，从而影响规模或多样性。早期明确优先级至关重要。
