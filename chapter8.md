# 第八章：Tokenizer 与词表：多模扩词、特殊符与对齐标注

## 开篇段落

本章是连接**原始多模态数据**与**Transformer 模型**的桥梁。我们的目标是将图像、音频、视频这些连续的、高维的信号，与离散的文本符号，统一转换成一个模型能够理解的“语言”——一个由离散 token 组成的序列。我们将详细阐述如何基于一个成熟的文本 Tokenizer (Qwen)，通过引入特殊控制符和利用残差矢量量化 (Residual Vector Quantization, RVQ) 技术，构建一个统一的多模态词表。学完本章，你将能够设计并实现一个完整的从原始数据到模型输入 ID 序列的转换流水线，为后续的模型结构设计和训练打下坚实的基础。

## 8.1 统一表征的哲学：万物皆 Token

自回归 Transformer 模型的核心是在一个固定的词汇表上进行一个 token 的预测。为了将这个强大的范式扩展到多模态，最直接的思路就是将所有模态都“翻译”成这个离散词汇表中的符号。这种“万物皆 Token”的哲学有几个核心优势：

1.  **架构统一**：模型的主体部分可以保持为一个标准的 Decoder-only Transformer，无需为不同模态设计复杂的融合模块或特殊的预测头。整个模型的学习目标被统一为简单的下一个 token 预测，大大降低了工程和研究的复杂度。
2.  **早期融合**：在 token 层面进行融合，意味着多模态信息在模型的第一层就可以开始交互。这使得模型能够学习跨模态的底层关联，而不是等到高层特征才开始融合，理论上潜力更大。
3.  **生成灵活性**：由于所有模态都表示为 token，模型在生成时可以自然地在不同模态之间切换，例如生成一段文字后，接着生成一段音频的 token，再生成一张图片的 token。

我们的技术路是：
*   **文本**：直接使用成熟的 BPE (Byte Pair Encoding) Tokenizer。
*   **其他模态（图像/音频/视频）**：通过一个两阶段过程实现离散化。
    1.  **编码器 (Encoder)**：一个特定于模态的神经网络（如 ViT 用于图像，1D CNN 用于音频）将原始数据压缩成一系列低维的连续特征向量。
    2.  **量化器 (Quantizer)**：将连续的特征向量映射到离散的 codebook (码本) 中的索引，这个索引就是我们需要的 token ID。我们将重点采用 RVQ 技术。

下面是这个流程的示意图：
```ascii
[原始模态数据]  -----> [特定模态编码器] -----> [连续特征序列] -----> [RVQ 量化器] -----> [离散 Token ID 序列]
(e.g., Image)        (e.g., ViT)              (e.g., 16x16 grid)       (e.g., 8-stage VQ)      (e.g., 256x8 tokens)
```

## 8.2 文本基石：复用并扩展 Qwen Tokenizer

我们选择直接复用 **Qwen Tokenizer** 作为文本处理的基石，而非从头训练。

*   **理由**：Qwen Tokenizer 是一个基于 BPE 的大规模多语言词表（约 15 万），对中英文都做了深度优化，并且经过了万亿级别 token 的验证。复用它可以让我们直接站在巨人的肩膀上，避免在文本侧踩坑。

我们的核心工作是**扩展 (Expand)** 这个词表，使其能够“理解”多模态的指令和数据。这需要添加一系列**特殊 token (Special Tokens)**。

| Token 类型           | 示例 Token                                     | 用途说明                                                                                               |
| -------------------- | ---------------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| **模态控制符**       | `<img>`, `</img>`, `<aud>`, `</aud>`, `<vid>`, `</vid>` | 明确标识多模态数据的开始和结束，让模型知道接下来要处理或生成的是哪种模态的 token。                       |
| **时间/空间分隔符**  | `<seg>`                                        | (可选) 用于视频或长音频中，标记镜头切换、段落更迭或重要的时间节点，为模型提供结构化提示。                |
| **多模态数据占位符** | `<image_vq1_0>` ... `<image_vq1_1023>`           | 用于图像模态，第一级 RVQ 的 1024 个码本 token。我们将为每个模态的每一级 RVQ 都分配一个专属的 token 范围。 |
|                      | `<audio_vq1_0>` ... `<audio_vq8_2047>`           | 用于音频模态，假设有 8 级 RVQ，每级 2048 个码本，就需要 8 * 2048 个新 token。                           |

**Rule-of-Thumb: 词表扩展策略**
> 新增的特殊 token 应被添加到 tokenizer 的词汇表中，并确保它们不会被 BPE 算法切分。在模型侧，需要调用 `model.resize_token_embeddings(len(tokenizer))` 来扩展词嵌入矩阵。新增的 embedding 向量通常以现有向量的均值进行初始化，以便在训练初期保持稳定。

## 8.3 连续模态的离散化：编码器与 RVQ

这是将连续信号转换为离散 token 的技术核心。对于图像、音频、视频，我们都需要训练一个专用的自编码器模型，其“瓶颈”部分就是一个矢量量化层。

### 8.3.1 特定模态编码器 (Encoder)

*   **图像**：主流选择是 **Vision Transformer (ViT)** 或 **VQGAN** 的编码器。它将一张 `224x224` 的图片切分成 `16x16` 的 patch，每个 patch 经过线性映射和 Transformer 编码后，输出一个 `14x14=196` 个特征向量的序列。
*   **音频**：通常使用基于 **1D CNN** 的编码器，如 **Encodec** 模型。它通过多层带步长 (stride) 的卷积，将原始音频波形（例如 24kHz 采样率）下采样，输出一个特征序列，其时间分辨率远低于原始波形（例如，每秒输出 75 个特征向量）。
*   **视频**：这是一个活跃的研究领域。一种有效的方法是“伪 3D”，即用 2D 的图像编码器（如 ViT逐帧处理，然后通过一个时间维度的 Transformer 融合。更直接的是使用 **3D CNN** 或 **Tubelet ViT**，将视频切分成时空立方体 (tubelets)，直接编码成特征序列。

### 8.3.2 残差矢量量化 (Residual Vector Quantization, RVQ)

标准的矢量量化 (VQ) 试图用一个码本向量来近似一个输入特征向量，当码本不够大时，近似误差（量化误差）会很大，导致信息损失。

**RVQ** 通过一种迭代求精的方式解决了这个问题。它使用多级 (stage) 码本，每一级都试图去量化上一级的“残差”（即量化误差）。

假设我们有 $D$ 维的输入向量 $x$，以及 $N$ 个码本 $\{C_1, C_2, ..., C_N\}$，每个码本有 $V$ 个码字 (codeword)。

1.  **第一级量化**:
    $$
    \hat{x}_1 = C_1(i_1) \quad \text{where} \quad i_1 = \arg\min_{j} \|x - C_1(j)\|_2
    $$
    残差为 $r_1 = x - \hat{x}_1$。

2.  **第二级量化**:
    $$
    \hat{r}_1 = C_2(i_2) \quad \text{where} \quad i_2 = \arg\min_{j} \|r_1 - C_2(j)\|_2
    $$
    新的残差为 $r_2 = r_1 - \hat{r}_1 = x - \hat{x}_1 - \hat{r}_1$。

3.  **... 第 n 级量化**:
    $$
    \hat{r}_{n-1} = C_n(i_n) \quad \text{where} \quad i_n = \arg\min_{j} \|r_{n-1} - C_n(j)\|_2
    $$

最终，原始向量 $x$ 的近似重构为 $\hat{x} = \sum_{n=1}^{N} C_n(i_n)$。而 $x$ 被成功地表示成了一组离散索引 $(i_1, i_2, ..., i_N)$。这组索引就是我们要送给大模型的 **token 序列**。

```ascii
      x
      |
[Quantize w/ C1] -> i_1
      |
   r_1 = x - C1(i_1)
      |
[Quantize w/ C2] -> i_2
      |
   r_2 = r_1 - C2(i_2)
      |
      ...
      |
[Quantize w/ CN] -> i_N
```

**Rule-of-Thumb: RVQ 参数选择**
> *   **音频**：通常需要较高的保真度。采用 **8-12 级 RVQ**，每级码本大小 **1024-2048** 是一个很好的起点。这可以在保证质量的同时，将音频压缩到约 400-800 token/秒。
> *   **图像/视频帧**：对视觉信息的压缩容忍度稍高。采用 **1-4 级 RVQ** 即可。对于单张图像，通常直接用一级 VQ（例如 VQGAN 的码本大小为 16384）或者多级 RVQ（如 4x1024）。我们推荐多级，因为它更灵活。
> *   **训练**：这些模态的 VQ-VAE/RVQ-VAE 编码器**必须**在各自的大规模数据集上预训练好，直到达到良好的重构质量。**不要**试图将它们的训练与大语言模型的预训练耦合在一起，这会大大增加训练的不稳定性。

## 8.4 最终输入序列的构建

当所有组件准备就绪后，我们可以将一段多模态内容转换成一个统一的 token ID 序列。假设我们有一段 "介绍我的新宠物猫 <img>，听听它的叫声 <aud>" 的内容。

1.  **文本 Tokenize**:
    *   `"介绍我的新宠物猫"` -> `[234, 567, 89, 1024, 666]` (示例 ID)
    *   `"，听听它的叫声"` -> `[11, 2048, 999, 314, 555]` (示例 ID)

2.  **图像离散化**:
    *   `cat.jpg` -> ViT Encoder -> `196` 个特征向量
    *   每个特征向量经过 4 级 RVQ (码本大小 1024) -> 得到 `196 x 4` 个 token ID。
    *   `img_tokens` = `[img_vq1_id_1, img_vq2_id_1, img_vq3_id_1, img_vq4_id_1, img_vq1_id_2, ...]`

3.  **音频离散化**:
    *   `meow.wav` (1 秒) -> 1D CNN Encoder -> `75` 个特征向量
    *   每个特征向量经过 8 级 RVQ (码本大小 2048) -> 得到 `75 x 8` 个 token ID。
    *   `aud_tokens` = `[aud_vq1_id_1, aud_vq2_id_1, ..., aud_vq8_id_1, aud_vq1_id_2, ...]`

4.  **序列拼接**:
    将上述所有部分用特殊 token 拼接起来。

    `[234, 567, ..., 666]` (文本)
    `+`
    `[<img>_id]` (图像开始)
    `+`
    `[img_tokens]` (图像数据，`196*4` 个 token)
    `+`
    `[</img>_id]` (图像结束)
    `+`
    `[11, 2048, ..., 555]` (文本)
    `+`
    `[<aud>_id]` (音频开始)
    `+`
    `[aud_tokens]` (音频数据，`75*8` 个 token)
    `+`
    `[</aud>_id]` (音频结束)

这就是最终喂给模型的输入序列。模型需要学习的，就是根据前面的 token 序列，预测下一个 token 是什么，无论这 token 是文本、图像、还是音频的一部分。

**空间与时间对齐**:
*   **图像**: 196 个 patch token 的顺序通常是光栅扫描序（从左到右，从上到下）。模型通过自注意力机制和位置编码来理解它们的 2D 空间关系。
*   **音频/视频**: token 序列天然地与时间对齐。视频中每一帧的 token 序列按时间顺序排列，帧与帧之间也是如此。我们可以插入 `<seg>` token 来显式地标记镜头切换，这可能有助于模型理解视频的结构。

## 本章小结

*   **核心思想**：采用“万物皆 Token”的哲学，将所有模态统一为离散 token 序列，以适配标准的自回归 Transformer 架构。
*   **文本侧**：复用并扩展一个强大的预训练 Tokenizer (如 Qwen)，通过添加 `<img>`, `<aud>`, `</vid>` 等特殊控制符来支持多模态。
*   **多模态侧**：使用“编码器 + RVQ”的两阶段方法进行离散化。编码器负责特征提取，RVQ 负责将连续特征高地映射为多级离散 token。
*   **RVQ**：通过逐级量化残差，RVQ 用较小的码本实现了高质量的向量重构，是平衡压缩率和保真度的关键技术。
*   **最终序列**：通过将各模态的 token 序列与特殊控制符交错拼接，构建出喂给模型的统一输入格式，实现了模态的早期融合。

## 常见陷阱与错误 (Gotchas)

1.  **陷阱：词表大小失控**
    *   **问题**：为每个模态的每一级 RVQ 都分配独立的 token，如果 RVQ 级数过多或码本过大（例如音频 12 级 * 2048 码本），会导致词表急剧膨胀。一个 30B 模型的词表若从 15 万膨胀到 25 万，仅 embedding 矩阵就会增加数十 GB 的显存占用。
    *   **解决方案**：a) 适度减小码本大小或级数；b) 探索**码本共享**，例如，不同模态的 RVQ 共享部分或全部码本。这需要实验验证是否会影响重构质量。c) 采用更先进的量化技术，如 Product Quantization (PQ) 的变体。

2.  **陷阱：离散化器训练不足**
    *   **问题**：用于离散化的 VQ-VAE 或 Encodec 模型没有在对应模态的高质量、大规模数据上充分训练，导致重构效果差（图像模糊、音频有杂音）。这等于给大模型提供了“有损”的、错误的输入。
    *   **调试技巧**：建立一个严格的离散化器评估流程。除了重构损失 (MSE)，还要有人工评估和客观指标（如图像的 PSNR/SSIM，音频的 PESQ/STOI）。**只有当离散化器达到“可接受”的保真度后，才应冻结并用于大模型预训练。**

3.  **陷阱：特殊 Token 初始化不当**
    *   **问题**：在 `resize_token_embeddings` 后，新增的 embedding 向量默认为随机初始化。这可能在训练初期引入巨大的梯度，导致不稳定。
    *   **解决方案**：将新 token 的 embedding 初始化为现有词表中所有 token embedding 的均值。对于成组的 token（如 `<image_vq1_0>` 到 `<image_vq1_1023>`），可以在均值基础上增加少量、独立的正态分布噪声，以帮助模型区分它们。

4.  **陷阱：上下文长度溢出**
    *   **问题**：音频和视频可以轻易产生极长的 token 序列。一段 30 秒的视频，若按 6fps 采样、每帧 256 token，就会产生 `30 * 6 * 256 = 46080` 个 token，远超当前模型的上下文窗口（8k/16k）。
    *   **解决方案**：必须在数据预处理阶段进行**切片或降采样**。例如，将长视频切成有意义的短片断（如 8-16 秒），或者降低帧率、采用更激进的编码器（更大的 patch 或 stride）来减少每秒生成的 token 数。这是一个必须权衡的工程决策：**保真度 vs. 计算可行性**。
