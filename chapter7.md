# 第 7 章 — 图像数据：采集、质量打分与离散化

## 7.1 开篇与学习目标

欢迎来到第七章。在本章中，我们将深入探讨多模态预训练中至关重要的一环：图像数据的处理。与结构化的文本不同，图像是高维、连续的信号。为了让自回归模型能够像处理文字一样处理图像，我们必须建立一套从原始像素到离散 token 的完整流水线。这个过程不仅是简单的格式转换，更是一系列精细的数据治理、质量控制和表征学习的组合。

本章的学习目标是：
- 掌握构建大规模图像-文本对数据集的核心流程。
- 学会使用小模型对图像进行多维度（相关性、美学、合规性）的质量打分与过滤。
- 理解并能选择合适的去重技术，以应对精确重复和语义重复。
- 深入理解图像离散化（Tokenization）的原理，特别是基于 VQ-VAE/VQGAN 的法。
- 能够为项目制定合理的图像 token 预算、分辨率和裁切策略。

完成本章后，您将有能力设计并实施一个生产级的图像数据预处理流水线，为后续的多模态模型训练提供高质量、格式统一的“视觉语料”。

## 7.2 ALT/TEXT 配对与文本相关性

多模态模型学习视觉概念的基础是海量的图像-文本对（Image-Text Pairs）。其中，网页图片附带的 `alt` 文本（alternative text）是最常见和最丰富的来源。然而，这些文本质量参差不齐，从精确描述到无关的 SEO 关键词，甚至只是“图片”二字。因此，确保图像和文本的语义相关性是数据清洗的第一步，也是最关键的一步。

**核心方法：CLIP Score 过滤**

我们借助一个预训练好的、强大的视觉-语言对齐模型（如 CLIP）来为我们的数据“打分”。

1.  **特征提取**：对于每一个图像-文本对 `(I, T)`，我们分别用 CLIP 的图像编码器 $E_I$ 和文本编码器 $E_T$ 提取特征向量：
    $v_I = E_I(I)$
    $v_T = E_T(T)$

2.  **计算相似度**：计算两个特征向量的余弦相似度，这个值即为“CLIP Score”：
    $S_{CLIP} = \frac{v_I \cdot v_T}{\|v_I\| \|v_T\|}$

3.  **设定阈值过滤**：我们设定一个经验阈值 $\theta$（例如，LAION 数据集使用了 0.28-0.32 范围内的阈值）。只有当 $S_{CLIP} > \theta$ 时，我们才认为这对数据是相关的，予以保留。

**经验法则 (Rule-of-Thumb):**
*   对于通用领域的网页抓取数据，CLIP Score 阈值设在 **0.28 到 0.32** 之间是一个比较均衡的起点。
*   阈值太高会过滤掉许多概念正确但描述方式抽象或不直接的样本（如诗意描述的图片），导致数据多样性下降。
*   阈值太低则会引入大量噪声，例如，图片是“一只猫”，文本是“点击查看更多萌宠图片”，这种样本对模型学习是有害的。

这个步骤能极大地提升数据集的信噪比，是构高质量视觉语言数据集的基石。

## 7.3 审美/主体性/可读性评分（小模型质量打分）

仅仅保证图文相关是不够的。一张低质量的图片（如模糊、过曝、包含大量水印或令人不适的内容）即便配有完美的描述，也会损害模型的学习效果。因此，我们需要一个“图像质量打分”流水线，利用一系列小模型对图像本身进行多维度评估。

这个流水线通常包含以下几个并行的“打分器”：

1.  **美学评分 (Aesthetic Score)**：
    *   **方法**：训练一个分类器或回归器，在人类标注的美学评分数据集（如 [AVA](https://github.com/mtobeiyf/ava_downloader)）上进行训练。该模型输入一张图片，输出一个 1-10 的分数。
    *   **应用**：保留美学评分高于某个阈值（如 5.0）的图片，过滤掉那些构图不佳、模糊、低分辨率的图像。

2.  **主体性/原创性评分 (Originality/Watermark Score)**：
    *   **方法**：训练一个水印检测模型，识别常见的商业图库水印或社交媒体水印。
    *   **应用**：过滤掉水印占比过高的图片，因为它们通常是商业素材，可能引入版权风险，并且水印本身也是一种视觉噪声。

3.  **安全性与合规性评分 (NSFW Score)**：
    *   **方法**：使用一个预训练的 NSFW（Not Safe For Work）分类器，识别色情、暴力、血腥等不适宜内容。
    *   **应用**：这是**必须**的步骤。根据项目需求，严格过滤掉 NSFW 分数高于安全阈值的图片，确保模型和数据的合规性。

4.  **文本可读性评分 (Text Readability Score)**：
    *   **方法**：对图片运行 OCR（光学字符识别），如果识别出的文本是乱码或置信度极低，说明图片中的文字质量不高。
    *   **应用**：过滤掉那些主要内容是文字但文字模糊不清的图片（如低质量的截图）。

**ASCII 系统图：多维度质量过滤流水线**

```
                  +----------------------+
原始图像 --------> |   美学评分模型       | --------> [美学分 > 5.0?] --+
(Image)           +----------------------+                           |
                      |                                              |
                      +----------------------+                           v
                      |   水印检测模型       | --------> [水印占比 < 10%?] --> [通过/过滤]
                      +----------------------+                           ^
                      |                                              |
                      +----------------------+                           |
                      |   NSFW 分类器        | --------> [NSFW分 < 0.1?] --+
                      +----------------------+
```

## 7.4 去重（感知哈希/CLIP 向量近重）

与文本一样，图像数据集中也存在大量的重复，这会浪费计算资源并让模型产生偏见。图像去重分为两个层次：

1.  **精确去重 (Exact Deduplication)**：
    *   **方法**：计算每个图像文件的哈希值（如 SHA256）。哈希值完全相同的图像被认为是精确副本。
    *   **优点**：速度快，准确率 100%。
    *   **缺点**：无法识别那些经过轻微变换（如缩放、裁剪、亮度调整、格式转换）的副本。

2.  **近重/感知去重 (Near-Duplicate Detection)**：
    *   **方法一：感知哈希 (Perceptual Hashing)**
        *   **原理**：如 pHash、dHash、aHash 等算法，它们将图片转换成一个简短的“指纹”（通常是 64 位或 256 位的哈希串）。这些算法的设计使得视觉上相似的图片会产生非常接近的哈希值（汉明距离很小）。
        *   **流程**：计算所有图片的感知哈希值，然后在一个高效的查找结构中，找出汉明距离小于某个阈值（如 4-8）的图片对，只保留其中一张。
        *   **优点**：计算速度较快，对旋转、缩放、水印等有一定鲁棒性。

    *   **法二：语义去重 (Semantic Deduplication)**
        *   **原理**：利用 CLIP 或其他视觉编码器提取的特征向量 $v_I$。视觉上和语义上相似的图片，其特征向量在空间中的距离也会很近。
        *   **流程**：
            a. 为数据集中所有图片提取特征向量。
            b. 使用近似最近邻（ANN）索引库（如 FAISS, ScaNN）构建索引。
            c. 对每个向量进行查询，找到余弦相似度高于某一极高阈值（如 > 0.98）的所有近邻。
            d. 将这些高度相似的图片视为一个簇，每个簇只保留一个代表（如质量最高的那个）。
        *   **优点**：功能最强大，能识别出内容主体相同但拍摄角度、背景、风格略有不同的图片，实现语义级别的去重。
        *   **缺点**：计算成本最高，需要 GPU 进行特征提取和索引构建。

**生产级方案**：通常采用组合策略。先用精确哈希去重，然后用感知哈希进行一轮粗筛，最后对高质量子集采用语义去重，以平衡效果与成本。

## 7.5 离散化：dVAE/VQGAN/VQ-VAE

这是将图像数据喂给自回归模型的“最后一公里”。我们的目标是将一张 `H x W x 3` 的图片，转换成一个一维的、长度为 `N` 的离散 token 序列。这背后最主流的技术是基于向量量化（Vector Quantization）的自编码器。

**核心思想**：
1.  **编码器 (Encoder)**：一个卷积神经网络（CNN），将输入图像（如 `256x256x3`）下采样到一个较小的特征图（如 `16x16x256`）。这个特征图的每个“像素”都是一个描述对应图像块（Patch）的特征向量。
2.  **码本 (Codebook)**：一个可学习的“视觉词典”，包含 `K` 个向量（例如 `K=8192`），每个向量 $c_k$ 称为一个 code。
3.  **量化 (Quantization)**：对于编码器输出的 `16x16` 个特征向量中的每一个，我们都在码本中找到距离它最近的那个 code $c_k$，并用这个 code 的**索引** `k` 来替代它。这个索引 `k` 就是我们的离散视觉 token。
4.  **展平 (Flatten)**：将 `16x16` 的 token 索引网格展平成一个长度为 `256` 的序列。

**ASCII 图解：VQ-VAE 离散化流程**

```
Input Image        Encoder (CNN)      Latent Grid        Quantization (find nearest code)   Token Sequence
(256x256x3)                             (16x16)                                               (len=256)
+---------+         +-------+          +---+---+          +----------+  (Codebook)           [102, 5, 8100, ...]
|         | --(1)-->| CNN   | --(2)--> |v1 |v2 | ... -->  | c1, c2,  | --(3)--> Index Grid --> [..., 43, 12, 1]
|  Cat    |         | Layers|          +---+---+          | ..., ck, |      (16x16)              |
|         |         +-------+          |...|...|          +----------+          |               |
+---------+                            +---+---+                              |--(4) Flatten--+
```

**关键参数与权衡**：
*   **码本大小 (K)**：决定了“视觉词汇”的丰富程度。通常在 4096 到 16384 之间。K 越大，重建图像的保真度越高，但增加了模型词表的负担。
*   **下采样率**：决定了最终 token 序列的长度。例如，对于 `256x256` 的图像，16x 的下采样率（即 patch size 为 16x16）会产生 `(256/16) * (256/16) = 16 * 16 = 256` 个 token。这是目前比较主流的配置。

**生产级选择**：
*   可以直接使用业界开源的、高质量的 VQGAN 或 VQ-VAE 的编码器和码本（如 Stable Diffusion 使用的 VAE Encoder，或 Google 的 SoundStream/ViT-VQGAN）。
*   也可以在自己的高质量图像数据上重新训练一个 VQ-VAE，以使其码本更适应特定领域。

## 7.6 Token 预算、分辨率与裁切策略

**Token 预算**：
在总计 30T token 的训练数据中，图像 token 通常占 10-20% 的比例，即 3T 到 6T tokens。假设平均每张图产生 256 个 token，这意味着我们需要大约 120 亿到 240 亿张高质量的图文对样本。这再次凸显了复用 LAION 等大型开源数据集的重要性。

**分辨率 (Resolution)**：
*   **1B 模型**：可以从较低分辨率开始，如 `224x224` 或 `256x256`。这能有效控制序列长度和计算成本。
*   **10B 模型**：可以支持更高分辨率，如 `336x336` 或 `448x448`。更高的分辨率能让模型看清更多细节，对于 OCR、细粒度识别等任务至关重要。需要注意的是，`448x448` 的图像在 16x下采样率下会产生 `(448/16)^2 = 28*28 = 784` 个 token，序列长度显著增加。

**裁切策略 (Cropping Strategy)**：
*   **训练时**：对于非正方形的图像，通常采用**随机裁切**（Random Cropping）到目标分辨率。这是一种有效的数据增强手段，能提高模型的泛化能力。
*   **推理时**：通常采用**中心裁切**（Center Cropping）或将图像缩放后用 padding 补齐（Letterboxing），以保留最核心的视觉信息。

## 7.7 数据卡与版权标注

对于一负责任的大模型项目，追踪数据来源和处理过程至关重要。

*   **数据卡 (Data Card)**：为每个最终进入训练集的数据点（或其所属的数据集）维护一份元数据记录。内容应包括：
    *   原始来源 URL。
    *   许可证信息（如 Creative Commons, Public Domain 等）。
    *   各阶段的质量分数（CLIP Score, Aesthetic Score, NSFW Score）。
    *   应用的变换（分辨率、裁切方式）。
    *   所属的数据集批次和版本。

*   **版权标注**：严格遵守数据源的 `robots.txt` 和服务条款（TOS）。对于有明确版权限制或权利人要求移除的数据，必须建立快速响应和追溯下架的机制。这是项目合规性的生命线。

## 7.8 本章小结

本章系统地介绍了将原始图像数据转化为模型可用的离散 token 序列的全过程。我们从确保图文相关性的 CLIP Score 过滤开始，接着利用一系列小模型对图像进行美学、安全、原创性等多度的质量评估。随后，我们讨论了如何通过感知哈希和语义向量两种手段进行高效的近重去重。核心步骤是将清理后的图像通过 VQ-VAE 等模型离散化为 token 序列。最后，我们明确了 token 预算、分辨率选择、裁切策略以及数据溯源的重要性。这一套完整的流水线是保障多模态模型视觉能力的基础。

## 7.9 常见陷阱与错误 (Gotchas)

1.  **过度依赖单一质量指标**：
    *   **陷阱**：仅使用 CLIP score 过滤，忽略了图像本身的美学和清晰度，可能导致模型学会将高质量文本与低质量、模糊的图像关联起来。
    *   **调试**：在数据预处理的每个阶段都进行抽样和人工检查。建立一个数据可视化工具，快速浏览被过滤掉或被保留的样本，以校准各个打分模型的阈值。

2.  **离散化模型领域不匹配**：
    *   **陷阱**：使用一个在通用网页图片上预训练的 VQ-VAE，去处理一个特定领域如医学影像、动漫）的数据集。这会导致重建效果差，产生的视觉 token 无法捕捉领域内的关键特征（码本“词不达意”）。
    *   **调试**：在目标领域的一小部分数据上，评估 VQ-VAE 的重建损失和视觉效果。如果效果不佳，考虑在该领域数据上微调 VQ-VAE，甚至重新训练。

3.  **忽视存储和 IO 成本**：
    *   **陷阱**：为每张图片存储原始文件、多种分辨率的副本、CLIP 特征向量、以及最终的 token 序列，导致存储成本爆炸。
    *   **调试**：制定清晰的数据生命周期管理策略。例如，特征向量和 token 序列一旦生成并验证，原始的高清图像可以归档到成本更低的冷存储。在训练时，使用 WebDataset 等格式直接从对象存储流式读取 tokenized 数据，避免在训练集群的本地磁盘上缓存海量文件。

4.  **文本-图像数据泄漏**：
    *   **陷阱**：图像中包含的文字（如截图、海报）可能与评测集中的文本高度重合，导致模型在 OCR 或相关问答任务上表现虚高。这是一种隐蔽的数据泄漏。
    *   **调试**：在构建评测集时，要特别注意这一点。可以对评测集图像运行 OCR，将其中的文本与训练集的文本语料进行 n-gram 或其他近重检测，筛除潜在的泄漏样本。

5.  **去重阈值设置不当**：
    *   **陷阱**：感知哈希或语义去重的阈值设得太宽松，导致许多视觉上不同但主题相似的图片被错误地删除，损害了数据的多样性。设得太严格，则去重效果不佳。
    *   **调试**：这是一个需要反复实验和人工评估的过程。随机抽取一些被判定为“重复”的簇，人工检查其内部相似度是否确实过高。目标是在保留多样性（如不同角度的同一地标）和去除冗余（如完全相同的 meme 图）之间找到平衡。
