# 第七章：图像数据——采集、质量打分与离散化

> **本章目标**：构建一个 **1.0T token** (约 **3.9B 张图片**) 的高质量、已离散化的图像数据集。
> **核心流水线**：合规采集 → 多维度质量过滤 → VQ-VAE 离散化。

---

## 1. 开篇段落

欢迎来到第七章。在本章中，我们将专注于多模态数据中最直观、信息密度最高的一种：**图像**。我们的目标是构建一个庞大（1.0T token）且优质的图像数据集，作为模型视觉理解能力的基石。不同于文本，图像数据的挑战不仅在于“量”，更在于“质”与“对齐”。一张低质量、模糊或与描述毫不相关的图片，对模型来说是噪声而非信号。

本章将引导您完成一个端到端的生产级图像数据处理流水线。我们将从两个主要来源——大规模网页抓取和高质量精选集——出发，设计合规采集策略。随后，我们将深入探讨一个多阶段的质量过滤与打分系统，涵盖从基础元数据、内容安全，到利用CLIP模型进行图文相关性打分等高级技术。最后，我们将详细阐述如何使用向量量化自编码器（VQ-VAE）将图像从连续的像素空间，统一转换为离散的 token 序列，使其能与文本等其他模态无缝拼接，送入自回归 Transformer。完成本章后，您将拥有一套可复现、可扩展的流程，用于生产模型预训练所需的图像“燃料”。

## 2. 文字论述

我们的图像数据策略旨在平衡**广度**与**深度**。根据 `index.md` 的规划，1.0T 图像 token 将由两部分构成：
*   **0.8T token（~3.1B 张图片）**：来自大规模网页图文对，类似 LAION 的自建管线，追求数据的多样性与规模。
*   **0.2T token（~0.8B 张图片）**：来自高质量、强对齐的图文对，如教育、百科、技术插图等，作为模型认知能力的“锚点。

### 2.1 数据采集策略 (Data Collection Strategy)

#### 2.1.1 大规模网页图文对（LAION-like 自建管线）

直接复用 LAION 等现有数据集可能存在合规风险或数据陈旧问题。自建一个轻量级采集管线能提供更大的灵活性与控制力。

**核心流程**:
1.  **数据源**：利用 Common Crawl 的 WARC 文件索引（或 WAT/WET 元数据文件）作为起点。这是互联网网页的公开快照。
2.  **HTML 解析**：使用高效的 HTML 解析库（如 `lxml`, `selectolax`）批量解析网页，提取 `<img>` 标签及其 `alt` 文本属性。
3.  **初步筛选**：在解析阶段就进行初步过滤，只保留包含非空 `alt` 文本、`src` 链接有效，且 `alt` 文本长度在合理范围（如 5-200 个字符）的 `<img>` 标签。
4.  **异步下载**：使用高并发的下载器（如 `aiohttp`）下载图片。设置严格的超时、重试与大小限制（如最大 10MB）。
5.  **合规性检查**：
    *   在下载前，检查目标域名的 `robots.txt`，确保允许爬取。
    *   记录每个图文对的来源 URL，以便追溯与审计。
    *   过滤掉已知受版权严格保护的网站域名（如商业图库）。

**Rule-of-Thumb**:
*   启动约 10-20 台云服务器并行处理 Common Crawl 数据切片，每台配置 32+ vCPU 和高速网络。
*   将原始下载的图片与元数据（URL, alt text）存储在对象存储（如 S3/OSS）中，按来源域名或日期分桶，方便后续管理。

#### 2.1.2 高质量精选数据集

这部分数据旨在提升模型在特定领域的专业性与图文对齐的精确度。
*   **来源**：
    *   **公共科学/教育资源**：维基百科、arXiv (插图与图表)、古登堡计划 (书籍插图)。
    *   **开源数据集**：Conceptual Captions (CC-BY)、SBU Captions、部分 COCO/Flickr30k (需仔细核对许可证)。
    *   **机构合作/自有数据**：如果企业有自有或授权的图片资源（如产品图、教学材料），应优先用。
*   **处理方式**：这些数据集通常较为干净，但仍需通过下文的质量过滤流水线，特别是图文相关性打分，以确保一致性。

### 2.2 质量过滤与打分流水线

这是图像数据处理的核心，一个糟糕的过滤系统会浪费大量算力训练一个“看图说话”能力低下的模型。我们的流水线设计如下：

```
ASCII Art: Image Processing Pipeline

  [Raw Data Batch]
         |
         v
+------------------------+
| 1. 基础元数据过滤      |   (Image Size > 256x256, Aspect Ratio 1:4-4:1, etc.)
+------------------------+
         |
         v
+------------------------+
| 2. 内容安全与合规过滤  |   (NSFW Classifier, Watermark Detection)
+------------------------+
         |
         v
+------------------------+
| 3. 近似去重            |   (pHash/dHash + Similarity Search)
+------------------------+
         |
         v
+------------------------+
| 4. 质量打分            |   (Aesthetic Score, Sharpness Score)
+------------------------+
         |
         v
+------------------------+
| 5. 图文相关性打分      |   (CLIP Score)
+------------------------+
         |
         v
[Scored & Filtered Data] --> (Ready for Discretization)
```

1.  **基础元数据过滤 (Basic Metadata Filtering)**
    *   **分辨率**：过滤掉长或宽小于 256 像素的图片，因为它们在缩放后会损失过多信息。
    *   **长宽比**：剔除极端长宽比的图片（如 > 4:1 或 < 1:4），它们通常是网页横幅或装饰性元素，信息量低。
    *   **文件大小/格式**：剔除损坏的文件和过小的文件（如 < 10KB）。

2.  **内容安全与合规过滤 (Content Safety & Compliance Filtering)**
    *   **NSFW 检测**：使用一个高效的预训练分类器（如基于 EfficientNet 的模型）对图片进行打分，过滤掉 NSFW (Not Safe For Work) 内容。设置一个保守的阈值。
    *   **水印检测**：水印会引入噪声。可以使用基于频域分析（FFT）或专门训练的分类器来识别和过滤带明显水印的图片。

3.  **近似去重 (Near-Duplicate Removal)**
    *   **方法**：使用感知哈希（Perceptual Hashing），如 `pHash` 或 `dHash`。它将图片转换成一个简短的哈希字符串，相似的图片会有相近的哈希值（汉明距离小）。
    *   **实现**：
        1.  为每张图片计算 64-bit 的 pHash 值。
        2.  将所有 pHash 值构建一个高效的索引结构（如 VP-Tree 或存储在支持快速位运算查询的数据库中）。
        3.  遍历所有图片，查询索引中是否存在汉明距离小于某个阈值（如 5）的邻居。如果存在，则只保留其中一张（通常是分辨率最高或质量分最高的）。
    *   **Rule-of-Thumb**: 对于十亿级别的图片库，这个过程需要分布式计算框架（如 Spark/Dask）来完成。

4.  **质量打分 (Quality Scoring)**
    *   **审美分 (Aesthetic Score)**：训练或使用一个预训练的模型来预测图片的“美感”。这类模型通常在包含人类审美评分的数据集上训练，能有效过滤掉构图不佳、光线糟糕的图片。LAION-Aesthetics 是一个很好的起点。
    *   **清晰度分 (Sharpness Score)**：使用如拉普拉斯算子方差（Variance of Laplacian）等经典计算机视觉方法快速评估图片是否模糊。

5.  **图文相关性打分 (Image-Text Relevance Scoring)**
    *   这是至关重要的一步，确保 `alt` 文本真正在描述图片内容。
    *   **方法**：利用预训练的多模态模型，如 **CLIP**。
    *   **公式**：
        $S_{relevance} = \frac{\text{CLIP.encode\_image}(I) \cdot \text{CLIP.encode\_text}(T)}{\|\text{CLIP.encode\_image}(I)\| \cdot \|\text{CLIP.encode\_text}(T)\|}$
    *   **实现**：
        1.  将图片和其对应的 `alt` 文本分别送入 CLIP 的图像编码器和文本编码器，得到嵌入向量。
        2.  计算两个向量的余弦相似度。
    *   **Rule-of-Thumb**: 设置一个合理的值，例如 **0.28**。低于此分数的图文对很可能是不相关的（例如，`alt` 文本是 "logo" 或 "advertisement"），应被丢弃或降权。

经过上述流水线后，我们得到一个带有丰富元数据的、高质量的图文对清单。这些元数据（如 aesthetic_score, clip_score）可以在后续的数据采样阶段用作权重。

### 2.3 图像离散化：从像素到 Token (Image Discretization)

为了让 Transformer 能够像处理文本一样处理图像，我们需要将图像“翻译”成一个离散的 token 序列。我们采用 **VQ-VAE (Vector Quantized-Variational AutoEncoder)** 或其变体（如 VQGAN, dVAE）来实现这一目标。

**核心思想**:
1.  **编码 (Encoder)**：一个卷积神经网络（CNN）将输入图像 $I \in \mathbb{R}^{H \times W \times C}$ 压缩成一个较低分辨率的特征图（latent map）$Z \in \mathbb{R}^{h \times w \times d}$。
2.  **量化 (Quantization)**：这是关键步骤。我们维护一个可学习的“码本”（codebook）$C = \{c_1, c_2, ..., c_K\}$，其中每个 $c_i \in \mathbb{R}^d$ 是一个嵌入向量。对于 $Z$ 中的每一个 $d$ 维向量，我们找到码本中与之最接近的向量 $c_k$，并用其索引 $k$ 来代替。
3.  **解码 (Decoder)**：一个转置卷积网络将量化后的特征图 $Z_q$ 重建回原始图像 $\hat{I}$。

通过这个过程，一张 $256 \times 256$ 的图像，经过一个下采样率为 16 的编码器，会产生一个 $16 \times 16$ 的特征图。对这 $16 \times 16 = 256$ 个向量进行量化后，我们就得到一个由 256 个整数索引（token）组成的序列。

**生产级方案**:
*   **选择预训练的 VQ-VAE**：从头训练一个高质量的 VQ-VAE 代价高昂。强烈建议使用一个在海量、多样化数据上预训练好的、公开发布的 VQ-VAE 模型（如 OpenAI 的 dVAE 或 Stability AI 的 VAE）。这能确保离散化的质量和泛化能力。
*   **离散化流程**:
    1.  加载预训练的 VQ-VAE 模型。
    2.  对于质量过滤后的每一张图片，进行预处理（resize到 $256 \times 256$ 或 $384 \times 384$，归一化）。
    3.  将图片送入 VQ-VAE 的编码器和量化层，获取 $16 \times 16$ 的 token 索引矩阵。
    4.  将该矩阵展平（flatten）为一个长度为 256 的 token 序列。
    5.  将这个序列与图片的元数据、`alt` 文本一同存储。
*   **参数配置 Rule-of-Thumb**:
    *   **输入分辨率**: $256 \times 256$。
    *   **下采样因子 (Downsampling Factor)**: 16。
    *   **码本大小 (Codebook Size)**: 8192 或 16384。
    *   **输出序列长度**: $ (256/16) \times (256/16) = 16 \times 16 = 256 $ tokens。

最终，我们原始的图像数据集被转换成了一个大规模的、由整数序列构成的数据集，可以被主模型直接消费。

## 3. 本章小结

本章详细介绍了构建生产级图像数据集的完整流程，将抽象的目标（1.0T token）分解为具体可执行的步骤。

1.  **数据采集**：们采用混合策略，结合大规模自建网页抓取（追求广度）和高质量精选集（追求深度），并始终强调合规性。
2.  **质量控制**：建立了一个五阶段的过滤与打分流水线，是确保数据质量的核心。关键技术包括**感知哈希去重**和**基于 CLIP 的图文相关性打分**，它们是决定模型最终性能的关键。
3.  **统一离散化**：通过使用预训练的 **VQ-VAE**，我们将连续的图像数据转换成了与文本兼容的**离散 token 序列**（每张图 256 个 token），为后续的多模态融合训练铺平了道路。

下表总结了本章的核心流水线与关键参数：

| 阶段           | 核心任务                                     | 关键技术/工具                       | Rule-of-Thumb / 输出                      |
| -------------- | -------------------------------------------- | ----------------------------------- | ----------------------------------------- |
| **采集**       | 获取原始图文对                               | Common Crawl, `aiohttp`, `selectolax` | 遵守 `robots.txt`，存储原始 URL            |
| **质量过滤**   | 清洗、去重、打分                             | NSFW-Classifier, pHash, CLIP        | CLIP Score > 0.28, pHash Hamming Dist < 5 |
| **离散化**     | 将图像转换为 token 序列                      | 预训练的 VQ-VAE/VQGAN               | $256 \times 256 \to 16 \times 16 = 256$ tokens/图    |

现在，我们拥有了高质量、格式统一的图像 token，准备好在下一章与其它模态的 token 进行融合了。

## 4. 常见陷阱与错误 (Gotchas)

1.  **陷阱：数据越多越好，忽视质量。**
    *   **后果**：模型花费大量时间学习噪声，导致生成结果模糊、文不对题，或者充满水印。
    *   **调试与避免**：严格执行质量过滤流水线。在训练初期，可以先用一小部分得分最高（如 CLIP score > 0.35, aesthetic score > 6.0）的数据进行试验，建立一个高质量的性能基线。

2.  **陷阱：文本描述质量被忽略。**
    *   **后果**：`alt` 文本常常是 "image", "logo", "1.jpg" 或者 SEO 关键词堆砌。这会严重毒害模型的图文对齐能力。
    *   **调试与避免**：CLIP score 是对抗此问题的利器。定期抽样检查低 CLIP score 的样本，分析 `alt` 文本的常见问题，甚至可以训练一个“垃圾文本”分类器来辅助过滤。

3.  **陷阱：近似重复内容泛滥。**
    *   **后果**：模型对某些常见图像（如名人、logo、meme）过拟合，在评测集上造成虚高的性能，但泛化能力差。
    *   **调试与避免**：必须执行严格的基于感知哈希的近似去重。在评测集构建时，也要用同样的方法确保训练集与评测集之间没有近似重复。

4.  **陷阱：从零开始训练 VQ-VAE。**
    *   **后果**：耗费大量算力，且训练不当的 VQ-VAE 会导致重建图像质量差（离散化损失信息过多），成为整个模型性能的瓶颈。
    *   **调试与避免**：除非有特殊需求和充足算力，否则优先使用社区公认的高质量预训练 VQ-VAE。将精力集中在数据质量和主模型的训练上。

5.  **陷阱：存储管理混乱。**
    *   **后果**：数十亿张图片的原始文件会轻易撑爆存储，管理成本高昂。
    *   **调试与避免**：设计一个清晰的存储策略。例如：`原始图片 (S3) -> [处理] -> 离散化 token + 元数据 (Parquet/Arrow on S3) -> 删除原始图片 (或归档到低成本存储)`。只保留 token 和必要的元数据用于训练，能极大降低存储和 I/O 成本。
